diff --git a/clang/test/CodeGen/mips16-mult.c b/clang/test/CodeGen/mips16-mult.c
new file mode 100644
index 000000000000..a78d3a7c90c7
--- /dev/null
+++ b/clang/test/CodeGen/mips16-mult.c
@@ -0,0 +1,11 @@
+// REQUIRES: mips-registered-target
+// RUN: %clang --target=mipsel-unknown-linux -mips16 -c -o %t %s
+// RUN: %clang --target=mipsel-unknown-linux -mips16 -c -S %s -o -| FileCheck %s
+
+// Check for BugZilla bug 49146 "Crash with MIPS16 Multiply".
+
+unsigned do_mult(unsigned n, unsigned m) {
+  return n * m;
+// CHECK: mult ${{[0-9]+}}, ${{[0-9]+}}
+// CHECK: mflo ${{[0-9]+}}
+}
diff --git a/llvm/lib/Target/Mips/Mips16ISelDAGToDAG.cpp b/llvm/lib/Target/Mips/Mips16ISelDAGToDAG.cpp
index ddd28d095e51..667a4b426381 100644
--- a/llvm/lib/Target/Mips/Mips16ISelDAGToDAG.cpp
+++ b/llvm/lib/Target/Mips/Mips16ISelDAGToDAG.cpp
@@ -1,225 +1,232 @@
 //===-- Mips16ISelDAGToDAG.cpp - A Dag to Dag Inst Selector for Mips16 ----===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
 //
 // Subclass of MipsDAGToDAGISel specialized for mips16.
 //
 //===----------------------------------------------------------------------===//
 
 #include "Mips16ISelDAGToDAG.h"
 #include "MCTargetDesc/MipsBaseInfo.h"
 #include "Mips.h"
 #include "MipsMachineFunction.h"
 #include "MipsRegisterInfo.h"
 #include "llvm/CodeGen/MachineConstantPool.h"
 #include "llvm/CodeGen/MachineFrameInfo.h"
 #include "llvm/CodeGen/MachineFunction.h"
 #include "llvm/CodeGen/MachineInstrBuilder.h"
 #include "llvm/CodeGen/MachineRegisterInfo.h"
 #include "llvm/CodeGen/SelectionDAGNodes.h"
 #include "llvm/IR/CFG.h"
 #include "llvm/IR/GlobalValue.h"
 #include "llvm/IR/Instructions.h"
 #include "llvm/IR/Intrinsics.h"
 #include "llvm/IR/Type.h"
 #include "llvm/Support/Debug.h"
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/raw_ostream.h"
 #include "llvm/Target/TargetMachine.h"
 using namespace llvm;
 
 #define DEBUG_TYPE "mips-isel"
 
 bool Mips16DAGToDAGISel::runOnMachineFunction(MachineFunction &MF) {
   Subtarget = &static_cast<const MipsSubtarget &>(MF.getSubtarget());
   if (!Subtarget->inMips16Mode())
     return false;
   return MipsDAGToDAGISel::runOnMachineFunction(MF);
 }
 /// Select multiply instructions.
 std::pair<SDNode *, SDNode *>
 Mips16DAGToDAGISel::selectMULT(SDNode *N, unsigned Opc, const SDLoc &DL, EVT Ty,
                                bool HasLo, bool HasHi) {
   SDNode *Lo = nullptr, *Hi = nullptr;
   SDNode *Mul = CurDAG->getMachineNode(Opc, DL, MVT::Glue, N->getOperand(0),
                                        N->getOperand(1));
   SDValue InFlag = SDValue(Mul, 0);
 
   if (HasLo) {
     unsigned Opcode = Mips::Mflo16;
     Lo = CurDAG->getMachineNode(Opcode, DL, Ty, MVT::Glue, InFlag);
     InFlag = SDValue(Lo, 1);
   }
   if (HasHi) {
     unsigned Opcode = Mips::Mfhi16;
     Hi = CurDAG->getMachineNode(Opcode, DL, Ty, InFlag);
   }
   return std::make_pair(Lo, Hi);
 }
 
 void Mips16DAGToDAGISel::initGlobalBaseReg(MachineFunction &MF) {
   MipsFunctionInfo *MipsFI = MF.getInfo<MipsFunctionInfo>();
 
   if (!MipsFI->globalBaseRegSet())
     return;
 
   MachineBasicBlock &MBB = MF.front();
   MachineBasicBlock::iterator I = MBB.begin();
   MachineRegisterInfo &RegInfo = MF.getRegInfo();
   const TargetInstrInfo &TII = *Subtarget->getInstrInfo();
   DebugLoc DL;
   Register V0, V1, V2, GlobalBaseReg = MipsFI->getGlobalBaseReg(MF);
   const TargetRegisterClass *RC = &Mips::CPU16RegsRegClass;
 
   V0 = RegInfo.createVirtualRegister(RC);
   V1 = RegInfo.createVirtualRegister(RC);
   V2 = RegInfo.createVirtualRegister(RC);
 
 
   BuildMI(MBB, I, DL, TII.get(Mips::LiRxImmX16), V0)
       .addExternalSymbol("_gp_disp", MipsII::MO_ABS_HI);
   BuildMI(MBB, I, DL, TII.get(Mips::AddiuRxPcImmX16), V1)
       .addExternalSymbol("_gp_disp", MipsII::MO_ABS_LO);
 
   BuildMI(MBB, I, DL, TII.get(Mips::SllX16), V2).addReg(V0).addImm(16);
   BuildMI(MBB, I, DL, TII.get(Mips::AdduRxRyRz16), GlobalBaseReg)
       .addReg(V1)
       .addReg(V2);
 }
 
 void Mips16DAGToDAGISel::processFunctionAfterISel(MachineFunction &MF) {
   initGlobalBaseReg(MF);
 }
 
 bool Mips16DAGToDAGISel::selectAddr(bool SPAllowed, SDValue Addr, SDValue &Base,
                                     SDValue &Offset) {
   SDLoc DL(Addr);
   EVT ValTy = Addr.getValueType();
 
   // if Address is FI, get the TargetFrameIndex.
   if (SPAllowed) {
     if (FrameIndexSDNode *FIN = dyn_cast<FrameIndexSDNode>(Addr)) {
       Base = CurDAG->getTargetFrameIndex(FIN->getIndex(), ValTy);
       Offset = CurDAG->getTargetConstant(0, DL, ValTy);
       return true;
     }
   }
   // on PIC code Load GA
   if (Addr.getOpcode() == MipsISD::Wrapper) {
     Base = Addr.getOperand(0);
     Offset = Addr.getOperand(1);
     return true;
   }
   if (!TM.isPositionIndependent()) {
     if ((Addr.getOpcode() == ISD::TargetExternalSymbol ||
          Addr.getOpcode() == ISD::TargetGlobalAddress))
       return false;
   }
   // Addresses of the form FI+const or FI|const
   if (CurDAG->isBaseWithConstantOffset(Addr)) {
     ConstantSDNode *CN = dyn_cast<ConstantSDNode>(Addr.getOperand(1));
     if (isInt<16>(CN->getSExtValue())) {
       // If the first operand is a FI, get the TargetFI Node
       if (SPAllowed) {
         if (FrameIndexSDNode *FIN =
                 dyn_cast<FrameIndexSDNode>(Addr.getOperand(0))) {
           Base = CurDAG->getTargetFrameIndex(FIN->getIndex(), ValTy);
           Offset = CurDAG->getTargetConstant(CN->getZExtValue(), DL, ValTy);
           return true;
         }
       }
 
       Base = Addr.getOperand(0);
       Offset = CurDAG->getTargetConstant(CN->getZExtValue(), DL, ValTy);
       return true;
     }
   }
   // Operand is a result from an ADD.
   if (Addr.getOpcode() == ISD::ADD) {
     // When loading from constant pools, load the lower address part in
     // the instruction itself. Example, instead of:
     //  lui $2, %hi($CPI1_0)
     //  addiu $2, $2, %lo($CPI1_0)
     //  lwc1 $f0, 0($2)
     // Generate:
     //  lui $2, %hi($CPI1_0)
     //  lwc1 $f0, %lo($CPI1_0)($2)
     if (Addr.getOperand(1).getOpcode() == MipsISD::Lo ||
         Addr.getOperand(1).getOpcode() == MipsISD::GPRel) {
       SDValue Opnd0 = Addr.getOperand(1).getOperand(0);
       if (isa<ConstantPoolSDNode>(Opnd0) || isa<GlobalAddressSDNode>(Opnd0) ||
           isa<JumpTableSDNode>(Opnd0)) {
         Base = Addr.getOperand(0);
         Offset = Opnd0;
         return true;
       }
     }
   }
   Base = Addr;
   Offset = CurDAG->getTargetConstant(0, DL, ValTy);
   return true;
 }
 
 bool Mips16DAGToDAGISel::selectAddr16(SDValue Addr, SDValue &Base,
                                       SDValue &Offset) {
   return selectAddr(false, Addr, Base, Offset);
 }
 
 bool Mips16DAGToDAGISel::selectAddr16SP(SDValue Addr, SDValue &Base,
                                         SDValue &Offset) {
   return selectAddr(true, Addr, Base, Offset);
 }
 
 /// Select instructions not customized! Used for
 /// expanded, promoted and normal instructions
 bool Mips16DAGToDAGISel::trySelect(SDNode *Node) {
   unsigned Opcode = Node->getOpcode();
   SDLoc DL(Node);
 
   ///
   // Instruction Selection not handled by the auto-generated
   // tablegen selection should be handled here.
   ///
   EVT NodeTy = Node->getValueType(0);
   unsigned MultOpc;
 
   switch (Opcode) {
   default:
     break;
 
+  case ISD::MUL: {
+    MultOpc = Mips::MultRxRy16;
+    auto LoHi = selectMULT(Node, MultOpc, DL, NodeTy, true, false);
+    ReplaceNode(Node, LoHi.first);
+    return true;
+  }
+
   /// Mul with two results
   case ISD::SMUL_LOHI:
   case ISD::UMUL_LOHI: {
     MultOpc = (Opcode == ISD::UMUL_LOHI ? Mips::MultuRxRy16 : Mips::MultRxRy16);
     std::pair<SDNode *, SDNode *> LoHi =
         selectMULT(Node, MultOpc, DL, NodeTy, true, true);
     if (!SDValue(Node, 0).use_empty())
       ReplaceUses(SDValue(Node, 0), SDValue(LoHi.first, 0));
 
     if (!SDValue(Node, 1).use_empty())
       ReplaceUses(SDValue(Node, 1), SDValue(LoHi.second, 0));
 
     CurDAG->RemoveDeadNode(Node);
     return true;
   }
 
   case ISD::MULHS:
   case ISD::MULHU: {
     MultOpc = (Opcode == ISD::MULHU ? Mips::MultuRxRy16 : Mips::MultRxRy16);
     auto LoHi = selectMULT(Node, MultOpc, DL, NodeTy, false, true);
     ReplaceNode(Node, LoHi.second);
     return true;
   }
   }
 
   return false;
 }
 
 FunctionPass *llvm::createMips16ISelDag(MipsTargetMachine &TM,
                                         CodeGenOpt::Level OptLevel) {
   return new Mips16DAGToDAGISel(TM, OptLevel);
 }
diff --git a/llvm/lib/Target/Mips/Mips16InstrInfo.td b/llvm/lib/Target/Mips/Mips16InstrInfo.td
index 990202b23bc0..c4a43d44b7c4 100644
--- a/llvm/lib/Target/Mips/Mips16InstrInfo.td
+++ b/llvm/lib/Target/Mips/Mips16InstrInfo.td
@@ -1,1910 +1,1891 @@
 //===- Mips16InstrInfo.td - Target Description for Mips16  -*- tablegen -*-=//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
 //
 // This file describes Mips16 instructions.
 //
 //===----------------------------------------------------------------------===//
 //
 //
 // Mips Address
 //
 def addr16 : ComplexPattern<iPTR, 2, "selectAddr16", [frameindex]>;
 def addr16sp : ComplexPattern<iPTR, 2, "selectAddr16SP", [frameindex]>;
 
 //
 // Address operand
 def mem16 : Operand<i32> {
   let PrintMethod = "printMemOperand";
   let MIOperandInfo = (ops CPU16Regs, simm16);
   let EncoderMethod = "getMemEncoding";
 }
 
 def mem16sp : Operand<i32> {
   let PrintMethod = "printMemOperand";
   // This should be CPUSPReg but the MIPS16 subtarget isn't good enough at
   // keeping the sp-relative load and the other varieties separate at the
   // moment. This lie fixes the problem sufficiently well to fix the errors
   // emitted by -verify-machineinstrs and the output ends up correct as long
   // as we use an external assembler (which is already a requirement for MIPS16
   // for several other reasons).
   let MIOperandInfo = (ops CPU16RegsPlusSP, simm16);
   let EncoderMethod = "getMemEncoding";
 }
 
 def mem16_ea : Operand<i32> {
   let PrintMethod = "printMemOperandEA";
   let MIOperandInfo = (ops CPU16RegsPlusSP, simm16);
   let EncoderMethod = "getMemEncoding";
 }
 
 def pcrel16 : Operand<i32>;
 
 //
 // I-type instruction format
 //
 // this is only used by bimm. the actual assembly value is a 12 bit signed
 // number
 //
 class FI16_ins<bits<5> op, string asmstr, InstrItinClass itin>:
   FI16<op, (outs), (ins brtarget:$imm16),
             !strconcat(asmstr, "\t$imm16 # 16 bit inst"), [], itin>;
 
 //
 //
 // I8 instruction format
 //
 
 class FI816_ins_base<bits<3> _func, string asmstr,
                      string asmstr2, InstrItinClass itin>:
   FI816<_func, (outs), (ins simm16:$imm), !strconcat(asmstr, asmstr2),
         [], itin>;
 
 class FI816_ins<bits<3> _func, string asmstr,
                 InstrItinClass itin>:
   FI816_ins_base<_func, asmstr, "\t$imm  # 16 bit inst", itin>;
 
 class FI816_SP_ins<bits<3> _func, string asmstr,
                    InstrItinClass itin>:
   FI816_ins_base<_func, asmstr, "\t$$sp, $imm # 16 bit inst", itin>;
 
 //
 // RI instruction format
 //
 
 
 class FRI16_ins_base<bits<5> op, string asmstr, string asmstr2,
                      InstrItinClass itin>:
   FRI16<op, (outs CPU16Regs:$rx), (ins simm16:$imm),
         !strconcat(asmstr, asmstr2), [], itin>;
 
 class FRI16_ins<bits<5> op, string asmstr,
                 InstrItinClass itin>:
   FRI16_ins_base<op, asmstr, "\t$rx, $imm \t# 16 bit inst", itin>;
 
 class FRI16_TCP_ins<bits<5> _op, string asmstr,
                     InstrItinClass itin>:
   FRI16<_op, (outs CPU16Regs:$rx), (ins pcrel16:$imm, i32imm:$size),
             !strconcat(asmstr, "\t$rx, $imm\t# 16 bit inst"), [], itin>;
 
 class FRI16R_ins_base<bits<5> op, string asmstr, string asmstr2,
                      InstrItinClass itin>:
   FRI16<op, (outs), (ins CPU16Regs:$rx, simm16:$imm),
         !strconcat(asmstr, asmstr2), [], itin>;
 
 class FRI16R_ins<bits<5> op, string asmstr,
                 InstrItinClass itin>:
   FRI16R_ins_base<op, asmstr, "\t$rx, $imm \t# 16 bit inst", itin>;
 
 class F2RI16_ins<bits<5> _op, string asmstr,
                      InstrItinClass itin>:
   FRI16<_op, (outs CPU16Regs:$rx), (ins CPU16Regs:$rx_, simm16:$imm),
         !strconcat(asmstr, "\t$rx, $imm\t# 16 bit inst"), [], itin> {
   let Constraints = "$rx_ = $rx";
 }
 
 class FRI16_B_ins<bits<5> _op, string asmstr,
                   InstrItinClass itin>:
   FRI16<_op, (outs), (ins  CPU16Regs:$rx, brtarget:$imm),
         !strconcat(asmstr, "\t$rx, $imm  # 16 bit inst"), [], itin>;
 //
 // Compare a register and immediate and place result in CC
 // Implicit use of T8
 //
 // EXT-CCRR Instruction format
 //
 class FEXT_CCRXI16_ins<string asmstr>:
   MipsPseudo16<(outs CPU16Regs:$cc), (ins CPU16Regs:$rx, simm16:$imm),
                !strconcat(asmstr, "\t$rx, $imm\n\tmove\t$cc, $$t8"), []> {
   let isCodeGenOnly=1;
   let usesCustomInserter = 1;
 }
 
 // JAL and JALX instruction format
 //
 class FJAL16_ins<bits<1> _X, string asmstr,
                  InstrItinClass itin>:
   FJAL16<_X, (outs), (ins uimm26:$imm),
          !strconcat(asmstr, "\t$imm\n\tnop"),[],
          itin>  {
   let isCodeGenOnly=1;
   let Size=6;
 }
 
 class FJALB16_ins<bits<1> _X, string asmstr,
                  InstrItinClass itin>:
   FJAL16<_X, (outs), (ins uimm26:$imm),
          !strconcat(asmstr, "\t$imm\t# branch\n\tnop"),[],
          itin>  {
   let isCodeGenOnly=1;
   let Size=6;
 }
 
 //
 // EXT-I instruction format
 //
 class FEXT_I16_ins<bits<5> eop, string asmstr, InstrItinClass itin> :
   FEXT_I16<eop, (outs), (ins brtarget:$imm16),
            !strconcat(asmstr, "\t$imm16"),[], itin>;
 
 //
 // EXT-I8 instruction format
 //
 
 class FEXT_I816_ins_base<bits<3> _func, string asmstr,
                          string asmstr2, InstrItinClass itin>:
   FEXT_I816<_func, (outs), (ins simm16:$imm), !strconcat(asmstr, asmstr2),
             [], itin>;
 
 class FEXT_I816_ins<bits<3> _func, string asmstr,
                     InstrItinClass itin>:
   FEXT_I816_ins_base<_func, asmstr, "\t$imm", itin>;
 
 class FEXT_I816_SP_ins<bits<3> _func, string asmstr,
                        InstrItinClass itin>:
       FEXT_I816_ins_base<_func, asmstr, "\t$$sp, $imm", itin>;
 
 //
 // Assembler formats in alphabetical order.
 // Natural and pseudos are mixed together.
 //
 // Compare two registers and place result in CC
 // Implicit use of T8
 //
 // CC-RR Instruction format
 //
 class FCCRR16_ins<string asmstr> :
   MipsPseudo16<(outs CPU16Regs:$cc), (ins CPU16Regs:$rx, CPU16Regs:$ry),
                !strconcat(asmstr, "\t$rx, $ry\n\tmove\t$cc, $$t8"), []> {
   let isCodeGenOnly=1;
   let usesCustomInserter = 1;
 }
 
 //
 // EXT-RI instruction format
 //
 
 class FEXT_RI16_ins_base<bits<5> _op, string asmstr, string asmstr2,
                          InstrItinClass itin>:
   FEXT_RI16<_op, (outs CPU16Regs:$rx), (ins simm16:$imm),
                   !strconcat(asmstr, asmstr2), [], itin>;
 
 class FEXT_RI16_ins<bits<5> _op, string asmstr,
                     InstrItinClass itin>:
   FEXT_RI16_ins_base<_op, asmstr, "\t$rx, $imm", itin>;
 
 class FEXT_RI16R_ins_base<bits<5> _op, string asmstr, string asmstr2,
                          InstrItinClass itin>:
   FEXT_RI16<_op, (outs ), (ins CPU16Regs:$rx, simm16:$imm),
                   !strconcat(asmstr, asmstr2), [], itin>;
 
 class FEXT_RI16R_ins<bits<5> _op, string asmstr,
                     InstrItinClass itin>:
   FEXT_RI16R_ins_base<_op, asmstr, "\t$rx, $imm", itin>;
 
 class FEXT_RI16_PC_ins<bits<5> _op, string asmstr, InstrItinClass itin>:
   FEXT_RI16_ins_base<_op, asmstr, "\t$rx, $$pc, $imm", itin>;
 
 class FEXT_RI16_B_ins<bits<5> _op, string asmstr,
                       InstrItinClass itin>:
   FEXT_RI16<_op, (outs), (ins  CPU16Regs:$rx, brtarget:$imm),
             !strconcat(asmstr, "\t$rx, $imm"), [], itin>;
 
 class FEXT_RI16_TCP_ins<bits<5> _op, string asmstr,
                         InstrItinClass itin>:
   FEXT_RI16<_op, (outs CPU16Regs:$rx), (ins pcrel16:$imm, i32imm:$size),
             !strconcat(asmstr, "\t$rx, $imm"), [], itin>;
 
 class FEXT_2RI16_ins<bits<5> _op, string asmstr,
                      InstrItinClass itin>:
   FEXT_RI16<_op, (outs CPU16Regs:$rx), (ins CPU16Regs:$rx_, simm16:$imm),
             !strconcat(asmstr, "\t$rx, $imm"), [], itin> {
   let Constraints = "$rx_ = $rx";
 }
 
 //
 // EXT-RRI instruction format
 //
 
 class FEXT_RRI16_mem_ins<bits<5> op, string asmstr, Operand MemOpnd,
                          InstrItinClass itin>:
   FEXT_RRI16<op, (outs CPU16Regs:$ry), (ins  MemOpnd:$addr),
              !strconcat(asmstr, "\t$ry, $addr"), [], itin>;
 
 class FEXT_RRI16_mem2_ins<bits<5> op, string asmstr, Operand MemOpnd,
                           InstrItinClass itin>:
   FEXT_RRI16<op, (outs ), (ins  CPU16Regs:$ry, MemOpnd:$addr),
              !strconcat(asmstr, "\t$ry, $addr"), [], itin>;
 
 //
 //
 // EXT-RRI-A instruction format
 //
 
 class FEXT_RRI_A16_mem_ins<bits<1> op, string asmstr, Operand MemOpnd,
                            InstrItinClass itin>:
   FEXT_RRI_A16<op, (outs CPU16Regs:$ry), (ins  MemOpnd:$addr),
                !strconcat(asmstr, "\t$ry, $addr"), [], itin>;
 
 //
 // EXT-SHIFT instruction format
 //
 class FEXT_SHIFT16_ins<bits<2> _f, string asmstr, InstrItinClass itin>:
   FEXT_SHIFT16<_f, (outs CPU16Regs:$rx), (ins CPU16Regs:$ry, uimm5:$sa),
                !strconcat(asmstr, "\t$rx, $ry, $sa"), [], itin>;
 
 //
 // EXT-T8I8
 //
 class FEXT_T8I816_ins<string asmstr, string asmstr2>:
   MipsPseudo16<(outs),
                (ins CPU16Regs:$rx, CPU16Regs:$ry, brtarget:$imm),
                !strconcat(asmstr2, !strconcat("\t$rx, $ry\n\t",
                !strconcat(asmstr, "\t$imm"))),[]> {
   let isCodeGenOnly=1;
   let usesCustomInserter = 1;
 }
 
 //
 // EXT-T8I8I
 //
 class FEXT_T8I8I16_ins<string asmstr, string asmstr2>:
   MipsPseudo16<(outs),
                (ins CPU16Regs:$rx, simm16:$imm, brtarget:$targ),
                !strconcat(asmstr2, !strconcat("\t$rx, $imm\n\t",
                !strconcat(asmstr, "\t$targ"))), []> {
   let isCodeGenOnly=1;
   let usesCustomInserter = 1;
 }
 //
 
 
 //
 // I8_MOVR32 instruction format (used only by the MOVR32 instructio
 //
 class FI8_MOVR3216_ins<string asmstr, InstrItinClass itin>:
        FI8_MOVR3216<(outs CPU16Regs:$rz), (ins GPR32:$r32),
        !strconcat(asmstr,  "\t$rz, $r32"), [], itin>;
 
 //
 // I8_MOV32R instruction format (used only by MOV32R instruction)
 //
 
 class FI8_MOV32R16_ins<string asmstr, InstrItinClass itin>:
   FI8_MOV32R16<(outs GPR32:$r32), (ins CPU16Regs:$rz),
                !strconcat(asmstr,  "\t$r32, $rz"), [], itin>;
 
-//
-// This are pseudo formats for multiply
-// This first one can be changed to non-pseudo now.
-//
-// MULT
-//
-class FMULT16_ins<string asmstr, InstrItinClass itin> :
-  MipsPseudo16<(outs), (ins CPU16Regs:$rx, CPU16Regs:$ry),
-               !strconcat(asmstr, "\t$rx, $ry"), []>;
-
-//
-// MULT-LO
-//
-class FMULT16_LO_ins<string asmstr, InstrItinClass itin> :
-  MipsPseudo16<(outs CPU16Regs:$rz), (ins CPU16Regs:$rx, CPU16Regs:$ry),
-               !strconcat(asmstr, "\t$rx, $ry\n\tmflo\t$rz"), []> {
-  let isCodeGenOnly=1;
-}
-
 //
 // RR-type instruction format
 //
 
 class FRR16_ins<bits<5> f, string asmstr, InstrItinClass itin> :
   FRR16<f, (outs CPU16Regs:$rx), (ins CPU16Regs:$ry),
         !strconcat(asmstr, "\t$rx, $ry"), [], itin> {
 }
 
 class FRRBreakNull16_ins<string asmstr, InstrItinClass itin> :
   FRRBreak16<(outs), (ins), asmstr, [], itin> {
   let Code=0;
 }
 
 class FRR16R_ins<bits<5> f, string asmstr, InstrItinClass itin> :
   FRR16<f, (outs), (ins  CPU16Regs:$rx, CPU16Regs:$ry),
         !strconcat(asmstr, "\t$rx, $ry"), [], itin> {
 }
 
 class FRRTR16_ins<string asmstr> :
   MipsPseudo16<(outs CPU16Regs:$rz), (ins CPU16Regs:$rx, CPU16Regs:$ry),
                !strconcat(asmstr, "\t$rx, $ry\n\tmove\t$rz, $$t8"), []> ;
 
 //
 // maybe refactor but need a $zero as a dummy first parameter
 //
 class FRR16_div_ins<bits<5> f, string asmstr, InstrItinClass itin> :
   FRR16<f, (outs ), (ins CPU16Regs:$rx, CPU16Regs:$ry),
         !strconcat(asmstr, "\t$$zero, $rx, $ry"), [], itin> ;
 
+class FRR16_mult_ins<bits<5> f, string asmstr, InstrItinClass itin> :
+  FRR16<f, (outs ), (ins CPU16Regs:$rx, CPU16Regs:$ry),
+        !strconcat(asmstr, "\t$rx, $ry"), [], itin> ;
+
 class FUnaryRR16_ins<bits<5> f, string asmstr, InstrItinClass itin> :
   FRR16<f, (outs CPU16Regs:$rx), (ins CPU16Regs:$ry),
         !strconcat(asmstr, "\t$rx, $ry"), [], itin> ;
 
 
 class FRR16_M_ins<bits<5> f, string asmstr,
                   InstrItinClass itin> :
   FRR16<f, (outs CPU16Regs:$rx), (ins),
         !strconcat(asmstr, "\t$rx"), [], itin>;
 
 class FRxRxRy16_ins<bits<5> f, string asmstr,
                     InstrItinClass itin> :
   FRR16<f, (outs CPU16Regs:$rz), (ins CPU16Regs:$rx, CPU16Regs:$ry),
             !strconcat(asmstr, "\t$rz, $ry"),
             [], itin> {
   let Constraints = "$rx = $rz";
 }
 
 let rx=0 in
 class FRR16_JALRC_RA_only_ins<bits<1> nd_, bits<1> l_,
                               string asmstr, InstrItinClass itin>:
   FRR16_JALRC<nd_, l_, 1, (outs), (ins), !strconcat(asmstr, "\t$$ra"),
               [], itin> ;
 
 
 class FRR16_JALRC_ins<bits<1> nd, bits<1> l, bits<1> ra,
                       string asmstr, InstrItinClass itin>:
   FRR16_JALRC<nd, l, ra, (outs), (ins CPU16Regs:$rx),
               !strconcat(asmstr, "\t$rx"), [], itin> ;
 
 class FRR_SF16_ins
   <bits<5> _funct, bits<3> _subfunc,
     string asmstr, InstrItinClass itin>:
   FRR_SF16<_funct, _subfunc, (outs CPU16Regs:$rx), (ins CPU16Regs:$rx_),
            !strconcat(asmstr, "\t $rx"),
            [], itin> {
   let Constraints = "$rx_ = $rx";
   }
 //
 // RRR-type instruction format
 //
 
 class FRRR16_ins<bits<2> _f, string asmstr,  InstrItinClass itin> :
   FRRR16<_f, (outs CPU16Regs:$rz), (ins CPU16Regs:$rx, CPU16Regs:$ry),
          !strconcat(asmstr, "\t$rz, $rx, $ry"), [], itin>;
 
 //
 // These Sel patterns support the generation of conditional move
 // pseudo instructions.
 //
 // The nomenclature uses the components making up the pseudo and may
 // be a bit counter intuitive when compared with the end result we seek.
 // For example using a bqez in the example directly below results in the
 // conditional move being done if the tested register is not zero.
 // I considered in easier to check by keeping the pseudo consistent with
 // it's components but it could have been done differently.
 //
 // The simplest case is when can test and operand directly and do the
 // conditional move based on a simple mips16 conditional
 //  branch instruction.
 // for example:
 // if $op == beqz or bnez:
 //
 // $op1 $rt, .+4
 // move $rd, $rs
 //
 // if $op == beqz, then if $rt != 0, then the conditional assignment
 // $rd = $rs is done.
 
 // if $op == bnez, then if $rt == 0, then the conditional assignment
 // $rd = $rs is done.
 //
 // So this pseudo class only has one operand, i.e. op
 //
 class Sel<string op>:
   MipsPseudo16<(outs CPU16Regs:$rd_), (ins CPU16Regs:$rd, CPU16Regs:$rs,
                CPU16Regs:$rt),
                !strconcat(op, "\t$rt, .+4\n\t\n\tmove $rd, $rs"), []> {
   //let isCodeGenOnly=1;
   let Constraints = "$rd = $rd_";
   let usesCustomInserter = 1;
 }
 
 //
 // The next two instruction classes allow for an operand which tests
 // two operands and returns a value in register T8 and
 //then does a conditional branch based on the value of T8
 //
 
 // op2 can be cmpi or slti/sltiu
 // op1 can bteqz or btnez
 // the operands for op2 are a register and a signed constant
 //
 // $op2 $t, $imm  ;test register t and branch conditionally
 // $op1 .+4       ;op1 is a conditional branch
 // move $rd, $rs
 //
 //
 class SeliT<string op1, string op2>:
   MipsPseudo16<(outs CPU16Regs:$rd_), (ins CPU16Regs:$rd, CPU16Regs:$rs,
                                        CPU16Regs:$rl, simm16:$imm),
                !strconcat(op2,
                !strconcat("\t$rl, $imm\n\t",
                !strconcat(op1, "\t.+4\n\tmove $rd, $rs"))), []> {
   let isCodeGenOnly=1;
   let Constraints = "$rd = $rd_";
   let usesCustomInserter = 1;
 }
 
 //
 // op2 can be cmp or slt/sltu
 // op1 can be bteqz or btnez
 // the operands for op2 are two registers
 // op1 is a conditional branch
 //
 //
 // $op2 $rl, $rr  ;test registers rl,rr
 // $op1 .+4       ;op2 is a conditional branch
 // move $rd, $rs
 //
 //
 class SelT<string op1, string op2>:
   MipsPseudo16<(outs CPU16Regs:$rd_),
                (ins CPU16Regs:$rd, CPU16Regs:$rs,
                 CPU16Regs:$rl, CPU16Regs:$rr),
                !strconcat(op2,
                !strconcat("\t$rl, $rr\n\t",
                !strconcat(op1, "\t.+4\n\tmove $rd, $rs"))), []> {
   let isCodeGenOnly=1;
   let Constraints = "$rd = $rd_";
   let usesCustomInserter = 1;
 }
 
 //
 // 32 bit constant
 //
 def Constant32 : MipsPseudo16<(outs), (ins simm32:$imm), "\t.word $imm", []>;
 
 def LwConstant32 :
   MipsPseudo16<(outs CPU16Regs:$rx), (ins simm32:$imm, simm32:$constid),
                "lw\t$rx, 1f\n\tb\t2f\n\t.align\t2\n1: \t.word\t$imm\n2:", []>;
 
 //
 // Some general instruction class info
 //
 //
 
 class ArithLogic16Defs<bit isCom=0> {
   bits<5> shamt = 0;
   bit isCommutable = isCom;
   bit isReMaterializable = 1;
   bit hasSideEffects = 0;
 }
 
 class branch16 {
   bit isBranch = 1;
   bit isTerminator = 1;
   bit isBarrier = 1;
 }
 
 class cbranch16 {
   bit isBranch = 1;
   bit isTerminator = 1;
 }
 
 class MayLoad {
   bit mayLoad = 1;
 }
 
 class MayStore {
   bit mayStore = 1;
 }
 //
 
 
 // Format: ADDIU rx, immediate MIPS16e
 // Purpose: Add Immediate Unsigned Word (2-Operand, Extended)
 // To add a constant to a 32-bit integer.
 //
 def AddiuRxImmX16: FEXT_RI16_ins<0b01001, "addiu", IIM16Alu>;
 
 def AddiuRxRxImm16: F2RI16_ins<0b01001, "addiu", IIM16Alu>,
   ArithLogic16Defs<0> {
   let AddedComplexity = 5;
 }
 def AddiuRxRxImmX16: FEXT_2RI16_ins<0b01001, "addiu", IIM16Alu>,
   ArithLogic16Defs<0> {
   let isCodeGenOnly = 1;
 }
 
 def AddiuRxRyOffMemX16:
   FEXT_RRI_A16_mem_ins<0, "addiu", mem16_ea, IIM16Alu>;
 
 //
 
 // Format: ADDIU rx, pc, immediate MIPS16e
 // Purpose: Add Immediate Unsigned Word (3-Operand, PC-Relative, Extended)
 // To add a constant to the program counter.
 //
 def AddiuRxPcImmX16: FEXT_RI16_PC_ins<0b00001, "addiu", IIM16Alu>;
 
 //
 // Format: ADDIU sp, immediate MIPS16e
 // Purpose: Add Immediate Unsigned Word (2-Operand, SP-Relative, Extended)
 // To add a constant to the stack pointer.
 //
 def AddiuSpImm16
   : FI816_SP_ins<0b011, "addiu", IIM16Alu> {
   let Defs = [SP];
   let Uses = [SP];
   let AddedComplexity = 5;
 }
 
 def AddiuSpImmX16
   : FEXT_I816_SP_ins<0b011, "addiu", IIM16Alu> {
   let Defs = [SP];
   let Uses = [SP];
 }
 
 //
 // Format: ADDU rz, rx, ry MIPS16e
 // Purpose: Add Unsigned Word (3-Operand)
 // To add 32-bit integers.
 //
 
 def AdduRxRyRz16: FRRR16_ins<01, "addu", IIM16Alu>, ArithLogic16Defs<1>;
 
 //
 // Format: AND rx, ry MIPS16e
 // Purpose: AND
 // To do a bitwise logical AND.
 
 def AndRxRxRy16: FRxRxRy16_ins<0b01100, "and", IIM16Alu>, ArithLogic16Defs<1>;
 
 
 //
 // Format: BEQZ rx, offset MIPS16e
 // Purpose: Branch on Equal to Zero
 // To test a GPR then do a PC-relative conditional branch.
 //
 def BeqzRxImm16: FRI16_B_ins<0b00100, "beqz", IIM16Alu>, cbranch16;
 
 
 //
 // Format: BEQZ rx, offset MIPS16e
 // Purpose: Branch on Equal to Zero (Extended)
 // To test a GPR then do a PC-relative conditional branch.
 //
 def BeqzRxImmX16: FEXT_RI16_B_ins<0b00100, "beqz", IIM16Alu>, cbranch16;
 
 //
 // Format: B offset MIPS16e
 // Purpose: Unconditional Branch (Extended)
 // To do an unconditional PC-relative branch.
 //
 
 def Bimm16: FI16_ins<0b00010, "b", IIM16Alu>, branch16;
 
 // Format: B offset MIPS16e
 // Purpose: Unconditional Branch
 // To do an unconditional PC-relative branch.
 //
 def BimmX16: FEXT_I16_ins<0b00010, "b", IIM16Alu>, branch16;
 
 //
 // Format: BNEZ rx, offset MIPS16e
 // Purpose: Branch on Not Equal to Zero
 // To test a GPR then do a PC-relative conditional branch.
 //
 def BnezRxImm16: FRI16_B_ins<0b00101, "bnez", IIM16Alu>, cbranch16;
 
 //
 // Format: BNEZ rx, offset MIPS16e
 // Purpose: Branch on Not Equal to Zero (Extended)
 // To test a GPR then do a PC-relative conditional branch.
 //
 def BnezRxImmX16: FEXT_RI16_B_ins<0b00101, "bnez", IIM16Alu>, cbranch16;
 
 
 //
 //Format: BREAK immediate
 // Purpose: Breakpoint
 // To cause a Breakpoint exception.
 
 def Break16: FRRBreakNull16_ins<"break 0", IIM16Alu>;
 //
 // Format: BTEQZ offset MIPS16e
 // Purpose: Branch on T Equal to Zero (Extended)
 // To test special register T then do a PC-relative conditional branch.
 //
 def Bteqz16: FI816_ins<0b000, "bteqz", IIM16Alu>, cbranch16 {
   let Uses = [T8];
 }
 
 def BteqzX16: FEXT_I816_ins<0b000, "bteqz", IIM16Alu>, cbranch16 {
   let Uses = [T8];
 }
 
 def BteqzT8CmpX16: FEXT_T8I816_ins<"bteqz", "cmp">, cbranch16;
 
 def BteqzT8CmpiX16: FEXT_T8I8I16_ins<"bteqz", "cmpi">,
   cbranch16;
 
 def BteqzT8SltX16: FEXT_T8I816_ins<"bteqz", "slt">, cbranch16;
 
 def BteqzT8SltuX16: FEXT_T8I816_ins<"bteqz", "sltu">, cbranch16;
 
 def BteqzT8SltiX16: FEXT_T8I8I16_ins<"bteqz", "slti">, cbranch16;
 
 def BteqzT8SltiuX16: FEXT_T8I8I16_ins<"bteqz", "sltiu">,
   cbranch16;
 
 //
 // Format: BTNEZ offset MIPS16e
 // Purpose: Branch on T Not Equal to Zero (Extended)
 // To test special register T then do a PC-relative conditional branch.
 //
 
 def Btnez16: FI816_ins<0b001, "btnez", IIM16Alu>, cbranch16 {
   let Uses = [T8];
 }
 
 def BtnezX16: FEXT_I816_ins<0b001, "btnez", IIM16Alu> ,cbranch16 {
   let Uses = [T8];
 }
 
 def BtnezT8CmpX16: FEXT_T8I816_ins<"btnez", "cmp">, cbranch16;
 
 def BtnezT8CmpiX16: FEXT_T8I8I16_ins<"btnez", "cmpi">, cbranch16;
 
 def BtnezT8SltX16: FEXT_T8I816_ins<"btnez", "slt">, cbranch16;
 
 def BtnezT8SltuX16: FEXT_T8I816_ins<"btnez", "sltu">, cbranch16;
 
 def BtnezT8SltiX16: FEXT_T8I8I16_ins<"btnez", "slti">, cbranch16;
 
 def BtnezT8SltiuX16: FEXT_T8I8I16_ins<"btnez", "sltiu">,
   cbranch16;
 
 //
 // Format: CMP rx, ry MIPS16e
 // Purpose: Compare
 // To compare the contents of two GPRs.
 //
 def CmpRxRy16: FRR16R_ins<0b01010, "cmp", IIM16Alu> {
   let Defs = [T8];
 }
 
 //
 // Format: CMPI rx, immediate MIPS16e
 // Purpose: Compare Immediate
 // To compare a constant with the contents of a GPR.
 //
 def CmpiRxImm16: FRI16R_ins<0b01110, "cmpi", IIM16Alu> {
   let Defs = [T8];
 }
 
 //
 // Format: CMPI rx, immediate MIPS16e
 // Purpose: Compare Immediate (Extended)
 // To compare a constant with the contents of a GPR.
 //
 def CmpiRxImmX16: FEXT_RI16R_ins<0b01110, "cmpi", IIM16Alu> {
   let Defs = [T8];
 }
 
 
 //
 // Format: DIV rx, ry MIPS16e
 // Purpose: Divide Word
 // To divide 32-bit signed integers.
 //
 def DivRxRy16: FRR16_div_ins<0b11010, "div", IIM16Alu> {
   let Defs = [HI0, LO0];
 }
 
 //
 // Format: DIVU rx, ry MIPS16e
 // Purpose: Divide Unsigned Word
 // To divide 32-bit unsigned integers.
 //
 def DivuRxRy16: FRR16_div_ins<0b11011, "divu", IIM16Alu> {
   let Defs = [HI0, LO0];
 }
+
 //
 // Format: JAL target MIPS16e
 // Purpose: Jump and Link
 // To execute a procedure call within the current 256 MB-aligned
 // region and preserve the current ISA.
 //
 
 def Jal16 : FJAL16_ins<0b0, "jal", IIM16Alu> {
   let hasDelaySlot = 0;  // not true, but we add the nop for now
   let isCall=1;
   let Defs = [RA];
 }
 
 def JalB16 : FJALB16_ins<0b0, "jal", IIM16Alu>, branch16 {
   let hasDelaySlot = 0;  // not true, but we add the nop for now
   let isBranch=1;
   let Defs = [RA];
 }
 
 //
 // Format: JR ra MIPS16e
 // Purpose: Jump Register Through Register ra
 // To execute a branch to the instruction address in the return
 // address register.
 //
 
 def JrRa16: FRR16_JALRC_RA_only_ins<0, 0, "jr", IIM16Alu> {
   let isBranch = 1;
   let isIndirectBranch = 1;
   let hasDelaySlot = 1;
   let isTerminator=1;
   let isBarrier=1;
   let isReturn=1;
 }
 
 def JrcRa16: FRR16_JALRC_RA_only_ins<1, 1, "jrc", IIM16Alu> {
   let isBranch = 1;
   let isIndirectBranch = 1;
   let isTerminator=1;
   let isBarrier=1;
   let isReturn=1;
 }
 
 def JrcRx16: FRR16_JALRC_ins<1, 1, 0, "jrc", IIM16Alu> {
   let isBranch = 1;
   let isIndirectBranch = 1;
   let isTerminator=1;
   let isBarrier=1;
 }
 //
 // Format: LB ry, offset(rx) MIPS16e
 // Purpose: Load Byte (Extended)
 // To load a byte from memory as a signed value.
 //
 def LbRxRyOffMemX16: FEXT_RRI16_mem_ins<0b10011, "lb", mem16, II_LB>, MayLoad{
   let isCodeGenOnly = 1;
 }
 
 //
 // Format: LBU ry, offset(rx) MIPS16e
 // Purpose: Load Byte Unsigned (Extended)
 // To load a byte from memory as a unsigned value.
 //
 def LbuRxRyOffMemX16:
   FEXT_RRI16_mem_ins<0b10100, "lbu", mem16, II_LBU>, MayLoad {
   let isCodeGenOnly = 1;
 }
 
 //
 // Format: LH ry, offset(rx) MIPS16e
 // Purpose: Load Halfword signed (Extended)
 // To load a halfword from memory as a signed value.
 //
 def LhRxRyOffMemX16: FEXT_RRI16_mem_ins<0b10100, "lh", mem16, II_LH>, MayLoad{
   let isCodeGenOnly = 1;
 }
 
 //
 // Format: LHU ry, offset(rx) MIPS16e
 // Purpose: Load Halfword unsigned (Extended)
 // To load a halfword from memory as an unsigned value.
 //
 def LhuRxRyOffMemX16:
   FEXT_RRI16_mem_ins<0b10100, "lhu", mem16, II_LHU>, MayLoad {
   let isCodeGenOnly = 1;
 }
 
 //
 // Format: LI rx, immediate MIPS16e
 // Purpose: Load Immediate
 // To load a constant into a GPR.
 //
 def LiRxImm16: FRI16_ins<0b01101, "li", IIM16Alu>;
 
 //
 // Format: LI rx, immediate MIPS16e
 // Purpose: Load Immediate (Extended)
 // To load a constant into a GPR.
 //
 def LiRxImmX16: FEXT_RI16_ins<0b01101, "li", IIM16Alu>;
 
 def LiRxImmAlignX16: FEXT_RI16_ins<0b01101, ".align 2\n\tli", IIM16Alu> {
   let isCodeGenOnly = 1;
 }
 
 //
 // Format: LW ry, offset(rx) MIPS16e
 // Purpose: Load Word (Extended)
 // To load a word from memory as a signed value.
 //
 def LwRxRyOffMemX16: FEXT_RRI16_mem_ins<0b10011, "lw", mem16, II_LW>, MayLoad{
   let isCodeGenOnly = 1;
 }
 
 // Format: LW rx, offset(sp) MIPS16e
 // Purpose: Load Word (SP-Relative, Extended)
 // To load an SP-relative word from memory as a signed value.
 //
 def LwRxSpImmX16: FEXT_RRI16_mem_ins<0b10010, "lw", mem16sp, II_LW>, MayLoad;
 
 def LwRxPcTcp16: FRI16_TCP_ins<0b10110, "lw", II_LW>, MayLoad;
 
 def LwRxPcTcpX16: FEXT_RI16_TCP_ins<0b10110, "lw", II_LW>, MayLoad;
 //
 // Format: MOVE r32, rz MIPS16e
 // Purpose: Move
 // To move the contents of a GPR to a GPR.
 //
 def Move32R16: FI8_MOV32R16_ins<"move", IIM16Alu>;
 
 //
 // Format: MOVE ry, r32 MIPS16e
 //Purpose: Move
 // To move the contents of a GPR to a GPR.
 //
 def MoveR3216: FI8_MOVR3216_ins<"move", IIM16Alu> {
   let isMoveReg = 1;
 }
 
 //
 // Format: MFHI rx MIPS16e
 // Purpose: Move From HI Register
 // To copy the special purpose HI register to a GPR.
 //
 def Mfhi16: FRR16_M_ins<0b10000, "mfhi", IIM16Alu> {
   let Uses = [HI0];
   let hasSideEffects = 0;
   let isMoveReg = 1;
 }
 
 //
 // Format: MFLO rx MIPS16e
 // Purpose: Move From LO Register
 // To copy the special purpose LO register to a GPR.
 //
 def Mflo16: FRR16_M_ins<0b10010, "mflo", IIM16Alu> {
   let Uses = [LO0];
   let hasSideEffects = 0;
   let isMoveReg = 0;
 }
 
-//
-// Pseudo Instruction for mult
-//
-def MultRxRy16:  FMULT16_ins<"mult",  IIM16Alu> {
-  let isCommutable = 1;
-  let hasSideEffects = 0;
-  let Defs = [HI0, LO0];
-}
-
-def MultuRxRy16: FMULT16_ins<"multu", IIM16Alu> {
-  let isCommutable = 1;
-  let hasSideEffects = 0;
-  let Defs = [HI0, LO0];
-}
-
 //
 // Format: MULT rx, ry MIPS16e
 // Purpose: Multiply Word
 // To multiply 32-bit signed integers.
 //
-def MultRxRyRz16: FMULT16_LO_ins<"mult", IIM16Alu> {
-  let isCommutable = 1;
-  let hasSideEffects = 0;
+def MultRxRy16: FRR16_mult_ins<0b11000, "mult", IIM16Alu> {
+  let isCommutable = 1;
   let Defs = [HI0, LO0];
 }
 
 //
 // Format: MULTU rx, ry MIPS16e
 // Purpose: Multiply Unsigned Word
 // To multiply 32-bit unsigned integers.
 //
-def MultuRxRyRz16: FMULT16_LO_ins<"multu", IIM16Alu> {
-  let isCommutable = 1;
-  let hasSideEffects = 0;
+def MultuRxRy16: FRR16_mult_ins<0b11001, "multu", IIM16Alu> {
+  let isCommutable = 1;
   let Defs = [HI0, LO0];
 }
 
+
 //
 // Format: NEG rx, ry MIPS16e
 // Purpose: Negate
 // To negate an integer value.
 //
 def NegRxRy16: FUnaryRR16_ins<0b11101, "neg", IIM16Alu>;
 
 //
 // Format: NOT rx, ry MIPS16e
 // Purpose: Not
 // To complement an integer value
 //
 def NotRxRy16: FUnaryRR16_ins<0b01111, "not", IIM16Alu>;
 
 //
 // Format: OR rx, ry MIPS16e
 // Purpose: Or
 // To do a bitwise logical OR.
 //
 def OrRxRxRy16: FRxRxRy16_ins<0b01101, "or", IIM16Alu>, ArithLogic16Defs<1>;
 
 //
 // Format: RESTORE {ra,}{s0/s1/s0-1,}{framesize}
 // (All args are optional) MIPS16e
 // Purpose: Restore Registers and Deallocate Stack Frame
 // To deallocate a stack frame before exit from a subroutine,
 // restoring return address and static registers, and adjusting
 // stack
 //
 
 def Restore16:
   FI8_SVRS16<0b1, (outs), (ins variable_ops),
              "", [], II_RESTORE >, MayLoad {
   let isCodeGenOnly = 1;
   let Defs = [SP];
   let Uses = [SP];
 }
 
 
 def RestoreX16:
   FI8_SVRS16<0b1, (outs), (ins variable_ops),
              "", [], II_RESTORE >, MayLoad {
   let isCodeGenOnly = 1;
   let Defs = [SP];
   let Uses = [SP];
 }
 
 //
 // Format: SAVE {ra,}{s0/s1/s0-1,}{framesize} (All arguments are optional)
 // MIPS16e
 // Purpose: Save Registers and Set Up Stack Frame
 // To set up a stack frame on entry to a subroutine,
 // saving return address and static registers, and adjusting stack
 //
 def Save16:
   FI8_SVRS16<0b1, (outs), (ins variable_ops),
              "", [], II_SAVE >, MayStore {
   let isCodeGenOnly = 1;
   let Uses = [SP];
   let Defs = [SP];
 }
 
 def SaveX16:
   FI8_SVRS16<0b1, (outs), (ins variable_ops),
              "", [], II_SAVE >, MayStore {
   let isCodeGenOnly = 1;
   let Uses = [SP];
   let Defs = [SP];
 }
 //
 // Format: SB ry, offset(rx) MIPS16e
 // Purpose: Store Byte (Extended)
 // To store a byte to memory.
 //
 def SbRxRyOffMemX16:
   FEXT_RRI16_mem2_ins<0b11000, "sb", mem16, II_SB>, MayStore;
 
 //
 // Format: SEB rx MIPS16e
 // Purpose: Sign-Extend Byte
 // Sign-extend least significant byte in register rx.
 //
 def SebRx16
   : FRR_SF16_ins<0b10001, 0b100, "seb", IIM16Alu>;
 
 //
 // Format: SEH rx MIPS16e
 // Purpose: Sign-Extend Halfword
 // Sign-extend least significant word in register rx.
 //
 def SehRx16
   : FRR_SF16_ins<0b10001, 0b101, "seh", IIM16Alu>;
 
 //
 // The Sel(T) instructions are pseudos
 // T means that they use T8 implicitly.
 //
 //
 // Format: SelBeqZ rd, rs, rt
 // Purpose: if rt==0, do nothing
 //          else rs = rt
 //
 def SelBeqZ: Sel<"beqz">;
 
 //
 // Format:  SelTBteqZCmp rd, rs, rl, rr
 // Purpose: b = Cmp rl, rr.
 //          If b==0 then do nothing.
 //          if b!=0 then rd = rs
 //
 def SelTBteqZCmp: SelT<"bteqz", "cmp">;
 
 //
 // Format:  SelTBteqZCmpi rd, rs, rl, rr
 // Purpose: b = Cmpi rl, imm.
 //          If b==0 then do nothing.
 //          if b!=0 then rd = rs
 //
 def SelTBteqZCmpi: SeliT<"bteqz", "cmpi">;
 
 //
 // Format:  SelTBteqZSlt rd, rs, rl, rr
 // Purpose: b = Slt rl, rr.
 //          If b==0 then do nothing.
 //          if b!=0 then rd = rs
 //
 def SelTBteqZSlt: SelT<"bteqz", "slt">;
 
 //
 // Format:  SelTBteqZSlti rd, rs, rl, rr
 // Purpose: b = Slti rl, imm.
 //          If b==0 then do nothing.
 //          if b!=0 then rd = rs
 //
 def SelTBteqZSlti: SeliT<"bteqz", "slti">;
 
 //
 // Format:  SelTBteqZSltu rd, rs, rl, rr
 // Purpose: b = Sltu rl, rr.
 //          If b==0 then do nothing.
 //          if b!=0 then rd = rs
 //
 def SelTBteqZSltu: SelT<"bteqz", "sltu">;
 
 //
 // Format:  SelTBteqZSltiu rd, rs, rl, rr
 // Purpose: b = Sltiu rl, imm.
 //          If b==0 then do nothing.
 //          if b!=0 then rd = rs
 //
 def SelTBteqZSltiu: SeliT<"bteqz", "sltiu">;
 
 //
 // Format: SelBnez rd, rs, rt
 // Purpose: if rt!=0, do nothing
 //          else rs = rt
 //
 def SelBneZ: Sel<"bnez">;
 
 //
 // Format:  SelTBtneZCmp rd, rs, rl, rr
 // Purpose: b = Cmp rl, rr.
 //          If b!=0 then do nothing.
 //          if b0=0 then rd = rs
 //
 def SelTBtneZCmp: SelT<"btnez", "cmp">;
 
 //
 // Format:  SelTBtnezCmpi rd, rs, rl, rr
 // Purpose: b = Cmpi rl, imm.
 //          If b!=0 then do nothing.
 //          if b==0 then rd = rs
 //
 def SelTBtneZCmpi: SeliT<"btnez", "cmpi">;
 
 //
 // Format:  SelTBtneZSlt rd, rs, rl, rr
 // Purpose: b = Slt rl, rr.
 //          If b!=0 then do nothing.
 //          if b==0 then rd = rs
 //
 def SelTBtneZSlt: SelT<"btnez", "slt">;
 
 //
 // Format:  SelTBtneZSlti rd, rs, rl, rr
 // Purpose: b = Slti rl, imm.
 //          If b!=0 then do nothing.
 //          if b==0 then rd = rs
 //
 def SelTBtneZSlti: SeliT<"btnez", "slti">;
 
 //
 // Format:  SelTBtneZSltu rd, rs, rl, rr
 // Purpose: b = Sltu rl, rr.
 //          If b!=0 then do nothing.
 //          if b==0 then rd = rs
 //
 def SelTBtneZSltu: SelT<"btnez", "sltu">;
 
 //
 // Format:  SelTBtneZSltiu rd, rs, rl, rr
 // Purpose: b = Slti rl, imm.
 //          If b!=0 then do nothing.
 //          if b==0 then rd = rs
 //
 def SelTBtneZSltiu: SeliT<"btnez", "sltiu">;
 //
 //
 // Format: SH ry, offset(rx) MIPS16e
 // Purpose: Store Halfword (Extended)
 // To store a halfword to memory.
 //
 def ShRxRyOffMemX16:
   FEXT_RRI16_mem2_ins<0b11001, "sh", mem16, II_SH>, MayStore;
 
 //
 // Format: SLL rx, ry, sa MIPS16e
 // Purpose: Shift Word Left Logical (Extended)
 // To execute a left-shift of a word by a fixed number of bits-0 to 31 bits.
 //
 def SllX16: FEXT_SHIFT16_ins<0b00, "sll", IIM16Alu>;
 
 //
 // Format: SLLV ry, rx MIPS16e
 // Purpose: Shift Word Left Logical Variable
 // To execute a left-shift of a word by a variable number of bits.
 //
 def SllvRxRy16 : FRxRxRy16_ins<0b00100, "sllv", IIM16Alu>;
 
 // Format: SLTI rx, immediate MIPS16e
 // Purpose: Set on Less Than Immediate
 // To record the result of a less-than comparison with a constant.
 //
 //
 def SltiRxImm16: FRI16R_ins<0b01010, "slti", IIM16Alu> {
   let Defs = [T8];
 }
 
 //
 // Format: SLTI rx, immediate MIPS16e
 // Purpose: Set on Less Than Immediate (Extended)
 // To record the result of a less-than comparison with a constant.
 //
 //
 def SltiRxImmX16: FEXT_RI16R_ins<0b01010, "slti", IIM16Alu> {
   let Defs = [T8];
 }
 
 def SltiCCRxImmX16: FEXT_CCRXI16_ins<"slti">;
 
 // Format: SLTIU rx, immediate MIPS16e
 // Purpose: Set on Less Than Immediate Unsigned
 // To record the result of a less-than comparison with a constant.
 //
 //
 def SltiuRxImm16: FRI16R_ins<0b01011, "sltiu", IIM16Alu> {
   let Defs = [T8];
 }
 
 //
 // Format: SLTI rx, immediate MIPS16e
 // Purpose: Set on Less Than Immediate Unsigned (Extended)
 // To record the result of a less-than comparison with a constant.
 //
 //
 def SltiuRxImmX16: FEXT_RI16R_ins<0b01011, "sltiu", IIM16Alu> {
   let Defs = [T8];
 }
 //
 // Format: SLTIU rx, immediate MIPS16e
 // Purpose: Set on Less Than Immediate Unsigned (Extended)
 // To record the result of a less-than comparison with a constant.
 //
 def SltiuCCRxImmX16: FEXT_CCRXI16_ins<"sltiu">;
 
 //
 // Format: SLT rx, ry MIPS16e
 // Purpose: Set on Less Than
 // To record the result of a less-than comparison.
 //
 def SltRxRy16: FRR16R_ins<0b00010, "slt", IIM16Alu>{
   let Defs = [T8];
 }
 
 def SltCCRxRy16: FCCRR16_ins<"slt">;
 
 // Format: SLTU rx, ry MIPS16e
 // Purpose: Set on Less Than Unsigned
 // To record the result of an unsigned less-than comparison.
 //
 def SltuRxRy16: FRR16R_ins<0b00011, "sltu", IIM16Alu>{
   let Defs = [T8];
 }
 
 def SltuRxRyRz16: FRRTR16_ins<"sltu"> {
   let isCodeGenOnly=1;
   let Defs = [T8];
 }
 
 
 def SltuCCRxRy16: FCCRR16_ins<"sltu">;
 //
 // Format: SRAV ry, rx MIPS16e
 // Purpose: Shift Word Right Arithmetic Variable
 // To execute an arithmetic right-shift of a word by a variable
 // number of bits.
 //
 def SravRxRy16: FRxRxRy16_ins<0b00111, "srav", IIM16Alu>;
 
 
 //
 // Format: SRA rx, ry, sa MIPS16e
 // Purpose: Shift Word Right Arithmetic (Extended)
 // To execute an arithmetic right-shift of a word by a fixed
 // number of bits-1 to 8 bits.
 //
 def SraX16: FEXT_SHIFT16_ins<0b11, "sra", IIM16Alu>;
 
 
 //
 // Format: SRLV ry, rx MIPS16e
 // Purpose: Shift Word Right Logical Variable
 // To execute a logical right-shift of a word by a variable
 // number of bits.
 //
 def SrlvRxRy16: FRxRxRy16_ins<0b00110, "srlv", IIM16Alu>;
 
 
 //
 // Format: SRL rx, ry, sa MIPS16e
 // Purpose: Shift Word Right Logical (Extended)
 // To execute a logical right-shift of a word by a fixed
 // number of bits-1 to 31 bits.
 //
 def SrlX16: FEXT_SHIFT16_ins<0b10, "srl", IIM16Alu>;
 
 //
 // Format: SUBU rz, rx, ry MIPS16e
 // Purpose: Subtract Unsigned Word
 // To subtract 32-bit integers
 //
 def SubuRxRyRz16: FRRR16_ins<0b11, "subu", IIM16Alu>, ArithLogic16Defs<0>;
 
 //
 // Format: SW ry, offset(rx) MIPS16e
 // Purpose: Store Word (Extended)
 // To store a word to memory.
 //
 def SwRxRyOffMemX16: FEXT_RRI16_mem2_ins<0b11011, "sw", mem16, II_SW>, MayStore;
 
 //
 // Format: SW rx, offset(sp) MIPS16e
 // Purpose: Store Word rx (SP-Relative)
 // To store an SP-relative word to memory.
 //
 def SwRxSpImmX16: FEXT_RRI16_mem2_ins<0b11010, "sw", mem16sp, II_SW>, MayStore;
 
 //
 //
 // Format: XOR rx, ry MIPS16e
 // Purpose: Xor
 // To do a bitwise logical XOR.
 //
 def XorRxRxRy16: FRxRxRy16_ins<0b01110, "xor", IIM16Alu>, ArithLogic16Defs<1>;
 
 class Mips16Pat<dag pattern, dag result> : Pat<pattern, result> {
   let Predicates = [InMips16Mode];
 }
 
 // Unary Arith/Logic
 //
 class ArithLogicU_pat<PatFrag OpNode, Instruction I> :
   Mips16Pat<(OpNode CPU16Regs:$r),
             (I CPU16Regs:$r)>;
 
 def: ArithLogicU_pat<not, NotRxRy16>;
 def: ArithLogicU_pat<ineg, NegRxRy16>;
 
 class ArithLogic16_pat<SDNode OpNode, Instruction I> :
   Mips16Pat<(OpNode CPU16Regs:$l, CPU16Regs:$r),
             (I CPU16Regs:$l, CPU16Regs:$r)>;
 
 def: ArithLogic16_pat<add, AdduRxRyRz16>;
 def: ArithLogic16_pat<and, AndRxRxRy16>;
-def: ArithLogic16_pat<mul, MultRxRyRz16>;
 def: ArithLogic16_pat<or, OrRxRxRy16>;
 def: ArithLogic16_pat<sub, SubuRxRyRz16>;
 def: ArithLogic16_pat<xor, XorRxRxRy16>;
 
 // Arithmetic and logical instructions with 2 register operands.
 
 class ArithLogicI16_pat<SDNode OpNode, PatFrag imm_type, Instruction I> :
   Mips16Pat<(OpNode CPU16Regs:$in, imm_type:$imm),
             (I CPU16Regs:$in, imm_type:$imm)>;
 
 def: ArithLogicI16_pat<add, immSExt8, AddiuRxRxImm16>;
 def: ArithLogicI16_pat<add, immSExt16, AddiuRxRxImmX16>;
 def: ArithLogicI16_pat<shl, immZExt5, SllX16>;
 def: ArithLogicI16_pat<srl, immZExt5, SrlX16>;
 def: ArithLogicI16_pat<sra, immZExt5, SraX16>;
 
 class shift_rotate_reg16_pat<SDNode OpNode, Instruction I> :
   Mips16Pat<(OpNode CPU16Regs:$r, CPU16Regs:$ra),
             (I CPU16Regs:$r, CPU16Regs:$ra)>;
 
 def: shift_rotate_reg16_pat<shl, SllvRxRy16>;
 def: shift_rotate_reg16_pat<sra, SravRxRy16>;
 def: shift_rotate_reg16_pat<srl, SrlvRxRy16>;
 
 class LoadM16_pat<PatFrag OpNode, Instruction I, ComplexPattern Addr> :
   Mips16Pat<(OpNode Addr:$addr), (I Addr:$addr)>;
 
 def: LoadM16_pat<sextloadi8, LbRxRyOffMemX16, addr16>;
 def: LoadM16_pat<zextloadi8, LbuRxRyOffMemX16, addr16>;
 def: LoadM16_pat<sextloadi16, LhRxRyOffMemX16, addr16>;
 def: LoadM16_pat<zextloadi16, LhuRxRyOffMemX16, addr16>;
 def: LoadM16_pat<load, LwRxSpImmX16, addr16sp>;
 
 class StoreM16_pat<PatFrag OpNode, Instruction I, ComplexPattern Addr> :
   Mips16Pat<(OpNode CPU16Regs:$r, Addr:$addr), (I CPU16Regs:$r, Addr:$addr)>;
 
 def: StoreM16_pat<truncstorei8, SbRxRyOffMemX16, addr16>;
 def: StoreM16_pat<truncstorei16, ShRxRyOffMemX16, addr16>;
 def: StoreM16_pat<store, SwRxSpImmX16, addr16sp>;
 
 // Unconditional branch
 class UncondBranch16_pat<SDNode OpNode, Instruction I>:
   Mips16Pat<(OpNode bb:$imm16), (I bb:$imm16)> {
     let Predicates = [InMips16Mode];
   }
 
 def : Mips16Pat<(MipsJmpLink (i32 tglobaladdr:$dst)),
                 (Jal16 tglobaladdr:$dst)>;
 
 def : Mips16Pat<(MipsJmpLink (i32 texternalsym:$dst)),
                 (Jal16 texternalsym:$dst)>;
 
 // Indirect branch
 def: Mips16Pat<(brind CPU16Regs:$rs), (JrcRx16 CPU16Regs:$rs)> {
   // Ensure that the addition of MIPS32r6/MIPS64r6 support does not change
   // MIPS16's behaviour.
   let AddedComplexity = 1;
 }
 
 // Jump and Link (Call)
 let isCall=1, hasDelaySlot=0 in
 def JumpLinkReg16:
   FRR16_JALRC<0, 0, 0, (outs), (ins CPU16Regs:$rs),
               "jalrc\t$rs", [(MipsJmpLink CPU16Regs:$rs)], II_JALRC> {
   let Defs = [RA];
 }
 
 // Mips16 pseudos
 let isReturn=1, isTerminator=1, hasDelaySlot=1, isBarrier=1, hasCtrlDep=1,
   hasExtraSrcRegAllocReq = 1 in
 def RetRA16 : MipsPseudo16<(outs), (ins), "", [(MipsRet)]>;
 
 
 // setcc patterns
 
 class SetCC_R16<PatFrag cond_op, Instruction I>:
   Mips16Pat<(cond_op CPU16Regs:$rx, CPU16Regs:$ry),
             (I CPU16Regs:$rx, CPU16Regs:$ry)>;
 
 class SetCC_I16<PatFrag cond_op, PatLeaf imm_type, Instruction I>:
   Mips16Pat<(cond_op CPU16Regs:$rx, imm_type:$imm16),
             (I CPU16Regs:$rx, imm_type:$imm16)>;
 
 
 def: Mips16Pat<(i32 addr16sp:$addr), (AddiuRxRyOffMemX16 addr16sp:$addr)>;
 
 
 // Large (>16 bit) immediate loads
 def : Mips16Pat<(i32 imm:$imm), (LwConstant32 imm:$imm, -1)>;
 
 //
 // Some branch conditional patterns are not generated by llvm at this time.
 // Some are for seemingly arbitrary reasons not used: i.e. with signed number
 // comparison they are used and for unsigned a different pattern is used.
 // I am pushing upstream from the full mips16 port and it seemed that I needed
 // these earlier and the mips32 port has these but now I cannot create test
 // cases that use these patterns. While I sort this all out I will leave these
 // extra patterns commented out and if I can be sure they are really not used,
 // I will delete the code. I don't want to check the code in uncommented without
 // a valid test case. In some cases, the compiler is generating patterns with
 // setcc instead and earlier I had implemented setcc first so may have masked
 // the problem. The setcc variants are suboptimal for mips16 so I may wantto
 // figure out how to enable the brcond patterns or else possibly new
 // combinations of brcond and setcc.
 //
 //
 // bcond-seteq
 //
 def: Mips16Pat
   <(brcond (i32 (seteq CPU16Regs:$rx, CPU16Regs:$ry)), bb:$imm16),
    (BteqzT8CmpX16 CPU16Regs:$rx, CPU16Regs:$ry,  bb:$imm16)
   >;
 
 
 def: Mips16Pat
   <(brcond (i32 (seteq CPU16Regs:$rx, immZExt16:$imm)), bb:$targ16),
    (BteqzT8CmpiX16 CPU16Regs:$rx, immSExt16:$imm,  bb:$targ16)
   >;
 
 def: Mips16Pat
   <(brcond (i32 (seteq CPU16Regs:$rx, 0)), bb:$targ16),
    (BeqzRxImm16 CPU16Regs:$rx, bb:$targ16)
   >;
 
 //
 // bcond-setgt (do we need to have this pair of setlt, setgt??)
 //
 def: Mips16Pat
   <(brcond (i32 (setgt CPU16Regs:$rx, CPU16Regs:$ry)), bb:$imm16),
    (BtnezT8SltX16 CPU16Regs:$ry, CPU16Regs:$rx,  bb:$imm16)
   >;
 
 //
 // bcond-setge
 //
 def: Mips16Pat
   <(brcond (i32 (setge CPU16Regs:$rx, CPU16Regs:$ry)), bb:$imm16),
    (BteqzT8SltX16 CPU16Regs:$rx, CPU16Regs:$ry,  bb:$imm16)
   >;
 
 //
 // never called because compiler transforms a >= k to a > (k-1)
 def: Mips16Pat
   <(brcond (i32 (setge CPU16Regs:$rx, immSExt16:$imm)), bb:$imm16),
    (BteqzT8SltiX16 CPU16Regs:$rx, immSExt16:$imm,  bb:$imm16)
   >;
 
 //
 // bcond-setlt
 //
 def: Mips16Pat
   <(brcond (i32 (setlt CPU16Regs:$rx, CPU16Regs:$ry)), bb:$imm16),
    (BtnezT8SltX16 CPU16Regs:$rx, CPU16Regs:$ry,  bb:$imm16)
   >;
 
 def: Mips16Pat
   <(brcond (i32 (setlt CPU16Regs:$rx, immSExt16:$imm)), bb:$imm16),
    (BtnezT8SltiX16 CPU16Regs:$rx, immSExt16:$imm,  bb:$imm16)
   >;
 
 //
 // bcond-setle
 //
 def: Mips16Pat
   <(brcond (i32 (setle CPU16Regs:$rx, CPU16Regs:$ry)), bb:$imm16),
    (BteqzT8SltX16 CPU16Regs:$ry, CPU16Regs:$rx,  bb:$imm16)
   >;
 
 //
 // bcond-setne
 //
 def: Mips16Pat
   <(brcond (i32 (setne CPU16Regs:$rx, CPU16Regs:$ry)), bb:$imm16),
    (BtnezT8CmpX16 CPU16Regs:$rx, CPU16Regs:$ry,  bb:$imm16)
   >;
 
 def: Mips16Pat
   <(brcond (i32 (setne CPU16Regs:$rx, immZExt16:$imm)), bb:$targ16),
    (BtnezT8CmpiX16 CPU16Regs:$rx, immSExt16:$imm,  bb:$targ16)
   >;
 
 def: Mips16Pat
   <(brcond (i32 (setne CPU16Regs:$rx, 0)), bb:$targ16),
    (BnezRxImm16 CPU16Regs:$rx, bb:$targ16)
   >;
 
 //
 // This needs to be there but I forget which code will generate it
 //
 def: Mips16Pat
   <(brcond CPU16Regs:$rx, bb:$targ16),
    (BnezRxImm16 CPU16Regs:$rx, bb:$targ16)
   >;
 
 //
 
 //
 // bcond-setugt
 //
 //def: Mips16Pat
 //  <(brcond (i32 (setugt CPU16Regs:$rx, CPU16Regs:$ry)), bb:$imm16),
 //   (BtnezT8SltuX16 CPU16Regs:$ry, CPU16Regs:$rx,  bb:$imm16)
 //  >;
 
 //
 // bcond-setuge
 //
 //def: Mips16Pat
 //  <(brcond (i32 (setuge CPU16Regs:$rx, CPU16Regs:$ry)), bb:$imm16),
 //   (BteqzT8SltuX16 CPU16Regs:$rx, CPU16Regs:$ry,  bb:$imm16)
 //  >;
 
 
 //
 // bcond-setult
 //
 //def: Mips16Pat
 //  <(brcond (i32 (setult CPU16Regs:$rx, CPU16Regs:$ry)), bb:$imm16),
 //   (BtnezT8SltuX16 CPU16Regs:$rx, CPU16Regs:$ry,  bb:$imm16)
 //  >;
 
 def: UncondBranch16_pat<br, Bimm16>;
 
 // Small immediates
 def: Mips16Pat<(i32 immSExt16:$in),
                (AddiuRxRxImmX16 (MoveR3216 ZERO), immSExt16:$in)>;
 
 def: Mips16Pat<(i32 immZExt16:$in), (LiRxImmX16 immZExt16:$in)>;
 
 //
 // MipsDivRem
 //
 def: Mips16Pat
   <(MipsDivRem16 CPU16Regs:$rx, CPU16Regs:$ry),
    (DivRxRy16 CPU16Regs:$rx, CPU16Regs:$ry)>;
 
 //
 // MipsDivRemU
 //
 def: Mips16Pat
   <(MipsDivRemU16 CPU16Regs:$rx, CPU16Regs:$ry),
    (DivuRxRy16 CPU16Regs:$rx, CPU16Regs:$ry)>;
 
+//
+// MipsMult
+//
+def: Mips16Pat
+  <(MipsMult16 CPU16Regs:$rx, CPU16Regs:$ry),
+   (MultRxRy16 CPU16Regs:$rx, CPU16Regs:$ry)>;
+
+//
+// MipsMultu
+//
+def: Mips16Pat
+  <(MipsMultu16 CPU16Regs:$rx, CPU16Regs:$ry),
+   (MultuRxRy16 CPU16Regs:$rx, CPU16Regs:$ry)>;
+
 //  signed a,b
 //  x = (a>=b)?x:y
 //
 //  if !(a < b) x = y
 //
 def : Mips16Pat<(select (i32 (setge CPU16Regs:$a, CPU16Regs:$b)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelTBteqZSlt CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$a, CPU16Regs:$b)>;
 
 //  signed a,b
 //  x = (a>b)?x:y
 //
 //  if  (b < a) x = y
 //
 def : Mips16Pat<(select (i32 (setgt CPU16Regs:$a, CPU16Regs:$b)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelTBtneZSlt CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$b, CPU16Regs:$a)>;
 
 // unsigned a,b
 // x = (a>=b)?x:y
 //
 // if !(a < b) x = y;
 //
 def : Mips16Pat<
   (select (i32 (setuge CPU16Regs:$a, CPU16Regs:$b)),
    CPU16Regs:$x, CPU16Regs:$y),
   (SelTBteqZSltu CPU16Regs:$x, CPU16Regs:$y,
    CPU16Regs:$a, CPU16Regs:$b)>;
 
 //  unsigned a,b
 //  x = (a>b)?x:y
 //
 //  if (b < a) x = y
 //
 def : Mips16Pat<(select (i32 (setugt CPU16Regs:$a, CPU16Regs:$b)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelTBtneZSltu CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$b, CPU16Regs:$a)>;
 
 // signed
 // x = (a >= k)?x:y
 // due to an llvm optimization, i don't think that this will ever
 // be used. This is transformed into x = (a > k-1)?x:y
 //
 //
 
 //def : Mips16Pat<
 //  (select (i32 (setge CPU16Regs:$lhs, immSExt16:$rhs)),
 //   CPU16Regs:$T, CPU16Regs:$F),
 //  (SelTBteqZSlti CPU16Regs:$T, CPU16Regs:$F,
 //   CPU16Regs:$lhs, immSExt16:$rhs)>;
 
 //def : Mips16Pat<
 //  (select (i32 (setuge CPU16Regs:$lhs, immSExt16:$rhs)),
 //   CPU16Regs:$T, CPU16Regs:$F),
 //  (SelTBteqZSltiu CPU16Regs:$T, CPU16Regs:$F,
 //   CPU16Regs:$lhs, immSExt16:$rhs)>;
 
 // signed
 // x = (a < k)?x:y
 //
 // if !(a < k) x = y;
 //
 def : Mips16Pat<
   (select (i32 (setlt CPU16Regs:$a, immSExt16:$b)),
    CPU16Regs:$x, CPU16Regs:$y),
   (SelTBtneZSlti CPU16Regs:$x, CPU16Regs:$y,
    CPU16Regs:$a, immSExt16:$b)>;
 
 
 //
 //
 // signed
 // x = (a <= b)? x : y
 //
 // if  (b < a) x = y
 //
 def : Mips16Pat<(select (i32 (setle CPU16Regs:$a, CPU16Regs:$b)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelTBteqZSlt CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$b, CPU16Regs:$a)>;
 
 //
 // unsigned
 // x = (a <= b)? x : y
 //
 // if  (b < a) x = y
 //
 def : Mips16Pat<(select (i32 (setule CPU16Regs:$a, CPU16Regs:$b)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelTBteqZSltu CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$b, CPU16Regs:$a)>;
 
 //
 // signed/unsigned
 // x = (a == b)? x : y
 //
 // if (a != b) x = y
 //
 def : Mips16Pat<(select (i32 (seteq CPU16Regs:$a, CPU16Regs:$b)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelTBteqZCmp CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$b, CPU16Regs:$a)>;
 
 //
 // signed/unsigned
 // x = (a == 0)? x : y
 //
 // if (a != 0) x = y
 //
 def : Mips16Pat<(select (i32 (seteq CPU16Regs:$a, 0)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelBeqZ CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$a)>;
 
 
 //
 // signed/unsigned
 // x = (a == k)? x : y
 //
 // if (a != k) x = y
 //
 def : Mips16Pat<(select (i32 (seteq CPU16Regs:$a, immZExt16:$k)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelTBteqZCmpi CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$a, immZExt16:$k)>;
 
 
 //
 // signed/unsigned
 // x = (a != b)? x : y
 //
 // if (a == b) x = y
 //
 //
 def : Mips16Pat<(select (i32 (setne CPU16Regs:$a, CPU16Regs:$b)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelTBtneZCmp CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$b, CPU16Regs:$a)>;
 
 //
 // signed/unsigned
 // x = (a != 0)? x : y
 //
 // if (a == 0) x = y
 //
 def : Mips16Pat<(select (i32 (setne CPU16Regs:$a, 0)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelBneZ CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$a)>;
 
 // signed/unsigned
 // x = (a)? x : y
 //
 // if (!a) x = y
 //
 def : Mips16Pat<(select  CPU16Regs:$a,
                  CPU16Regs:$x, CPU16Regs:$y),
       (SelBneZ CPU16Regs:$x, CPU16Regs:$y,
        CPU16Regs:$a)>;
 
 
 //
 // signed/unsigned
 // x = (a != k)? x : y
 //
 // if (a == k) x = y
 //
 def : Mips16Pat<(select (i32 (setne CPU16Regs:$a, immZExt16:$k)),
                  CPU16Regs:$x, CPU16Regs:$y),
                 (SelTBtneZCmpi CPU16Regs:$x, CPU16Regs:$y,
                  CPU16Regs:$a, immZExt16:$k)>;
 
 //
 // When writing C code to test setxx these patterns,
 // some will be transformed into
 // other things. So we test using C code but using -O3 and -O0
 //
 // seteq
 //
 def : Mips16Pat
   <(seteq CPU16Regs:$lhs,CPU16Regs:$rhs),
    (SltiuCCRxImmX16 (XorRxRxRy16 CPU16Regs:$lhs, CPU16Regs:$rhs), 1)>;
 
 def : Mips16Pat
   <(seteq CPU16Regs:$lhs, 0),
    (SltiuCCRxImmX16 CPU16Regs:$lhs, 1)>;
 
 
 //
 // setge
 //
 
 def: Mips16Pat
   <(setge CPU16Regs:$lhs, CPU16Regs:$rhs),
    (XorRxRxRy16 (SltCCRxRy16 CPU16Regs:$lhs, CPU16Regs:$rhs),
    (LiRxImmX16 1))>;
 
 //
 // For constants, llvm transforms this to:
 // x > (k - 1) and then reverses the operands to use setlt. So this pattern
 // is not used now by the compiler. (Presumably checking that k-1 does not
 // overflow). The compiler never uses this at the current time, due to
 // other optimizations.
 //
 //def: Mips16Pat
 //  <(setge CPU16Regs:$lhs, immSExt16:$rhs),
 //   (XorRxRxRy16 (SltiCCRxImmX16 CPU16Regs:$lhs, immSExt16:$rhs),
 //   (LiRxImmX16 1))>;
 
 // This catches the x >= -32768 case by transforming it to  x > -32769
 //
 def: Mips16Pat
   <(setgt CPU16Regs:$lhs, -32769),
    (XorRxRxRy16 (SltiCCRxImmX16 CPU16Regs:$lhs, -32768),
    (LiRxImmX16 1))>;
 
 //
 // setgt
 //
 //
 
 def: Mips16Pat
   <(setgt CPU16Regs:$lhs, CPU16Regs:$rhs),
    (SltCCRxRy16 CPU16Regs:$rhs, CPU16Regs:$lhs)>;
 
 //
 // setle
 //
 def: Mips16Pat
   <(setle CPU16Regs:$lhs, CPU16Regs:$rhs),
    (XorRxRxRy16 (SltCCRxRy16 CPU16Regs:$rhs, CPU16Regs:$lhs), (LiRxImm16 1))>;
 
 //
 // setlt
 //
 def: SetCC_R16<setlt, SltCCRxRy16>;
 
 def: SetCC_I16<setlt, immSExt16, SltiCCRxImmX16>;
 
 //
 // setne
 //
 def : Mips16Pat
   <(setne CPU16Regs:$lhs,CPU16Regs:$rhs),
    (SltuCCRxRy16 (LiRxImmX16 0),
    (XorRxRxRy16 CPU16Regs:$lhs, CPU16Regs:$rhs))>;
 
 
 //
 // setuge
 //
 def: Mips16Pat
   <(setuge CPU16Regs:$lhs, CPU16Regs:$rhs),
    (XorRxRxRy16 (SltuCCRxRy16 CPU16Regs:$lhs, CPU16Regs:$rhs),
    (LiRxImmX16 1))>;
 
 // this pattern will never be used because the compiler will transform
 // x >= k to x > (k - 1) and then use SLT
 //
 //def: Mips16Pat
 //  <(setuge CPU16Regs:$lhs, immZExt16:$rhs),
 //   (XorRxRxRy16 (SltiuCCRxImmX16 CPU16Regs:$lhs, immZExt16:$rhs),
 //   (LiRxImmX16 1))>;
 
 //
 // setugt
 //
 def: Mips16Pat
   <(setugt CPU16Regs:$lhs, CPU16Regs:$rhs),
    (SltuCCRxRy16 CPU16Regs:$rhs, CPU16Regs:$lhs)>;
 
 //
 // setule
 //
 def: Mips16Pat
   <(setule CPU16Regs:$lhs, CPU16Regs:$rhs),
    (XorRxRxRy16 (SltuCCRxRy16 CPU16Regs:$rhs, CPU16Regs:$lhs), (LiRxImmX16 1))>;
 
 //
 // setult
 //
 def: SetCC_R16<setult, SltuCCRxRy16>;
 
 def: SetCC_I16<setult, immSExt16, SltiuCCRxImmX16>;
 
 def: Mips16Pat<(add CPU16Regs:$hi, (MipsLo tglobaladdr:$lo)),
                (AddiuRxRxImmX16 CPU16Regs:$hi, tglobaladdr:$lo)>;
 
 // hi/lo relocs
 def : Mips16Pat<(MipsHi tblockaddress:$in),
                 (SllX16 (LiRxImmX16 tblockaddress:$in), 16)>;
 def : Mips16Pat<(MipsHi tglobaladdr:$in),
                 (SllX16 (LiRxImmX16 tglobaladdr:$in), 16)>;
 def : Mips16Pat<(MipsHi tjumptable:$in),
                 (SllX16 (LiRxImmX16 tjumptable:$in), 16)>;
 
 def : Mips16Pat<(MipsLo tblockaddress:$in), (LiRxImmX16 tblockaddress:$in)>;
 
 def : Mips16Pat<(MipsTlsHi tglobaltlsaddr:$in),
                 (SllX16 (LiRxImmX16 tglobaltlsaddr:$in), 16)>;
 
 // wrapper_pic
 class Wrapper16Pat<SDNode node, Instruction ADDiuOp, RegisterClass RC>:
   Mips16Pat<(MipsWrapper RC:$gp, node:$in),
             (ADDiuOp RC:$gp, node:$in)>;
 
 
 def : Wrapper16Pat<tglobaladdr, AddiuRxRxImmX16, CPU16Regs>;
 def : Wrapper16Pat<tglobaltlsaddr, AddiuRxRxImmX16, CPU16Regs>;
 
 def : Mips16Pat<(i32 (extloadi8   addr16:$src)),
                 (LbuRxRyOffMemX16  addr16:$src)>;
 def : Mips16Pat<(i32 (extloadi16  addr16:$src)),
                 (LhuRxRyOffMemX16  addr16:$src)>;
 
 def: Mips16Pat<(trap), (Break16)>;
 
 def : Mips16Pat<(sext_inreg CPU16Regs:$val, i8),
                 (SebRx16 CPU16Regs:$val)>;
 
 def : Mips16Pat<(sext_inreg CPU16Regs:$val, i16),
                 (SehRx16 CPU16Regs:$val)>;
 
 def GotPrologue16:
   MipsPseudo16<
     (outs CPU16Regs:$rh, CPU16Regs:$rl),
     (ins simm16:$immHi, simm16:$immLo),
     "li\t$rh, $immHi\n\taddiu\t$rl, $$pc, $immLo\n ",[]> ;
 
 // An operand for the CONSTPOOL_ENTRY pseudo-instruction.
 def cpinst_operand : Operand<i32> {
   // let PrintMethod = "printCPInstOperand";
 }
 
 // CONSTPOOL_ENTRY - This instruction represents a floating constant pool in
 // the function.  The first operand is the ID# for this instruction, the second
 // is the index into the MachineConstantPool that this is, the third is the
 // size in bytes of this constant pool entry.
 //
 let hasSideEffects = 0, isNotDuplicable = 1 in
 def CONSTPOOL_ENTRY :
 MipsPseudo16<(outs), (ins cpinst_operand:$instid, cpinst_operand:$cpidx,
                       i32imm:$size), "foo", []>;
 
 // Instruction Aliases
 
 let EncodingPredicates = [InMips16Mode] in
 def : MipsInstAlias<"nop", (Move32R16 ZERO, S0)>;
diff --git a/llvm/lib/Target/Mips/MipsISelLowering.cpp b/llvm/lib/Target/Mips/MipsISelLowering.cpp
index 7d5c71980c49..a77f3eb20fe0 100644
--- a/llvm/lib/Target/Mips/MipsISelLowering.cpp
+++ b/llvm/lib/Target/Mips/MipsISelLowering.cpp
@@ -1,5002 +1,5004 @@
 //===- MipsISelLowering.cpp - Mips DAG Lowering Implementation ------------===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
 //
 // This file defines the interfaces that Mips uses to lower LLVM code into a
 // selection DAG.
 //
 //===----------------------------------------------------------------------===//
 
 #include "MipsISelLowering.h"
 #include "MCTargetDesc/MipsBaseInfo.h"
 #include "MCTargetDesc/MipsInstPrinter.h"
 #include "MCTargetDesc/MipsMCTargetDesc.h"
 #include "MipsCCState.h"
 #include "MipsInstrInfo.h"
 #include "MipsMachineFunction.h"
 #include "MipsRegisterInfo.h"
 #include "MipsSubtarget.h"
 #include "MipsTargetMachine.h"
 #include "MipsTargetObjectFile.h"
 #include "llvm/ADT/APFloat.h"
 #include "llvm/ADT/ArrayRef.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/Statistic.h"
 #include "llvm/ADT/StringRef.h"
 #include "llvm/ADT/StringSwitch.h"
 #include "llvm/CodeGen/CallingConvLower.h"
 #include "llvm/CodeGen/FunctionLoweringInfo.h"
 #include "llvm/CodeGen/ISDOpcodes.h"
 #include "llvm/CodeGen/MachineBasicBlock.h"
 #include "llvm/CodeGen/MachineFrameInfo.h"
 #include "llvm/CodeGen/MachineFunction.h"
 #include "llvm/CodeGen/MachineInstr.h"
 #include "llvm/CodeGen/MachineInstrBuilder.h"
 #include "llvm/CodeGen/MachineJumpTableInfo.h"
 #include "llvm/CodeGen/MachineMemOperand.h"
 #include "llvm/CodeGen/MachineOperand.h"
 #include "llvm/CodeGen/MachineRegisterInfo.h"
 #include "llvm/CodeGen/RuntimeLibcalls.h"
 #include "llvm/CodeGen/SelectionDAG.h"
 #include "llvm/CodeGen/SelectionDAGNodes.h"
 #include "llvm/CodeGen/TargetFrameLowering.h"
 #include "llvm/CodeGen/TargetInstrInfo.h"
 #include "llvm/CodeGen/TargetRegisterInfo.h"
 #include "llvm/CodeGen/ValueTypes.h"
 #include "llvm/IR/CallingConv.h"
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/DataLayout.h"
 #include "llvm/IR/DebugLoc.h"
 #include "llvm/IR/DerivedTypes.h"
 #include "llvm/IR/Function.h"
 #include "llvm/IR/GlobalValue.h"
 #include "llvm/IR/Type.h"
 #include "llvm/IR/Value.h"
 #include "llvm/MC/MCContext.h"
 #include "llvm/MC/MCRegisterInfo.h"
 #include "llvm/Support/Casting.h"
 #include "llvm/Support/CodeGen.h"
 #include "llvm/Support/CommandLine.h"
 #include "llvm/Support/Compiler.h"
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/MachineValueType.h"
 #include "llvm/Support/MathExtras.h"
 #include "llvm/Target/TargetMachine.h"
 #include "llvm/Target/TargetOptions.h"
 #include <algorithm>
 #include <cassert>
 #include <cctype>
 #include <cstdint>
 #include <deque>
 #include <iterator>
 #include <utility>
 #include <vector>
 
 using namespace llvm;
 
 #define DEBUG_TYPE "mips-lower"
 
 STATISTIC(NumTailCalls, "Number of tail calls");
 
 static cl::opt<bool>
 NoZeroDivCheck("mno-check-zero-division", cl::Hidden,
                cl::desc("MIPS: Don't trap on integer division by zero."),
                cl::init(false));
 
 extern cl::opt<bool> EmitJalrReloc;
 
 static const MCPhysReg Mips64DPRegs[8] = {
   Mips::D12_64, Mips::D13_64, Mips::D14_64, Mips::D15_64,
   Mips::D16_64, Mips::D17_64, Mips::D18_64, Mips::D19_64
 };
 
 // If I is a shifted mask, set the size (Size) and the first bit of the
 // mask (Pos), and return true.
 // For example, if I is 0x003ff800, (Pos, Size) = (11, 11).
 static bool isShiftedMask(uint64_t I, uint64_t &Pos, uint64_t &Size) {
   if (!isShiftedMask_64(I))
     return false;
 
   Size = countPopulation(I);
   Pos = countTrailingZeros(I);
   return true;
 }
 
 // The MIPS MSA ABI passes vector arguments in the integer register set.
 // The number of integer registers used is dependant on the ABI used.
 MVT MipsTargetLowering::getRegisterTypeForCallingConv(LLVMContext &Context,
                                                       CallingConv::ID CC,
                                                       EVT VT) const {
   if (!VT.isVector())
     return getRegisterType(Context, VT);
 
   return Subtarget.isABI_O32() || VT.getSizeInBits() == 32 ? MVT::i32
                                                            : MVT::i64;
 }
 
 unsigned MipsTargetLowering::getNumRegistersForCallingConv(LLVMContext &Context,
                                                            CallingConv::ID CC,
                                                            EVT VT) const {
   if (VT.isVector())
     return divideCeil(VT.getSizeInBits(), Subtarget.isABI_O32() ? 32 : 64);
   return MipsTargetLowering::getNumRegisters(Context, VT);
 }
 
 unsigned MipsTargetLowering::getVectorTypeBreakdownForCallingConv(
     LLVMContext &Context, CallingConv::ID CC, EVT VT, EVT &IntermediateVT,
     unsigned &NumIntermediates, MVT &RegisterVT) const {
   // Break down vector types to either 2 i64s or 4 i32s.
   RegisterVT = getRegisterTypeForCallingConv(Context, CC, VT);
   IntermediateVT = RegisterVT;
   NumIntermediates =
       VT.getFixedSizeInBits() < RegisterVT.getFixedSizeInBits()
           ? VT.getVectorNumElements()
           : divideCeil(VT.getSizeInBits(), RegisterVT.getSizeInBits());
   return NumIntermediates;
 }
 
 SDValue MipsTargetLowering::getGlobalReg(SelectionDAG &DAG, EVT Ty) const {
   MachineFunction &MF = DAG.getMachineFunction();
   MipsFunctionInfo *FI = MF.getInfo<MipsFunctionInfo>();
   return DAG.getRegister(FI->getGlobalBaseReg(MF), Ty);
 }
 
 SDValue MipsTargetLowering::getTargetNode(GlobalAddressSDNode *N, EVT Ty,
                                           SelectionDAG &DAG,
                                           unsigned Flag) const {
   return DAG.getTargetGlobalAddress(N->getGlobal(), SDLoc(N), Ty, 0, Flag);
 }
 
 SDValue MipsTargetLowering::getTargetNode(ExternalSymbolSDNode *N, EVT Ty,
                                           SelectionDAG &DAG,
                                           unsigned Flag) const {
   return DAG.getTargetExternalSymbol(N->getSymbol(), Ty, Flag);
 }
 
 SDValue MipsTargetLowering::getTargetNode(BlockAddressSDNode *N, EVT Ty,
                                           SelectionDAG &DAG,
                                           unsigned Flag) const {
   return DAG.getTargetBlockAddress(N->getBlockAddress(), Ty, 0, Flag);
 }
 
 SDValue MipsTargetLowering::getTargetNode(JumpTableSDNode *N, EVT Ty,
                                           SelectionDAG &DAG,
                                           unsigned Flag) const {
   return DAG.getTargetJumpTable(N->getIndex(), Ty, Flag);
 }
 
 SDValue MipsTargetLowering::getTargetNode(ConstantPoolSDNode *N, EVT Ty,
                                           SelectionDAG &DAG,
                                           unsigned Flag) const {
   return DAG.getTargetConstantPool(N->getConstVal(), Ty, N->getAlign(),
                                    N->getOffset(), Flag);
 }
 
 const char *MipsTargetLowering::getTargetNodeName(unsigned Opcode) const {
   switch ((MipsISD::NodeType)Opcode) {
   case MipsISD::FIRST_NUMBER:      break;
   case MipsISD::JmpLink:           return "MipsISD::JmpLink";
   case MipsISD::TailCall:          return "MipsISD::TailCall";
   case MipsISD::Highest:           return "MipsISD::Highest";
   case MipsISD::Higher:            return "MipsISD::Higher";
   case MipsISD::Hi:                return "MipsISD::Hi";
   case MipsISD::Lo:                return "MipsISD::Lo";
   case MipsISD::GotHi:             return "MipsISD::GotHi";
   case MipsISD::TlsHi:             return "MipsISD::TlsHi";
   case MipsISD::GPRel:             return "MipsISD::GPRel";
   case MipsISD::ThreadPointer:     return "MipsISD::ThreadPointer";
   case MipsISD::Ret:               return "MipsISD::Ret";
   case MipsISD::ERet:              return "MipsISD::ERet";
   case MipsISD::EH_RETURN:         return "MipsISD::EH_RETURN";
   case MipsISD::FMS:               return "MipsISD::FMS";
   case MipsISD::FPBrcond:          return "MipsISD::FPBrcond";
   case MipsISD::FPCmp:             return "MipsISD::FPCmp";
   case MipsISD::FSELECT:           return "MipsISD::FSELECT";
   case MipsISD::MTC1_D64:          return "MipsISD::MTC1_D64";
   case MipsISD::CMovFP_T:          return "MipsISD::CMovFP_T";
   case MipsISD::CMovFP_F:          return "MipsISD::CMovFP_F";
   case MipsISD::TruncIntFP:        return "MipsISD::TruncIntFP";
   case MipsISD::MFHI:              return "MipsISD::MFHI";
   case MipsISD::MFLO:              return "MipsISD::MFLO";
   case MipsISD::MTLOHI:            return "MipsISD::MTLOHI";
   case MipsISD::Mult:              return "MipsISD::Mult";
   case MipsISD::Multu:             return "MipsISD::Multu";
+  case MipsISD::Mult16:            return "MipsISD::Mult16";
+  case MipsISD::Multu16:           return "MipsISD::Multu16";
   case MipsISD::MAdd:              return "MipsISD::MAdd";
   case MipsISD::MAddu:             return "MipsISD::MAddu";
   case MipsISD::MSub:              return "MipsISD::MSub";
   case MipsISD::MSubu:             return "MipsISD::MSubu";
   case MipsISD::DivRem:            return "MipsISD::DivRem";
   case MipsISD::DivRemU:           return "MipsISD::DivRemU";
   case MipsISD::DivRem16:          return "MipsISD::DivRem16";
   case MipsISD::DivRemU16:         return "MipsISD::DivRemU16";
   case MipsISD::BuildPairF64:      return "MipsISD::BuildPairF64";
   case MipsISD::ExtractElementF64: return "MipsISD::ExtractElementF64";
   case MipsISD::Wrapper:           return "MipsISD::Wrapper";
   case MipsISD::DynAlloc:          return "MipsISD::DynAlloc";
   case MipsISD::Sync:              return "MipsISD::Sync";
   case MipsISD::Ext:               return "MipsISD::Ext";
   case MipsISD::Ins:               return "MipsISD::Ins";
   case MipsISD::CIns:              return "MipsISD::CIns";
   case MipsISD::LWL:               return "MipsISD::LWL";
   case MipsISD::LWR:               return "MipsISD::LWR";
   case MipsISD::SWL:               return "MipsISD::SWL";
   case MipsISD::SWR:               return "MipsISD::SWR";
   case MipsISD::LDL:               return "MipsISD::LDL";
   case MipsISD::LDR:               return "MipsISD::LDR";
   case MipsISD::SDL:               return "MipsISD::SDL";
   case MipsISD::SDR:               return "MipsISD::SDR";
   case MipsISD::EXTP:              return "MipsISD::EXTP";
   case MipsISD::EXTPDP:            return "MipsISD::EXTPDP";
   case MipsISD::EXTR_S_H:          return "MipsISD::EXTR_S_H";
   case MipsISD::EXTR_W:            return "MipsISD::EXTR_W";
   case MipsISD::EXTR_R_W:          return "MipsISD::EXTR_R_W";
   case MipsISD::EXTR_RS_W:         return "MipsISD::EXTR_RS_W";
   case MipsISD::SHILO:             return "MipsISD::SHILO";
   case MipsISD::MTHLIP:            return "MipsISD::MTHLIP";
   case MipsISD::MULSAQ_S_W_PH:     return "MipsISD::MULSAQ_S_W_PH";
   case MipsISD::MAQ_S_W_PHL:       return "MipsISD::MAQ_S_W_PHL";
   case MipsISD::MAQ_S_W_PHR:       return "MipsISD::MAQ_S_W_PHR";
   case MipsISD::MAQ_SA_W_PHL:      return "MipsISD::MAQ_SA_W_PHL";
   case MipsISD::MAQ_SA_W_PHR:      return "MipsISD::MAQ_SA_W_PHR";
   case MipsISD::DPAU_H_QBL:        return "MipsISD::DPAU_H_QBL";
   case MipsISD::DPAU_H_QBR:        return "MipsISD::DPAU_H_QBR";
   case MipsISD::DPSU_H_QBL:        return "MipsISD::DPSU_H_QBL";
   case MipsISD::DPSU_H_QBR:        return "MipsISD::DPSU_H_QBR";
   case MipsISD::DPAQ_S_W_PH:       return "MipsISD::DPAQ_S_W_PH";
   case MipsISD::DPSQ_S_W_PH:       return "MipsISD::DPSQ_S_W_PH";
   case MipsISD::DPAQ_SA_L_W:       return "MipsISD::DPAQ_SA_L_W";
   case MipsISD::DPSQ_SA_L_W:       return "MipsISD::DPSQ_SA_L_W";
   case MipsISD::DPA_W_PH:          return "MipsISD::DPA_W_PH";
   case MipsISD::DPS_W_PH:          return "MipsISD::DPS_W_PH";
   case MipsISD::DPAQX_S_W_PH:      return "MipsISD::DPAQX_S_W_PH";
   case MipsISD::DPAQX_SA_W_PH:     return "MipsISD::DPAQX_SA_W_PH";
   case MipsISD::DPAX_W_PH:         return "MipsISD::DPAX_W_PH";
   case MipsISD::DPSX_W_PH:         return "MipsISD::DPSX_W_PH";
   case MipsISD::DPSQX_S_W_PH:      return "MipsISD::DPSQX_S_W_PH";
   case MipsISD::DPSQX_SA_W_PH:     return "MipsISD::DPSQX_SA_W_PH";
   case MipsISD::MULSA_W_PH:        return "MipsISD::MULSA_W_PH";
   case MipsISD::MULT:              return "MipsISD::MULT";
   case MipsISD::MULTU:             return "MipsISD::MULTU";
   case MipsISD::MADD_DSP:          return "MipsISD::MADD_DSP";
   case MipsISD::MADDU_DSP:         return "MipsISD::MADDU_DSP";
   case MipsISD::MSUB_DSP:          return "MipsISD::MSUB_DSP";
   case MipsISD::MSUBU_DSP:         return "MipsISD::MSUBU_DSP";
   case MipsISD::SHLL_DSP:          return "MipsISD::SHLL_DSP";
   case MipsISD::SHRA_DSP:          return "MipsISD::SHRA_DSP";
   case MipsISD::SHRL_DSP:          return "MipsISD::SHRL_DSP";
   case MipsISD::SETCC_DSP:         return "MipsISD::SETCC_DSP";
   case MipsISD::SELECT_CC_DSP:     return "MipsISD::SELECT_CC_DSP";
   case MipsISD::VALL_ZERO:         return "MipsISD::VALL_ZERO";
   case MipsISD::VANY_ZERO:         return "MipsISD::VANY_ZERO";
   case MipsISD::VALL_NONZERO:      return "MipsISD::VALL_NONZERO";
   case MipsISD::VANY_NONZERO:      return "MipsISD::VANY_NONZERO";
   case MipsISD::VCEQ:              return "MipsISD::VCEQ";
   case MipsISD::VCLE_S:            return "MipsISD::VCLE_S";
   case MipsISD::VCLE_U:            return "MipsISD::VCLE_U";
   case MipsISD::VCLT_S:            return "MipsISD::VCLT_S";
   case MipsISD::VCLT_U:            return "MipsISD::VCLT_U";
   case MipsISD::VEXTRACT_SEXT_ELT: return "MipsISD::VEXTRACT_SEXT_ELT";
   case MipsISD::VEXTRACT_ZEXT_ELT: return "MipsISD::VEXTRACT_ZEXT_ELT";
   case MipsISD::VNOR:              return "MipsISD::VNOR";
   case MipsISD::VSHF:              return "MipsISD::VSHF";
   case MipsISD::SHF:               return "MipsISD::SHF";
   case MipsISD::ILVEV:             return "MipsISD::ILVEV";
   case MipsISD::ILVOD:             return "MipsISD::ILVOD";
   case MipsISD::ILVL:              return "MipsISD::ILVL";
   case MipsISD::ILVR:              return "MipsISD::ILVR";
   case MipsISD::PCKEV:             return "MipsISD::PCKEV";
   case MipsISD::PCKOD:             return "MipsISD::PCKOD";
   case MipsISD::INSVE:             return "MipsISD::INSVE";
   }
   return nullptr;
 }
 
 MipsTargetLowering::MipsTargetLowering(const MipsTargetMachine &TM,
                                        const MipsSubtarget &STI)
     : TargetLowering(TM), Subtarget(STI), ABI(TM.getABI()) {
   // Mips does not have i1 type, so use i32 for
   // setcc operations results (slt, sgt, ...).
   setBooleanContents(ZeroOrOneBooleanContent);
   setBooleanVectorContents(ZeroOrNegativeOneBooleanContent);
   // The cmp.cond.fmt instruction in MIPS32r6/MIPS64r6 uses 0 and -1 like MSA
   // does. Integer booleans still use 0 and 1.
   if (Subtarget.hasMips32r6())
     setBooleanContents(ZeroOrOneBooleanContent,
                        ZeroOrNegativeOneBooleanContent);
 
   // Load extented operations for i1 types must be promoted
   for (MVT VT : MVT::integer_valuetypes()) {
     setLoadExtAction(ISD::EXTLOAD,  VT, MVT::i1,  Promote);
     setLoadExtAction(ISD::ZEXTLOAD, VT, MVT::i1,  Promote);
     setLoadExtAction(ISD::SEXTLOAD, VT, MVT::i1,  Promote);
   }
 
   // MIPS doesn't have extending float->double load/store.  Set LoadExtAction
   // for f32, f16
   for (MVT VT : MVT::fp_valuetypes()) {
     setLoadExtAction(ISD::EXTLOAD, VT, MVT::f32, Expand);
     setLoadExtAction(ISD::EXTLOAD, VT, MVT::f16, Expand);
   }
 
   // Set LoadExtAction for f16 vectors to Expand
   for (MVT VT : MVT::fp_fixedlen_vector_valuetypes()) {
     MVT F16VT = MVT::getVectorVT(MVT::f16, VT.getVectorNumElements());
     if (F16VT.isValid())
       setLoadExtAction(ISD::EXTLOAD, VT, F16VT, Expand);
   }
 
   setTruncStoreAction(MVT::f32, MVT::f16, Expand);
   setTruncStoreAction(MVT::f64, MVT::f16, Expand);
 
   setTruncStoreAction(MVT::f64, MVT::f32, Expand);
 
   // Used by legalize types to correctly generate the setcc result.
   // Without this, every float setcc comes with a AND/OR with the result,
   // we don't want this, since the fpcmp result goes to a flag register,
   // which is used implicitly by brcond and select operations.
   AddPromotedToType(ISD::SETCC, MVT::i1, MVT::i32);
 
   // Mips Custom Operations
   setOperationAction(ISD::BR_JT,              MVT::Other, Expand);
   setOperationAction(ISD::GlobalAddress,      MVT::i32,   Custom);
   setOperationAction(ISD::BlockAddress,       MVT::i32,   Custom);
   setOperationAction(ISD::GlobalTLSAddress,   MVT::i32,   Custom);
   setOperationAction(ISD::JumpTable,          MVT::i32,   Custom);
   setOperationAction(ISD::ConstantPool,       MVT::i32,   Custom);
   setOperationAction(ISD::SELECT,             MVT::f32,   Custom);
   setOperationAction(ISD::SELECT,             MVT::f64,   Custom);
   setOperationAction(ISD::SELECT,             MVT::i32,   Custom);
   setOperationAction(ISD::SETCC,              MVT::f32,   Custom);
   setOperationAction(ISD::SETCC,              MVT::f64,   Custom);
   setOperationAction(ISD::BRCOND,             MVT::Other, Custom);
   setOperationAction(ISD::FCOPYSIGN,          MVT::f32,   Custom);
   setOperationAction(ISD::FCOPYSIGN,          MVT::f64,   Custom);
   setOperationAction(ISD::FP_TO_SINT,         MVT::i32,   Custom);
 
   if (!(TM.Options.NoNaNsFPMath || Subtarget.inAbs2008Mode())) {
     setOperationAction(ISD::FABS, MVT::f32, Custom);
     setOperationAction(ISD::FABS, MVT::f64, Custom);
   }
 
   if (Subtarget.isGP64bit()) {
     setOperationAction(ISD::GlobalAddress,      MVT::i64,   Custom);
     setOperationAction(ISD::BlockAddress,       MVT::i64,   Custom);
     setOperationAction(ISD::GlobalTLSAddress,   MVT::i64,   Custom);
     setOperationAction(ISD::JumpTable,          MVT::i64,   Custom);
     setOperationAction(ISD::ConstantPool,       MVT::i64,   Custom);
     setOperationAction(ISD::SELECT,             MVT::i64,   Custom);
     setOperationAction(ISD::LOAD,               MVT::i64,   Custom);
     setOperationAction(ISD::STORE,              MVT::i64,   Custom);
     setOperationAction(ISD::FP_TO_SINT,         MVT::i64,   Custom);
     setOperationAction(ISD::SHL_PARTS,          MVT::i64,   Custom);
     setOperationAction(ISD::SRA_PARTS,          MVT::i64,   Custom);
     setOperationAction(ISD::SRL_PARTS,          MVT::i64,   Custom);
   }
 
   if (!Subtarget.isGP64bit()) {
     setOperationAction(ISD::SHL_PARTS,          MVT::i32,   Custom);
     setOperationAction(ISD::SRA_PARTS,          MVT::i32,   Custom);
     setOperationAction(ISD::SRL_PARTS,          MVT::i32,   Custom);
   }
 
   setOperationAction(ISD::EH_DWARF_CFA,         MVT::i32,   Custom);
   if (Subtarget.isGP64bit())
     setOperationAction(ISD::EH_DWARF_CFA,       MVT::i64,   Custom);
 
   setOperationAction(ISD::SDIV, MVT::i32, Expand);
   setOperationAction(ISD::SREM, MVT::i32, Expand);
   setOperationAction(ISD::UDIV, MVT::i32, Expand);
   setOperationAction(ISD::UREM, MVT::i32, Expand);
   setOperationAction(ISD::SDIV, MVT::i64, Expand);
   setOperationAction(ISD::SREM, MVT::i64, Expand);
   setOperationAction(ISD::UDIV, MVT::i64, Expand);
   setOperationAction(ISD::UREM, MVT::i64, Expand);
 
   // Operations not directly supported by Mips.
   setOperationAction(ISD::BR_CC,             MVT::f32,   Expand);
   setOperationAction(ISD::BR_CC,             MVT::f64,   Expand);
   setOperationAction(ISD::BR_CC,             MVT::i32,   Expand);
   setOperationAction(ISD::BR_CC,             MVT::i64,   Expand);
   setOperationAction(ISD::SELECT_CC,         MVT::i32,   Expand);
   setOperationAction(ISD::SELECT_CC,         MVT::i64,   Expand);
   setOperationAction(ISD::SELECT_CC,         MVT::f32,   Expand);
   setOperationAction(ISD::SELECT_CC,         MVT::f64,   Expand);
   setOperationAction(ISD::UINT_TO_FP,        MVT::i32,   Expand);
   setOperationAction(ISD::UINT_TO_FP,        MVT::i64,   Expand);
   setOperationAction(ISD::FP_TO_UINT,        MVT::i32,   Expand);
   setOperationAction(ISD::FP_TO_UINT,        MVT::i64,   Expand);
   setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i1,    Expand);
   if (Subtarget.hasCnMips()) {
     setOperationAction(ISD::CTPOP,           MVT::i32,   Legal);
     setOperationAction(ISD::CTPOP,           MVT::i64,   Legal);
   } else {
     setOperationAction(ISD::CTPOP,           MVT::i32,   Expand);
     setOperationAction(ISD::CTPOP,           MVT::i64,   Expand);
   }
   setOperationAction(ISD::CTTZ,              MVT::i32,   Expand);
   setOperationAction(ISD::CTTZ,              MVT::i64,   Expand);
   setOperationAction(ISD::ROTL,              MVT::i32,   Expand);
   setOperationAction(ISD::ROTL,              MVT::i64,   Expand);
   setOperationAction(ISD::DYNAMIC_STACKALLOC, MVT::i32,  Expand);
   setOperationAction(ISD::DYNAMIC_STACKALLOC, MVT::i64,  Expand);
 
   if (!Subtarget.hasMips32r2())
     setOperationAction(ISD::ROTR, MVT::i32,   Expand);
 
   if (!Subtarget.hasMips64r2())
     setOperationAction(ISD::ROTR, MVT::i64,   Expand);
 
   setOperationAction(ISD::FSIN,              MVT::f32,   Expand);
   setOperationAction(ISD::FSIN,              MVT::f64,   Expand);
   setOperationAction(ISD::FCOS,              MVT::f32,   Expand);
   setOperationAction(ISD::FCOS,              MVT::f64,   Expand);
   setOperationAction(ISD::FSINCOS,           MVT::f32,   Expand);
   setOperationAction(ISD::FSINCOS,           MVT::f64,   Expand);
   setOperationAction(ISD::FPOW,              MVT::f32,   Expand);
   setOperationAction(ISD::FPOW,              MVT::f64,   Expand);
   setOperationAction(ISD::FLOG,              MVT::f32,   Expand);
   setOperationAction(ISD::FLOG2,             MVT::f32,   Expand);
   setOperationAction(ISD::FLOG10,            MVT::f32,   Expand);
   setOperationAction(ISD::FEXP,              MVT::f32,   Expand);
   setOperationAction(ISD::FMA,               MVT::f32,   Expand);
   setOperationAction(ISD::FMA,               MVT::f64,   Expand);
   setOperationAction(ISD::FREM,              MVT::f32,   Expand);
   setOperationAction(ISD::FREM,              MVT::f64,   Expand);
 
   // Lower f16 conversion operations into library calls
   setOperationAction(ISD::FP16_TO_FP,        MVT::f32,   Expand);
   setOperationAction(ISD::FP_TO_FP16,        MVT::f32,   Expand);
   setOperationAction(ISD::FP16_TO_FP,        MVT::f64,   Expand);
   setOperationAction(ISD::FP_TO_FP16,        MVT::f64,   Expand);
 
   setOperationAction(ISD::EH_RETURN, MVT::Other, Custom);
 
   setOperationAction(ISD::VASTART,           MVT::Other, Custom);
   setOperationAction(ISD::VAARG,             MVT::Other, Custom);
   setOperationAction(ISD::VACOPY,            MVT::Other, Expand);
   setOperationAction(ISD::VAEND,             MVT::Other, Expand);
 
   // Use the default for now
   setOperationAction(ISD::STACKSAVE,         MVT::Other, Expand);
   setOperationAction(ISD::STACKRESTORE,      MVT::Other, Expand);
 
   if (!Subtarget.isGP64bit()) {
     setOperationAction(ISD::ATOMIC_LOAD,     MVT::i64,   Expand);
     setOperationAction(ISD::ATOMIC_STORE,    MVT::i64,   Expand);
   }
 
   if (!Subtarget.hasMips32r2()) {
     setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i8,  Expand);
     setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i16, Expand);
   }
 
   // MIPS16 lacks MIPS32's clz and clo instructions.
   if (!Subtarget.hasMips32() || Subtarget.inMips16Mode())
     setOperationAction(ISD::CTLZ, MVT::i32, Expand);
   if (!Subtarget.hasMips64())
     setOperationAction(ISD::CTLZ, MVT::i64, Expand);
 
   if (!Subtarget.hasMips32r2())
     setOperationAction(ISD::BSWAP, MVT::i32, Expand);
   if (!Subtarget.hasMips64r2())
     setOperationAction(ISD::BSWAP, MVT::i64, Expand);
 
   if (Subtarget.isGP64bit()) {
     setLoadExtAction(ISD::SEXTLOAD, MVT::i64, MVT::i32, Custom);
     setLoadExtAction(ISD::ZEXTLOAD, MVT::i64, MVT::i32, Custom);
     setLoadExtAction(ISD::EXTLOAD, MVT::i64, MVT::i32, Custom);
     setTruncStoreAction(MVT::i64, MVT::i32, Custom);
   }
 
   setOperationAction(ISD::TRAP, MVT::Other, Legal);
 
   setTargetDAGCombine(ISD::SDIVREM);
   setTargetDAGCombine(ISD::UDIVREM);
   setTargetDAGCombine(ISD::SELECT);
   setTargetDAGCombine(ISD::AND);
   setTargetDAGCombine(ISD::OR);
   setTargetDAGCombine(ISD::ADD);
   setTargetDAGCombine(ISD::SUB);
   setTargetDAGCombine(ISD::AssertZext);
   setTargetDAGCombine(ISD::SHL);
 
   if (ABI.IsO32()) {
     // These libcalls are not available in 32-bit.
     setLibcallName(RTLIB::SHL_I128, nullptr);
     setLibcallName(RTLIB::SRL_I128, nullptr);
     setLibcallName(RTLIB::SRA_I128, nullptr);
   }
 
   setMinFunctionAlignment(Subtarget.isGP64bit() ? Align(8) : Align(4));
 
   // The arguments on the stack are defined in terms of 4-byte slots on O32
   // and 8-byte slots on N32/N64.
   setMinStackArgumentAlignment((ABI.IsN32() || ABI.IsN64()) ? Align(8)
                                                             : Align(4));
 
   setStackPointerRegisterToSaveRestore(ABI.IsN64() ? Mips::SP_64 : Mips::SP);
 
   MaxStoresPerMemcpy = 16;
 
   isMicroMips = Subtarget.inMicroMipsMode();
 }
 
 const MipsTargetLowering *
 MipsTargetLowering::create(const MipsTargetMachine &TM,
                            const MipsSubtarget &STI) {
   if (STI.inMips16Mode())
     return createMips16TargetLowering(TM, STI);
 
   return createMipsSETargetLowering(TM, STI);
 }
 
 // Create a fast isel object.
 FastISel *
 MipsTargetLowering::createFastISel(FunctionLoweringInfo &funcInfo,
                                   const TargetLibraryInfo *libInfo) const {
   const MipsTargetMachine &TM =
       static_cast<const MipsTargetMachine &>(funcInfo.MF->getTarget());
 
   // We support only the standard encoding [MIPS32,MIPS32R5] ISAs.
   bool UseFastISel = TM.Options.EnableFastISel && Subtarget.hasMips32() &&
                      !Subtarget.hasMips32r6() && !Subtarget.inMips16Mode() &&
                      !Subtarget.inMicroMipsMode();
 
   // Disable if either of the following is true:
   // We do not generate PIC, the ABI is not O32, XGOT is being used.
   if (!TM.isPositionIndependent() || !TM.getABI().IsO32() ||
       Subtarget.useXGOT())
     UseFastISel = false;
 
   return UseFastISel ? Mips::createFastISel(funcInfo, libInfo) : nullptr;
 }
 
 EVT MipsTargetLowering::getSetCCResultType(const DataLayout &, LLVMContext &,
                                            EVT VT) const {
   if (!VT.isVector())
     return MVT::i32;
   return VT.changeVectorElementTypeToInteger();
 }
 
 static SDValue performDivRemCombine(SDNode *N, SelectionDAG &DAG,
                                     TargetLowering::DAGCombinerInfo &DCI,
                                     const MipsSubtarget &Subtarget) {
   if (DCI.isBeforeLegalizeOps())
     return SDValue();
 
   EVT Ty = N->getValueType(0);
   unsigned LO = (Ty == MVT::i32) ? Mips::LO0 : Mips::LO0_64;
   unsigned HI = (Ty == MVT::i32) ? Mips::HI0 : Mips::HI0_64;
   unsigned Opc = N->getOpcode() == ISD::SDIVREM ? MipsISD::DivRem16 :
                                                   MipsISD::DivRemU16;
   SDLoc DL(N);
 
   SDValue DivRem = DAG.getNode(Opc, DL, MVT::Glue,
                                N->getOperand(0), N->getOperand(1));
   SDValue InChain = DAG.getEntryNode();
   SDValue InGlue = DivRem;
 
   // insert MFLO
   if (N->hasAnyUseOfValue(0)) {
     SDValue CopyFromLo = DAG.getCopyFromReg(InChain, DL, LO, Ty,
                                             InGlue);
     DAG.ReplaceAllUsesOfValueWith(SDValue(N, 0), CopyFromLo);
     InChain = CopyFromLo.getValue(1);
     InGlue = CopyFromLo.getValue(2);
   }
 
   // insert MFHI
   if (N->hasAnyUseOfValue(1)) {
     SDValue CopyFromHi = DAG.getCopyFromReg(InChain, DL,
                                             HI, Ty, InGlue);
     DAG.ReplaceAllUsesOfValueWith(SDValue(N, 1), CopyFromHi);
   }
 
   return SDValue();
 }
 
 static Mips::CondCode condCodeToFCC(ISD::CondCode CC) {
   switch (CC) {
   default: llvm_unreachable("Unknown fp condition code!");
   case ISD::SETEQ:
   case ISD::SETOEQ: return Mips::FCOND_OEQ;
   case ISD::SETUNE: return Mips::FCOND_UNE;
   case ISD::SETLT:
   case ISD::SETOLT: return Mips::FCOND_OLT;
   case ISD::SETGT:
   case ISD::SETOGT: return Mips::FCOND_OGT;
   case ISD::SETLE:
   case ISD::SETOLE: return Mips::FCOND_OLE;
   case ISD::SETGE:
   case ISD::SETOGE: return Mips::FCOND_OGE;
   case ISD::SETULT: return Mips::FCOND_ULT;
   case ISD::SETULE: return Mips::FCOND_ULE;
   case ISD::SETUGT: return Mips::FCOND_UGT;
   case ISD::SETUGE: return Mips::FCOND_UGE;
   case ISD::SETUO:  return Mips::FCOND_UN;
   case ISD::SETO:   return Mips::FCOND_OR;
   case ISD::SETNE:
   case ISD::SETONE: return Mips::FCOND_ONE;
   case ISD::SETUEQ: return Mips::FCOND_UEQ;
   }
 }
 
 /// This function returns true if the floating point conditional branches and
 /// conditional moves which use condition code CC should be inverted.
 static bool invertFPCondCodeUser(Mips::CondCode CC) {
   if (CC >= Mips::FCOND_F && CC <= Mips::FCOND_NGT)
     return false;
 
   assert((CC >= Mips::FCOND_T && CC <= Mips::FCOND_GT) &&
          "Illegal Condition Code");
 
   return true;
 }
 
 // Creates and returns an FPCmp node from a setcc node.
 // Returns Op if setcc is not a floating point comparison.
 static SDValue createFPCmp(SelectionDAG &DAG, const SDValue &Op) {
   // must be a SETCC node
   if (Op.getOpcode() != ISD::SETCC)
     return Op;
 
   SDValue LHS = Op.getOperand(0);
 
   if (!LHS.getValueType().isFloatingPoint())
     return Op;
 
   SDValue RHS = Op.getOperand(1);
   SDLoc DL(Op);
 
   // Assume the 3rd operand is a CondCodeSDNode. Add code to check the type of
   // node if necessary.
   ISD::CondCode CC = cast<CondCodeSDNode>(Op.getOperand(2))->get();
 
   return DAG.getNode(MipsISD::FPCmp, DL, MVT::Glue, LHS, RHS,
                      DAG.getConstant(condCodeToFCC(CC), DL, MVT::i32));
 }
 
 // Creates and returns a CMovFPT/F node.
 static SDValue createCMovFP(SelectionDAG &DAG, SDValue Cond, SDValue True,
                             SDValue False, const SDLoc &DL) {
   ConstantSDNode *CC = cast<ConstantSDNode>(Cond.getOperand(2));
   bool invert = invertFPCondCodeUser((Mips::CondCode)CC->getSExtValue());
   SDValue FCC0 = DAG.getRegister(Mips::FCC0, MVT::i32);
 
   return DAG.getNode((invert ? MipsISD::CMovFP_F : MipsISD::CMovFP_T), DL,
                      True.getValueType(), True, FCC0, False, Cond);
 }
 
 static SDValue performSELECTCombine(SDNode *N, SelectionDAG &DAG,
                                     TargetLowering::DAGCombinerInfo &DCI,
                                     const MipsSubtarget &Subtarget) {
   if (DCI.isBeforeLegalizeOps())
     return SDValue();
 
   SDValue SetCC = N->getOperand(0);
 
   if ((SetCC.getOpcode() != ISD::SETCC) ||
       !SetCC.getOperand(0).getValueType().isInteger())
     return SDValue();
 
   SDValue False = N->getOperand(2);
   EVT FalseTy = False.getValueType();
 
   if (!FalseTy.isInteger())
     return SDValue();
 
   ConstantSDNode *FalseC = dyn_cast<ConstantSDNode>(False);
 
   // If the RHS (False) is 0, we swap the order of the operands
   // of ISD::SELECT (obviously also inverting the condition) so that we can
   // take advantage of conditional moves using the $0 register.
   // Example:
   //   return (a != 0) ? x : 0;
   //     load $reg, x
   //     movz $reg, $0, a
   if (!FalseC)
     return SDValue();
 
   const SDLoc DL(N);
 
   if (!FalseC->getZExtValue()) {
     ISD::CondCode CC = cast<CondCodeSDNode>(SetCC.getOperand(2))->get();
     SDValue True = N->getOperand(1);
 
     SetCC = DAG.getSetCC(DL, SetCC.getValueType(), SetCC.getOperand(0),
                          SetCC.getOperand(1),
                          ISD::getSetCCInverse(CC, SetCC.getValueType()));
 
     return DAG.getNode(ISD::SELECT, DL, FalseTy, SetCC, False, True);
   }
 
   // If both operands are integer constants there's a possibility that we
   // can do some interesting optimizations.
   SDValue True = N->getOperand(1);
   ConstantSDNode *TrueC = dyn_cast<ConstantSDNode>(True);
 
   if (!TrueC || !True.getValueType().isInteger())
     return SDValue();
 
   // We'll also ignore MVT::i64 operands as this optimizations proves
   // to be ineffective because of the required sign extensions as the result
   // of a SETCC operator is always MVT::i32 for non-vector types.
   if (True.getValueType() == MVT::i64)
     return SDValue();
 
   int64_t Diff = TrueC->getSExtValue() - FalseC->getSExtValue();
 
   // 1)  (a < x) ? y : y-1
   //  slti $reg1, a, x
   //  addiu $reg2, $reg1, y-1
   if (Diff == 1)
     return DAG.getNode(ISD::ADD, DL, SetCC.getValueType(), SetCC, False);
 
   // 2)  (a < x) ? y-1 : y
   //  slti $reg1, a, x
   //  xor $reg1, $reg1, 1
   //  addiu $reg2, $reg1, y-1
   if (Diff == -1) {
     ISD::CondCode CC = cast<CondCodeSDNode>(SetCC.getOperand(2))->get();
     SetCC = DAG.getSetCC(DL, SetCC.getValueType(), SetCC.getOperand(0),
                          SetCC.getOperand(1),
                          ISD::getSetCCInverse(CC, SetCC.getValueType()));
     return DAG.getNode(ISD::ADD, DL, SetCC.getValueType(), SetCC, True);
   }
 
   // Could not optimize.
   return SDValue();
 }
 
 static SDValue performCMovFPCombine(SDNode *N, SelectionDAG &DAG,
                                     TargetLowering::DAGCombinerInfo &DCI,
                                     const MipsSubtarget &Subtarget) {
   if (DCI.isBeforeLegalizeOps())
     return SDValue();
 
   SDValue ValueIfTrue = N->getOperand(0), ValueIfFalse = N->getOperand(2);
 
   ConstantSDNode *FalseC = dyn_cast<ConstantSDNode>(ValueIfFalse);
   if (!FalseC || FalseC->getZExtValue())
     return SDValue();
 
   // Since RHS (False) is 0, we swap the order of the True/False operands
   // (obviously also inverting the condition) so that we can
   // take advantage of conditional moves using the $0 register.
   // Example:
   //   return (a != 0) ? x : 0;
   //     load $reg, x
   //     movz $reg, $0, a
   unsigned Opc = (N->getOpcode() == MipsISD::CMovFP_T) ? MipsISD::CMovFP_F :
                                                          MipsISD::CMovFP_T;
 
   SDValue FCC = N->getOperand(1), Glue = N->getOperand(3);
   return DAG.getNode(Opc, SDLoc(N), ValueIfFalse.getValueType(),
                      ValueIfFalse, FCC, ValueIfTrue, Glue);
 }
 
 static SDValue performANDCombine(SDNode *N, SelectionDAG &DAG,
                                  TargetLowering::DAGCombinerInfo &DCI,
                                  const MipsSubtarget &Subtarget) {
   if (DCI.isBeforeLegalizeOps() || !Subtarget.hasExtractInsert())
     return SDValue();
 
   SDValue FirstOperand = N->getOperand(0);
   unsigned FirstOperandOpc = FirstOperand.getOpcode();
   SDValue Mask = N->getOperand(1);
   EVT ValTy = N->getValueType(0);
   SDLoc DL(N);
 
   uint64_t Pos = 0, SMPos, SMSize;
   ConstantSDNode *CN;
   SDValue NewOperand;
   unsigned Opc;
 
   // Op's second operand must be a shifted mask.
   if (!(CN = dyn_cast<ConstantSDNode>(Mask)) ||
       !isShiftedMask(CN->getZExtValue(), SMPos, SMSize))
     return SDValue();
 
   if (FirstOperandOpc == ISD::SRA || FirstOperandOpc == ISD::SRL) {
     // Pattern match EXT.
     //  $dst = and ((sra or srl) $src , pos), (2**size - 1)
     //  => ext $dst, $src, pos, size
 
     // The second operand of the shift must be an immediate.
     if (!(CN = dyn_cast<ConstantSDNode>(FirstOperand.getOperand(1))))
       return SDValue();
 
     Pos = CN->getZExtValue();
 
     // Return if the shifted mask does not start at bit 0 or the sum of its size
     // and Pos exceeds the word's size.
     if (SMPos != 0 || Pos + SMSize > ValTy.getSizeInBits())
       return SDValue();
 
     Opc = MipsISD::Ext;
     NewOperand = FirstOperand.getOperand(0);
   } else if (FirstOperandOpc == ISD::SHL && Subtarget.hasCnMips()) {
     // Pattern match CINS.
     //  $dst = and (shl $src , pos), mask
     //  => cins $dst, $src, pos, size
     // mask is a shifted mask with consecutive 1's, pos = shift amount,
     // size = population count.
 
     // The second operand of the shift must be an immediate.
     if (!(CN = dyn_cast<ConstantSDNode>(FirstOperand.getOperand(1))))
       return SDValue();
 
     Pos = CN->getZExtValue();
 
     if (SMPos != Pos || Pos >= ValTy.getSizeInBits() || SMSize >= 32 ||
         Pos + SMSize > ValTy.getSizeInBits())
       return SDValue();
 
     NewOperand = FirstOperand.getOperand(0);
     // SMSize is 'location' (position) in this case, not size.
     SMSize--;
     Opc = MipsISD::CIns;
   } else {
     // Pattern match EXT.
     //  $dst = and $src, (2**size - 1) , if size > 16
     //  => ext $dst, $src, pos, size , pos = 0
 
     // If the mask is <= 0xffff, andi can be used instead.
     if (CN->getZExtValue() <= 0xffff)
       return SDValue();
 
     // Return if the mask doesn't start at position 0.
     if (SMPos)
       return SDValue();
 
     Opc = MipsISD::Ext;
     NewOperand = FirstOperand;
   }
   return DAG.getNode(Opc, DL, ValTy, NewOperand,
                      DAG.getConstant(Pos, DL, MVT::i32),
                      DAG.getConstant(SMSize, DL, MVT::i32));
 }
 
 static SDValue performORCombine(SDNode *N, SelectionDAG &DAG,
                                 TargetLowering::DAGCombinerInfo &DCI,
                                 const MipsSubtarget &Subtarget) {
   // Pattern match INS.
   //  $dst = or (and $src1 , mask0), (and (shl $src, pos), mask1),
   //  where mask1 = (2**size - 1) << pos, mask0 = ~mask1
   //  => ins $dst, $src, size, pos, $src1
   if (DCI.isBeforeLegalizeOps() || !Subtarget.hasExtractInsert())
     return SDValue();
 
   SDValue And0 = N->getOperand(0), And1 = N->getOperand(1);
   uint64_t SMPos0, SMSize0, SMPos1, SMSize1;
   ConstantSDNode *CN, *CN1;
 
   // See if Op's first operand matches (and $src1 , mask0).
   if (And0.getOpcode() != ISD::AND)
     return SDValue();
 
   if (!(CN = dyn_cast<ConstantSDNode>(And0.getOperand(1))) ||
       !isShiftedMask(~CN->getSExtValue(), SMPos0, SMSize0))
     return SDValue();
 
   // See if Op's second operand matches (and (shl $src, pos), mask1).
   if (And1.getOpcode() == ISD::AND &&
       And1.getOperand(0).getOpcode() == ISD::SHL) {
 
     if (!(CN = dyn_cast<ConstantSDNode>(And1.getOperand(1))) ||
         !isShiftedMask(CN->getZExtValue(), SMPos1, SMSize1))
       return SDValue();
 
     // The shift masks must have the same position and size.
     if (SMPos0 != SMPos1 || SMSize0 != SMSize1)
       return SDValue();
 
     SDValue Shl = And1.getOperand(0);
 
     if (!(CN = dyn_cast<ConstantSDNode>(Shl.getOperand(1))))
       return SDValue();
 
     unsigned Shamt = CN->getZExtValue();
 
     // Return if the shift amount and the first bit position of mask are not the
     // same.
     EVT ValTy = N->getValueType(0);
     if ((Shamt != SMPos0) || (SMPos0 + SMSize0 > ValTy.getSizeInBits()))
       return SDValue();
 
     SDLoc DL(N);
     return DAG.getNode(MipsISD::Ins, DL, ValTy, Shl.getOperand(0),
                        DAG.getConstant(SMPos0, DL, MVT::i32),
                        DAG.getConstant(SMSize0, DL, MVT::i32),
                        And0.getOperand(0));
   } else {
     // Pattern match DINS.
     //  $dst = or (and $src, mask0), mask1
     //  where mask0 = ((1 << SMSize0) -1) << SMPos0
     //  => dins $dst, $src, pos, size
     if (~CN->getSExtValue() == ((((int64_t)1 << SMSize0) - 1) << SMPos0) &&
         ((SMSize0 + SMPos0 <= 64 && Subtarget.hasMips64r2()) ||
          (SMSize0 + SMPos0 <= 32))) {
       // Check if AND instruction has constant as argument
       bool isConstCase = And1.getOpcode() != ISD::AND;
       if (And1.getOpcode() == ISD::AND) {
         if (!(CN1 = dyn_cast<ConstantSDNode>(And1->getOperand(1))))
           return SDValue();
       } else {
         if (!(CN1 = dyn_cast<ConstantSDNode>(N->getOperand(1))))
           return SDValue();
       }
       // Don't generate INS if constant OR operand doesn't fit into bits
       // cleared by constant AND operand.
       if (CN->getSExtValue() & CN1->getSExtValue())
         return SDValue();
 
       SDLoc DL(N);
       EVT ValTy = N->getOperand(0)->getValueType(0);
       SDValue Const1;
       SDValue SrlX;
       if (!isConstCase) {
         Const1 = DAG.getConstant(SMPos0, DL, MVT::i32);
         SrlX = DAG.getNode(ISD::SRL, DL, And1->getValueType(0), And1, Const1);
       }
       return DAG.getNode(
           MipsISD::Ins, DL, N->getValueType(0),
           isConstCase
               ? DAG.getConstant(CN1->getSExtValue() >> SMPos0, DL, ValTy)
               : SrlX,
           DAG.getConstant(SMPos0, DL, MVT::i32),
           DAG.getConstant(ValTy.getSizeInBits() / 8 < 8 ? SMSize0 & 31
                                                         : SMSize0,
                           DL, MVT::i32),
           And0->getOperand(0));
 
     }
     return SDValue();
   }
 }
 
 static SDValue performMADD_MSUBCombine(SDNode *ROOTNode, SelectionDAG &CurDAG,
                                        const MipsSubtarget &Subtarget) {
   // ROOTNode must have a multiplication as an operand for the match to be
   // successful.
   if (ROOTNode->getOperand(0).getOpcode() != ISD::MUL &&
       ROOTNode->getOperand(1).getOpcode() != ISD::MUL)
     return SDValue();
 
   // We don't handle vector types here.
   if (ROOTNode->getValueType(0).isVector())
     return SDValue();
 
   // For MIPS64, madd / msub instructions are inefficent to use with 64 bit
   // arithmetic. E.g.
   // (add (mul a b) c) =>
   //   let res = (madd (mthi (drotr c 32))x(mtlo c) a b) in
   //   MIPS64:   (or (dsll (mfhi res) 32) (dsrl (dsll (mflo res) 32) 32)
   //   or
   //   MIPS64R2: (dins (mflo res) (mfhi res) 32 32)
   //
   // The overhead of setting up the Hi/Lo registers and reassembling the
   // result makes this a dubious optimzation for MIPS64. The core of the
   // problem is that Hi/Lo contain the upper and lower 32 bits of the
   // operand and result.
   //
   // It requires a chain of 4 add/mul for MIPS64R2 to get better code
   // density than doing it naively, 5 for MIPS64. Additionally, using
   // madd/msub on MIPS64 requires the operands actually be 32 bit sign
   // extended operands, not true 64 bit values.
   //
   // FIXME: For the moment, disable this completely for MIPS64.
   if (Subtarget.hasMips64())
     return SDValue();
 
   SDValue Mult = ROOTNode->getOperand(0).getOpcode() == ISD::MUL
                      ? ROOTNode->getOperand(0)
                      : ROOTNode->getOperand(1);
 
   SDValue AddOperand = ROOTNode->getOperand(0).getOpcode() == ISD::MUL
                      ? ROOTNode->getOperand(1)
                      : ROOTNode->getOperand(0);
 
   // Transform this to a MADD only if the user of this node is the add.
   // If there are other users of the mul, this function returns here.
   if (!Mult.hasOneUse())
     return SDValue();
 
   // maddu and madd are unusual instructions in that on MIPS64 bits 63..31
   // must be in canonical form, i.e. sign extended. For MIPS32, the operands
   // of the multiply must have 32 or more sign bits, otherwise we cannot
   // perform this optimization. We have to check this here as we're performing
   // this optimization pre-legalization.
   SDValue MultLHS = Mult->getOperand(0);
   SDValue MultRHS = Mult->getOperand(1);
 
   bool IsSigned = MultLHS->getOpcode() == ISD::SIGN_EXTEND &&
                   MultRHS->getOpcode() == ISD::SIGN_EXTEND;
   bool IsUnsigned = MultLHS->getOpcode() == ISD::ZERO_EXTEND &&
                     MultRHS->getOpcode() == ISD::ZERO_EXTEND;
 
   if (!IsSigned && !IsUnsigned)
     return SDValue();
 
   // Initialize accumulator.
   SDLoc DL(ROOTNode);
   SDValue TopHalf;
   SDValue BottomHalf;
   BottomHalf = CurDAG.getNode(ISD::EXTRACT_ELEMENT, DL, MVT::i32, AddOperand,
                               CurDAG.getIntPtrConstant(0, DL));
 
   TopHalf = CurDAG.getNode(ISD::EXTRACT_ELEMENT, DL, MVT::i32, AddOperand,
                            CurDAG.getIntPtrConstant(1, DL));
   SDValue ACCIn = CurDAG.getNode(MipsISD::MTLOHI, DL, MVT::Untyped,
                                   BottomHalf,
                                   TopHalf);
 
   // Create MipsMAdd(u) / MipsMSub(u) node.
   bool IsAdd = ROOTNode->getOpcode() == ISD::ADD;
   unsigned Opcode = IsAdd ? (IsUnsigned ? MipsISD::MAddu : MipsISD::MAdd)
                           : (IsUnsigned ? MipsISD::MSubu : MipsISD::MSub);
   SDValue MAddOps[3] = {
       CurDAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Mult->getOperand(0)),
       CurDAG.getNode(ISD::TRUNCATE, DL, MVT::i32, Mult->getOperand(1)), ACCIn};
   EVT VTs[2] = {MVT::i32, MVT::i32};
   SDValue MAdd = CurDAG.getNode(Opcode, DL, VTs, MAddOps);
 
   SDValue ResLo = CurDAG.getNode(MipsISD::MFLO, DL, MVT::i32, MAdd);
   SDValue ResHi = CurDAG.getNode(MipsISD::MFHI, DL, MVT::i32, MAdd);
   SDValue Combined =
       CurDAG.getNode(ISD::BUILD_PAIR, DL, MVT::i64, ResLo, ResHi);
   return Combined;
 }
 
 static SDValue performSUBCombine(SDNode *N, SelectionDAG &DAG,
                                  TargetLowering::DAGCombinerInfo &DCI,
                                  const MipsSubtarget &Subtarget) {
   // (sub v0 (mul v1, v2)) => (msub v1, v2, v0)
   if (DCI.isBeforeLegalizeOps()) {
     if (Subtarget.hasMips32() && !Subtarget.hasMips32r6() &&
         !Subtarget.inMips16Mode() && N->getValueType(0) == MVT::i64)
       return performMADD_MSUBCombine(N, DAG, Subtarget);
 
     return SDValue();
   }
 
   return SDValue();
 }
 
 static SDValue performADDCombine(SDNode *N, SelectionDAG &DAG,
                                  TargetLowering::DAGCombinerInfo &DCI,
                                  const MipsSubtarget &Subtarget) {
   // (add v0 (mul v1, v2)) => (madd v1, v2, v0)
   if (DCI.isBeforeLegalizeOps()) {
     if (Subtarget.hasMips32() && !Subtarget.hasMips32r6() &&
         !Subtarget.inMips16Mode() && N->getValueType(0) == MVT::i64)
       return performMADD_MSUBCombine(N, DAG, Subtarget);
 
     return SDValue();
   }
 
   // (add v0, (add v1, abs_lo(tjt))) => (add (add v0, v1), abs_lo(tjt))
   SDValue Add = N->getOperand(1);
 
   if (Add.getOpcode() != ISD::ADD)
     return SDValue();
 
   SDValue Lo = Add.getOperand(1);
 
   if ((Lo.getOpcode() != MipsISD::Lo) ||
       (Lo.getOperand(0).getOpcode() != ISD::TargetJumpTable))
     return SDValue();
 
   EVT ValTy = N->getValueType(0);
   SDLoc DL(N);
 
   SDValue Add1 = DAG.getNode(ISD::ADD, DL, ValTy, N->getOperand(0),
                              Add.getOperand(0));
   return DAG.getNode(ISD::ADD, DL, ValTy, Add1, Lo);
 }
 
 static SDValue performSHLCombine(SDNode *N, SelectionDAG &DAG,
                                  TargetLowering::DAGCombinerInfo &DCI,
                                  const MipsSubtarget &Subtarget) {
   // Pattern match CINS.
   //  $dst = shl (and $src , imm), pos
   //  => cins $dst, $src, pos, size
 
   if (DCI.isBeforeLegalizeOps() || !Subtarget.hasCnMips())
     return SDValue();
 
   SDValue FirstOperand = N->getOperand(0);
   unsigned FirstOperandOpc = FirstOperand.getOpcode();
   SDValue SecondOperand = N->getOperand(1);
   EVT ValTy = N->getValueType(0);
   SDLoc DL(N);
 
   uint64_t Pos = 0, SMPos, SMSize;
   ConstantSDNode *CN;
   SDValue NewOperand;
 
   // The second operand of the shift must be an immediate.
   if (!(CN = dyn_cast<ConstantSDNode>(SecondOperand)))
     return SDValue();
 
   Pos = CN->getZExtValue();
 
   if (Pos >= ValTy.getSizeInBits())
     return SDValue();
 
   if (FirstOperandOpc != ISD::AND)
     return SDValue();
 
   // AND's second operand must be a shifted mask.
   if (!(CN = dyn_cast<ConstantSDNode>(FirstOperand.getOperand(1))) ||
       !isShiftedMask(CN->getZExtValue(), SMPos, SMSize))
     return SDValue();
 
   // Return if the shifted mask does not start at bit 0 or the sum of its size
   // and Pos exceeds the word's size.
   if (SMPos != 0 || SMSize > 32 || Pos + SMSize > ValTy.getSizeInBits())
     return SDValue();
 
   NewOperand = FirstOperand.getOperand(0);
   // SMSize is 'location' (position) in this case, not size.
   SMSize--;
 
   return DAG.getNode(MipsISD::CIns, DL, ValTy, NewOperand,
                      DAG.getConstant(Pos, DL, MVT::i32),
                      DAG.getConstant(SMSize, DL, MVT::i32));
 }
 
 SDValue  MipsTargetLowering::PerformDAGCombine(SDNode *N, DAGCombinerInfo &DCI)
   const {
   SelectionDAG &DAG = DCI.DAG;
   unsigned Opc = N->getOpcode();
 
   switch (Opc) {
   default: break;
   case ISD::SDIVREM:
   case ISD::UDIVREM:
     return performDivRemCombine(N, DAG, DCI, Subtarget);
   case ISD::SELECT:
     return performSELECTCombine(N, DAG, DCI, Subtarget);
   case MipsISD::CMovFP_F:
   case MipsISD::CMovFP_T:
     return performCMovFPCombine(N, DAG, DCI, Subtarget);
   case ISD::AND:
     return performANDCombine(N, DAG, DCI, Subtarget);
   case ISD::OR:
     return performORCombine(N, DAG, DCI, Subtarget);
   case ISD::ADD:
     return performADDCombine(N, DAG, DCI, Subtarget);
   case ISD::SHL:
     return performSHLCombine(N, DAG, DCI, Subtarget);
   case ISD::SUB:
     return performSUBCombine(N, DAG, DCI, Subtarget);
   }
 
   return SDValue();
 }
 
 bool MipsTargetLowering::isCheapToSpeculateCttz() const {
   return Subtarget.hasMips32();
 }
 
 bool MipsTargetLowering::isCheapToSpeculateCtlz() const {
   return Subtarget.hasMips32();
 }
 
 bool MipsTargetLowering::shouldFoldConstantShiftPairToMask(
     const SDNode *N, CombineLevel Level) const {
   if (N->getOperand(0).getValueType().isVector())
     return false;
   return true;
 }
 
 void
 MipsTargetLowering::ReplaceNodeResults(SDNode *N,
                                        SmallVectorImpl<SDValue> &Results,
                                        SelectionDAG &DAG) const {
   return LowerOperationWrapper(N, Results, DAG);
 }
 
 SDValue MipsTargetLowering::
 LowerOperation(SDValue Op, SelectionDAG &DAG) const
 {
   switch (Op.getOpcode())
   {
   case ISD::BRCOND:             return lowerBRCOND(Op, DAG);
   case ISD::ConstantPool:       return lowerConstantPool(Op, DAG);
   case ISD::GlobalAddress:      return lowerGlobalAddress(Op, DAG);
   case ISD::BlockAddress:       return lowerBlockAddress(Op, DAG);
   case ISD::GlobalTLSAddress:   return lowerGlobalTLSAddress(Op, DAG);
   case ISD::JumpTable:          return lowerJumpTable(Op, DAG);
   case ISD::SELECT:             return lowerSELECT(Op, DAG);
   case ISD::SETCC:              return lowerSETCC(Op, DAG);
   case ISD::VASTART:            return lowerVASTART(Op, DAG);
   case ISD::VAARG:              return lowerVAARG(Op, DAG);
   case ISD::FCOPYSIGN:          return lowerFCOPYSIGN(Op, DAG);
   case ISD::FABS:               return lowerFABS(Op, DAG);
   case ISD::FRAMEADDR:          return lowerFRAMEADDR(Op, DAG);
   case ISD::RETURNADDR:         return lowerRETURNADDR(Op, DAG);
   case ISD::EH_RETURN:          return lowerEH_RETURN(Op, DAG);
   case ISD::ATOMIC_FENCE:       return lowerATOMIC_FENCE(Op, DAG);
   case ISD::SHL_PARTS:          return lowerShiftLeftParts(Op, DAG);
   case ISD::SRA_PARTS:          return lowerShiftRightParts(Op, DAG, true);
   case ISD::SRL_PARTS:          return lowerShiftRightParts(Op, DAG, false);
   case ISD::LOAD:               return lowerLOAD(Op, DAG);
   case ISD::STORE:              return lowerSTORE(Op, DAG);
   case ISD::EH_DWARF_CFA:       return lowerEH_DWARF_CFA(Op, DAG);
   case ISD::FP_TO_SINT:         return lowerFP_TO_SINT(Op, DAG);
   }
   return SDValue();
 }
 
 //===----------------------------------------------------------------------===//
 //  Lower helper functions
 //===----------------------------------------------------------------------===//
 
 // addLiveIn - This helper function adds the specified physical register to the
 // MachineFunction as a live in value.  It also creates a corresponding
 // virtual register for it.
 static unsigned
 addLiveIn(MachineFunction &MF, unsigned PReg, const TargetRegisterClass *RC)
 {
   Register VReg = MF.getRegInfo().createVirtualRegister(RC);
   MF.getRegInfo().addLiveIn(PReg, VReg);
   return VReg;
 }
 
 static MachineBasicBlock *insertDivByZeroTrap(MachineInstr &MI,
                                               MachineBasicBlock &MBB,
                                               const TargetInstrInfo &TII,
                                               bool Is64Bit, bool IsMicroMips) {
   if (NoZeroDivCheck)
     return &MBB;
 
   // Insert instruction "teq $divisor_reg, $zero, 7".
   MachineBasicBlock::iterator I(MI);
   MachineInstrBuilder MIB;
   MachineOperand &Divisor = MI.getOperand(2);
   MIB = BuildMI(MBB, std::next(I), MI.getDebugLoc(),
                 TII.get(IsMicroMips ? Mips::TEQ_MM : Mips::TEQ))
             .addReg(Divisor.getReg(), getKillRegState(Divisor.isKill()))
             .addReg(Mips::ZERO)
             .addImm(7);
 
   // Use the 32-bit sub-register if this is a 64-bit division.
   if (Is64Bit)
     MIB->getOperand(0).setSubReg(Mips::sub_32);
 
   // Clear Divisor's kill flag.
   Divisor.setIsKill(false);
 
   // We would normally delete the original instruction here but in this case
   // we only needed to inject an additional instruction rather than replace it.
 
   return &MBB;
 }
 
 MachineBasicBlock *
 MipsTargetLowering::EmitInstrWithCustomInserter(MachineInstr &MI,
                                                 MachineBasicBlock *BB) const {
   switch (MI.getOpcode()) {
   default:
     llvm_unreachable("Unexpected instr type to insert");
   case Mips::ATOMIC_LOAD_ADD_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_LOAD_ADD_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_LOAD_ADD_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_LOAD_ADD_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::ATOMIC_LOAD_AND_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_LOAD_AND_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_LOAD_AND_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_LOAD_AND_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::ATOMIC_LOAD_OR_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_LOAD_OR_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_LOAD_OR_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_LOAD_OR_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::ATOMIC_LOAD_XOR_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_LOAD_XOR_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_LOAD_XOR_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_LOAD_XOR_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::ATOMIC_LOAD_NAND_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_LOAD_NAND_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_LOAD_NAND_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_LOAD_NAND_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::ATOMIC_LOAD_SUB_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_LOAD_SUB_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_LOAD_SUB_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_LOAD_SUB_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::ATOMIC_SWAP_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_SWAP_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_SWAP_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_SWAP_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::ATOMIC_CMP_SWAP_I8:
     return emitAtomicCmpSwapPartword(MI, BB, 1);
   case Mips::ATOMIC_CMP_SWAP_I16:
     return emitAtomicCmpSwapPartword(MI, BB, 2);
   case Mips::ATOMIC_CMP_SWAP_I32:
     return emitAtomicCmpSwap(MI, BB);
   case Mips::ATOMIC_CMP_SWAP_I64:
     return emitAtomicCmpSwap(MI, BB);
 
   case Mips::ATOMIC_LOAD_MIN_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_LOAD_MIN_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_LOAD_MIN_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_LOAD_MIN_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::ATOMIC_LOAD_MAX_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_LOAD_MAX_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_LOAD_MAX_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_LOAD_MAX_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::ATOMIC_LOAD_UMIN_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_LOAD_UMIN_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_LOAD_UMIN_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_LOAD_UMIN_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::ATOMIC_LOAD_UMAX_I8:
     return emitAtomicBinaryPartword(MI, BB, 1);
   case Mips::ATOMIC_LOAD_UMAX_I16:
     return emitAtomicBinaryPartword(MI, BB, 2);
   case Mips::ATOMIC_LOAD_UMAX_I32:
     return emitAtomicBinary(MI, BB);
   case Mips::ATOMIC_LOAD_UMAX_I64:
     return emitAtomicBinary(MI, BB);
 
   case Mips::PseudoSDIV:
   case Mips::PseudoUDIV:
   case Mips::DIV:
   case Mips::DIVU:
   case Mips::MOD:
   case Mips::MODU:
     return insertDivByZeroTrap(MI, *BB, *Subtarget.getInstrInfo(), false,
                                false);
   case Mips::SDIV_MM_Pseudo:
   case Mips::UDIV_MM_Pseudo:
   case Mips::SDIV_MM:
   case Mips::UDIV_MM:
   case Mips::DIV_MMR6:
   case Mips::DIVU_MMR6:
   case Mips::MOD_MMR6:
   case Mips::MODU_MMR6:
     return insertDivByZeroTrap(MI, *BB, *Subtarget.getInstrInfo(), false, true);
   case Mips::PseudoDSDIV:
   case Mips::PseudoDUDIV:
   case Mips::DDIV:
   case Mips::DDIVU:
   case Mips::DMOD:
   case Mips::DMODU:
     return insertDivByZeroTrap(MI, *BB, *Subtarget.getInstrInfo(), true, false);
 
   case Mips::PseudoSELECT_I:
   case Mips::PseudoSELECT_I64:
   case Mips::PseudoSELECT_S:
   case Mips::PseudoSELECT_D32:
   case Mips::PseudoSELECT_D64:
     return emitPseudoSELECT(MI, BB, false, Mips::BNE);
   case Mips::PseudoSELECTFP_F_I:
   case Mips::PseudoSELECTFP_F_I64:
   case Mips::PseudoSELECTFP_F_S:
   case Mips::PseudoSELECTFP_F_D32:
   case Mips::PseudoSELECTFP_F_D64:
     return emitPseudoSELECT(MI, BB, true, Mips::BC1F);
   case Mips::PseudoSELECTFP_T_I:
   case Mips::PseudoSELECTFP_T_I64:
   case Mips::PseudoSELECTFP_T_S:
   case Mips::PseudoSELECTFP_T_D32:
   case Mips::PseudoSELECTFP_T_D64:
     return emitPseudoSELECT(MI, BB, true, Mips::BC1T);
   case Mips::PseudoD_SELECT_I:
   case Mips::PseudoD_SELECT_I64:
     return emitPseudoD_SELECT(MI, BB);
   case Mips::LDR_W:
     return emitLDR_W(MI, BB);
   case Mips::LDR_D:
     return emitLDR_D(MI, BB);
   case Mips::STR_W:
     return emitSTR_W(MI, BB);
   case Mips::STR_D:
     return emitSTR_D(MI, BB);
   }
 }
 
 // This function also handles Mips::ATOMIC_SWAP_I32 (when BinOpcode == 0), and
 // Mips::ATOMIC_LOAD_NAND_I32 (when Nand == true)
 MachineBasicBlock *
 MipsTargetLowering::emitAtomicBinary(MachineInstr &MI,
                                      MachineBasicBlock *BB) const {
 
   MachineFunction *MF = BB->getParent();
   MachineRegisterInfo &RegInfo = MF->getRegInfo();
   const TargetInstrInfo *TII = Subtarget.getInstrInfo();
   DebugLoc DL = MI.getDebugLoc();
 
   unsigned AtomicOp;
   bool NeedsAdditionalReg = false;
   switch (MI.getOpcode()) {
   case Mips::ATOMIC_LOAD_ADD_I32:
     AtomicOp = Mips::ATOMIC_LOAD_ADD_I32_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_SUB_I32:
     AtomicOp = Mips::ATOMIC_LOAD_SUB_I32_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_AND_I32:
     AtomicOp = Mips::ATOMIC_LOAD_AND_I32_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_OR_I32:
     AtomicOp = Mips::ATOMIC_LOAD_OR_I32_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_XOR_I32:
     AtomicOp = Mips::ATOMIC_LOAD_XOR_I32_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_NAND_I32:
     AtomicOp = Mips::ATOMIC_LOAD_NAND_I32_POSTRA;
     break;
   case Mips::ATOMIC_SWAP_I32:
     AtomicOp = Mips::ATOMIC_SWAP_I32_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_ADD_I64:
     AtomicOp = Mips::ATOMIC_LOAD_ADD_I64_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_SUB_I64:
     AtomicOp = Mips::ATOMIC_LOAD_SUB_I64_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_AND_I64:
     AtomicOp = Mips::ATOMIC_LOAD_AND_I64_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_OR_I64:
     AtomicOp = Mips::ATOMIC_LOAD_OR_I64_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_XOR_I64:
     AtomicOp = Mips::ATOMIC_LOAD_XOR_I64_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_NAND_I64:
     AtomicOp = Mips::ATOMIC_LOAD_NAND_I64_POSTRA;
     break;
   case Mips::ATOMIC_SWAP_I64:
     AtomicOp = Mips::ATOMIC_SWAP_I64_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_MIN_I32:
     AtomicOp = Mips::ATOMIC_LOAD_MIN_I32_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_MAX_I32:
     AtomicOp = Mips::ATOMIC_LOAD_MAX_I32_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_UMIN_I32:
     AtomicOp = Mips::ATOMIC_LOAD_UMIN_I32_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_UMAX_I32:
     AtomicOp = Mips::ATOMIC_LOAD_UMAX_I32_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_MIN_I64:
     AtomicOp = Mips::ATOMIC_LOAD_MIN_I64_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_MAX_I64:
     AtomicOp = Mips::ATOMIC_LOAD_MAX_I64_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_UMIN_I64:
     AtomicOp = Mips::ATOMIC_LOAD_UMIN_I64_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_UMAX_I64:
     AtomicOp = Mips::ATOMIC_LOAD_UMAX_I64_POSTRA;
     NeedsAdditionalReg = true;
     break;
   default:
     llvm_unreachable("Unknown pseudo atomic for replacement!");
   }
 
   Register OldVal = MI.getOperand(0).getReg();
   Register Ptr = MI.getOperand(1).getReg();
   Register Incr = MI.getOperand(2).getReg();
   Register Scratch = RegInfo.createVirtualRegister(RegInfo.getRegClass(OldVal));
 
   MachineBasicBlock::iterator II(MI);
 
   // The scratch registers here with the EarlyClobber | Define | Implicit
   // flags is used to persuade the register allocator and the machine
   // verifier to accept the usage of this register. This has to be a real
   // register which has an UNDEF value but is dead after the instruction which
   // is unique among the registers chosen for the instruction.
 
   // The EarlyClobber flag has the semantic properties that the operand it is
   // attached to is clobbered before the rest of the inputs are read. Hence it
   // must be unique among the operands to the instruction.
   // The Define flag is needed to coerce the machine verifier that an Undef
   // value isn't a problem.
   // The Dead flag is needed as the value in scratch isn't used by any other
   // instruction. Kill isn't used as Dead is more precise.
   // The implicit flag is here due to the interaction between the other flags
   // and the machine verifier.
 
   // For correctness purpose, a new pseudo is introduced here. We need this
   // new pseudo, so that FastRegisterAllocator does not see an ll/sc sequence
   // that is spread over >1 basic blocks. A register allocator which
   // introduces (or any codegen infact) a store, can violate the expectations
   // of the hardware.
   //
   // An atomic read-modify-write sequence starts with a linked load
   // instruction and ends with a store conditional instruction. The atomic
   // read-modify-write sequence fails if any of the following conditions
   // occur between the execution of ll and sc:
   //   * A coherent store is completed by another process or coherent I/O
   //     module into the block of synchronizable physical memory containing
   //     the word. The size and alignment of the block is
   //     implementation-dependent.
   //   * A coherent store is executed between an LL and SC sequence on the
   //     same processor to the block of synchornizable physical memory
   //     containing the word.
   //
 
   Register PtrCopy = RegInfo.createVirtualRegister(RegInfo.getRegClass(Ptr));
   Register IncrCopy = RegInfo.createVirtualRegister(RegInfo.getRegClass(Incr));
 
   BuildMI(*BB, II, DL, TII->get(Mips::COPY), IncrCopy).addReg(Incr);
   BuildMI(*BB, II, DL, TII->get(Mips::COPY), PtrCopy).addReg(Ptr);
 
   MachineInstrBuilder MIB =
       BuildMI(*BB, II, DL, TII->get(AtomicOp))
           .addReg(OldVal, RegState::Define | RegState::EarlyClobber)
           .addReg(PtrCopy)
           .addReg(IncrCopy)
           .addReg(Scratch, RegState::Define | RegState::EarlyClobber |
                                RegState::Implicit | RegState::Dead);
   if (NeedsAdditionalReg) {
     Register Scratch2 =
         RegInfo.createVirtualRegister(RegInfo.getRegClass(OldVal));
     MIB.addReg(Scratch2, RegState::Define | RegState::EarlyClobber |
                              RegState::Implicit | RegState::Dead);
   }
 
   MI.eraseFromParent();
 
   return BB;
 }
 
 MachineBasicBlock *MipsTargetLowering::emitSignExtendToI32InReg(
     MachineInstr &MI, MachineBasicBlock *BB, unsigned Size, unsigned DstReg,
     unsigned SrcReg) const {
   const TargetInstrInfo *TII = Subtarget.getInstrInfo();
   const DebugLoc &DL = MI.getDebugLoc();
 
   if (Subtarget.hasMips32r2() && Size == 1) {
     BuildMI(BB, DL, TII->get(Mips::SEB), DstReg).addReg(SrcReg);
     return BB;
   }
 
   if (Subtarget.hasMips32r2() && Size == 2) {
     BuildMI(BB, DL, TII->get(Mips::SEH), DstReg).addReg(SrcReg);
     return BB;
   }
 
   MachineFunction *MF = BB->getParent();
   MachineRegisterInfo &RegInfo = MF->getRegInfo();
   const TargetRegisterClass *RC = getRegClassFor(MVT::i32);
   Register ScrReg = RegInfo.createVirtualRegister(RC);
 
   assert(Size < 32);
   int64_t ShiftImm = 32 - (Size * 8);
 
   BuildMI(BB, DL, TII->get(Mips::SLL), ScrReg).addReg(SrcReg).addImm(ShiftImm);
   BuildMI(BB, DL, TII->get(Mips::SRA), DstReg).addReg(ScrReg).addImm(ShiftImm);
 
   return BB;
 }
 
 MachineBasicBlock *MipsTargetLowering::emitAtomicBinaryPartword(
     MachineInstr &MI, MachineBasicBlock *BB, unsigned Size) const {
   assert((Size == 1 || Size == 2) &&
          "Unsupported size for EmitAtomicBinaryPartial.");
 
   MachineFunction *MF = BB->getParent();
   MachineRegisterInfo &RegInfo = MF->getRegInfo();
   const TargetRegisterClass *RC = getRegClassFor(MVT::i32);
   const bool ArePtrs64bit = ABI.ArePtrs64bit();
   const TargetRegisterClass *RCp =
     getRegClassFor(ArePtrs64bit ? MVT::i64 : MVT::i32);
   const TargetInstrInfo *TII = Subtarget.getInstrInfo();
   DebugLoc DL = MI.getDebugLoc();
 
   Register Dest = MI.getOperand(0).getReg();
   Register Ptr = MI.getOperand(1).getReg();
   Register Incr = MI.getOperand(2).getReg();
 
   Register AlignedAddr = RegInfo.createVirtualRegister(RCp);
   Register ShiftAmt = RegInfo.createVirtualRegister(RC);
   Register Mask = RegInfo.createVirtualRegister(RC);
   Register Mask2 = RegInfo.createVirtualRegister(RC);
   Register Incr2 = RegInfo.createVirtualRegister(RC);
   Register MaskLSB2 = RegInfo.createVirtualRegister(RCp);
   Register PtrLSB2 = RegInfo.createVirtualRegister(RC);
   Register MaskUpper = RegInfo.createVirtualRegister(RC);
   Register Scratch = RegInfo.createVirtualRegister(RC);
   Register Scratch2 = RegInfo.createVirtualRegister(RC);
   Register Scratch3 = RegInfo.createVirtualRegister(RC);
 
   unsigned AtomicOp = 0;
   bool NeedsAdditionalReg = false;
   switch (MI.getOpcode()) {
   case Mips::ATOMIC_LOAD_NAND_I8:
     AtomicOp = Mips::ATOMIC_LOAD_NAND_I8_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_NAND_I16:
     AtomicOp = Mips::ATOMIC_LOAD_NAND_I16_POSTRA;
     break;
   case Mips::ATOMIC_SWAP_I8:
     AtomicOp = Mips::ATOMIC_SWAP_I8_POSTRA;
     break;
   case Mips::ATOMIC_SWAP_I16:
     AtomicOp = Mips::ATOMIC_SWAP_I16_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_ADD_I8:
     AtomicOp = Mips::ATOMIC_LOAD_ADD_I8_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_ADD_I16:
     AtomicOp = Mips::ATOMIC_LOAD_ADD_I16_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_SUB_I8:
     AtomicOp = Mips::ATOMIC_LOAD_SUB_I8_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_SUB_I16:
     AtomicOp = Mips::ATOMIC_LOAD_SUB_I16_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_AND_I8:
     AtomicOp = Mips::ATOMIC_LOAD_AND_I8_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_AND_I16:
     AtomicOp = Mips::ATOMIC_LOAD_AND_I16_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_OR_I8:
     AtomicOp = Mips::ATOMIC_LOAD_OR_I8_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_OR_I16:
     AtomicOp = Mips::ATOMIC_LOAD_OR_I16_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_XOR_I8:
     AtomicOp = Mips::ATOMIC_LOAD_XOR_I8_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_XOR_I16:
     AtomicOp = Mips::ATOMIC_LOAD_XOR_I16_POSTRA;
     break;
   case Mips::ATOMIC_LOAD_MIN_I8:
     AtomicOp = Mips::ATOMIC_LOAD_MIN_I8_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_MIN_I16:
     AtomicOp = Mips::ATOMIC_LOAD_MIN_I16_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_MAX_I8:
     AtomicOp = Mips::ATOMIC_LOAD_MAX_I8_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_MAX_I16:
     AtomicOp = Mips::ATOMIC_LOAD_MAX_I16_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_UMIN_I8:
     AtomicOp = Mips::ATOMIC_LOAD_UMIN_I8_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_UMIN_I16:
     AtomicOp = Mips::ATOMIC_LOAD_UMIN_I16_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_UMAX_I8:
     AtomicOp = Mips::ATOMIC_LOAD_UMAX_I8_POSTRA;
     NeedsAdditionalReg = true;
     break;
   case Mips::ATOMIC_LOAD_UMAX_I16:
     AtomicOp = Mips::ATOMIC_LOAD_UMAX_I16_POSTRA;
     NeedsAdditionalReg = true;
     break;
   default:
     llvm_unreachable("Unknown subword atomic pseudo for expansion!");
   }
 
   // insert new blocks after the current block
   const BasicBlock *LLVM_BB = BB->getBasicBlock();
   MachineBasicBlock *exitMBB = MF->CreateMachineBasicBlock(LLVM_BB);
   MachineFunction::iterator It = ++BB->getIterator();
   MF->insert(It, exitMBB);
 
   // Transfer the remainder of BB and its successor edges to exitMBB.
   exitMBB->splice(exitMBB->begin(), BB,
                   std::next(MachineBasicBlock::iterator(MI)), BB->end());
   exitMBB->transferSuccessorsAndUpdatePHIs(BB);
 
   BB->addSuccessor(exitMBB, BranchProbability::getOne());
 
   //  thisMBB:
   //    addiu   masklsb2,$0,-4                # 0xfffffffc
   //    and     alignedaddr,ptr,masklsb2
   //    andi    ptrlsb2,ptr,3
   //    sll     shiftamt,ptrlsb2,3
   //    ori     maskupper,$0,255               # 0xff
   //    sll     mask,maskupper,shiftamt
   //    nor     mask2,$0,mask
   //    sll     incr2,incr,shiftamt
 
   int64_t MaskImm = (Size == 1) ? 255 : 65535;
   BuildMI(BB, DL, TII->get(ABI.GetPtrAddiuOp()), MaskLSB2)
     .addReg(ABI.GetNullPtr()).addImm(-4);
   BuildMI(BB, DL, TII->get(ABI.GetPtrAndOp()), AlignedAddr)
     .addReg(Ptr).addReg(MaskLSB2);
   BuildMI(BB, DL, TII->get(Mips::ANDi), PtrLSB2)
       .addReg(Ptr, 0, ArePtrs64bit ? Mips::sub_32 : 0).addImm(3);
   if (Subtarget.isLittle()) {
     BuildMI(BB, DL, TII->get(Mips::SLL), ShiftAmt).addReg(PtrLSB2).addImm(3);
   } else {
     Register Off = RegInfo.createVirtualRegister(RC);
     BuildMI(BB, DL, TII->get(Mips::XORi), Off)
       .addReg(PtrLSB2).addImm((Size == 1) ? 3 : 2);
     BuildMI(BB, DL, TII->get(Mips::SLL), ShiftAmt).addReg(Off).addImm(3);
   }
   BuildMI(BB, DL, TII->get(Mips::ORi), MaskUpper)
     .addReg(Mips::ZERO).addImm(MaskImm);
   BuildMI(BB, DL, TII->get(Mips::SLLV), Mask)
     .addReg(MaskUpper).addReg(ShiftAmt);
   BuildMI(BB, DL, TII->get(Mips::NOR), Mask2).addReg(Mips::ZERO).addReg(Mask);
   BuildMI(BB, DL, TII->get(Mips::SLLV), Incr2).addReg(Incr).addReg(ShiftAmt);
 
 
   // The purposes of the flags on the scratch registers is explained in
   // emitAtomicBinary. In summary, we need a scratch register which is going to
   // be undef, that is unique among registers chosen for the instruction.
 
   MachineInstrBuilder MIB =
       BuildMI(BB, DL, TII->get(AtomicOp))
           .addReg(Dest, RegState::Define | RegState::EarlyClobber)
           .addReg(AlignedAddr)
           .addReg(Incr2)
           .addReg(Mask)
           .addReg(Mask2)
           .addReg(ShiftAmt)
           .addReg(Scratch, RegState::EarlyClobber | RegState::Define |
                                RegState::Dead | RegState::Implicit)
           .addReg(Scratch2, RegState::EarlyClobber | RegState::Define |
                                 RegState::Dead | RegState::Implicit)
           .addReg(Scratch3, RegState::EarlyClobber | RegState::Define |
                                 RegState::Dead | RegState::Implicit);
   if (NeedsAdditionalReg) {
     Register Scratch4 = RegInfo.createVirtualRegister(RC);
     MIB.addReg(Scratch4, RegState::EarlyClobber | RegState::Define |
                              RegState::Dead | RegState::Implicit);
   }
 
   MI.eraseFromParent(); // The instruction is gone now.
 
   return exitMBB;
 }
 
 // Lower atomic compare and swap to a pseudo instruction, taking care to
 // define a scratch register for the pseudo instruction's expansion. The
 // instruction is expanded after the register allocator as to prevent
 // the insertion of stores between the linked load and the store conditional.
 
 MachineBasicBlock *
 MipsTargetLowering::emitAtomicCmpSwap(MachineInstr &MI,
                                       MachineBasicBlock *BB) const {
 
   assert((MI.getOpcode() == Mips::ATOMIC_CMP_SWAP_I32 ||
           MI.getOpcode() == Mips::ATOMIC_CMP_SWAP_I64) &&
          "Unsupported atomic pseudo for EmitAtomicCmpSwap.");
 
   const unsigned Size = MI.getOpcode() == Mips::ATOMIC_CMP_SWAP_I32 ? 4 : 8;
 
   MachineFunction *MF = BB->getParent();
   MachineRegisterInfo &MRI = MF->getRegInfo();
   const TargetRegisterClass *RC = getRegClassFor(MVT::getIntegerVT(Size * 8));
   const TargetInstrInfo *TII = Subtarget.getInstrInfo();
   DebugLoc DL = MI.getDebugLoc();
 
   unsigned AtomicOp = MI.getOpcode() == Mips::ATOMIC_CMP_SWAP_I32
                           ? Mips::ATOMIC_CMP_SWAP_I32_POSTRA
                           : Mips::ATOMIC_CMP_SWAP_I64_POSTRA;
   Register Dest = MI.getOperand(0).getReg();
   Register Ptr = MI.getOperand(1).getReg();
   Register OldVal = MI.getOperand(2).getReg();
   Register NewVal = MI.getOperand(3).getReg();
 
   Register Scratch = MRI.createVirtualRegister(RC);
   MachineBasicBlock::iterator II(MI);
 
   // We need to create copies of the various registers and kill them at the
   // atomic pseudo. If the copies are not made, when the atomic is expanded
   // after fast register allocation, the spills will end up outside of the
   // blocks that their values are defined in, causing livein errors.
 
   Register PtrCopy = MRI.createVirtualRegister(MRI.getRegClass(Ptr));
   Register OldValCopy = MRI.createVirtualRegister(MRI.getRegClass(OldVal));
   Register NewValCopy = MRI.createVirtualRegister(MRI.getRegClass(NewVal));
 
   BuildMI(*BB, II, DL, TII->get(Mips::COPY), PtrCopy).addReg(Ptr);
   BuildMI(*BB, II, DL, TII->get(Mips::COPY), OldValCopy).addReg(OldVal);
   BuildMI(*BB, II, DL, TII->get(Mips::COPY), NewValCopy).addReg(NewVal);
 
   // The purposes of the flags on the scratch registers is explained in
   // emitAtomicBinary. In summary, we need a scratch register which is going to
   // be undef, that is unique among registers chosen for the instruction.
 
   BuildMI(*BB, II, DL, TII->get(AtomicOp))
       .addReg(Dest, RegState::Define | RegState::EarlyClobber)
       .addReg(PtrCopy, RegState::Kill)
       .addReg(OldValCopy, RegState::Kill)
       .addReg(NewValCopy, RegState::Kill)
       .addReg(Scratch, RegState::EarlyClobber | RegState::Define |
                            RegState::Dead | RegState::Implicit);
 
   MI.eraseFromParent(); // The instruction is gone now.
 
   return BB;
 }
 
 MachineBasicBlock *MipsTargetLowering::emitAtomicCmpSwapPartword(
     MachineInstr &MI, MachineBasicBlock *BB, unsigned Size) const {
   assert((Size == 1 || Size == 2) &&
       "Unsupported size for EmitAtomicCmpSwapPartial.");
 
   MachineFunction *MF = BB->getParent();
   MachineRegisterInfo &RegInfo = MF->getRegInfo();
   const TargetRegisterClass *RC = getRegClassFor(MVT::i32);
   const bool ArePtrs64bit = ABI.ArePtrs64bit();
   const TargetRegisterClass *RCp =
     getRegClassFor(ArePtrs64bit ? MVT::i64 : MVT::i32);
   const TargetInstrInfo *TII = Subtarget.getInstrInfo();
   DebugLoc DL = MI.getDebugLoc();
 
   Register Dest = MI.getOperand(0).getReg();
   Register Ptr = MI.getOperand(1).getReg();
   Register CmpVal = MI.getOperand(2).getReg();
   Register NewVal = MI.getOperand(3).getReg();
 
   Register AlignedAddr = RegInfo.createVirtualRegister(RCp);
   Register ShiftAmt = RegInfo.createVirtualRegister(RC);
   Register Mask = RegInfo.createVirtualRegister(RC);
   Register Mask2 = RegInfo.createVirtualRegister(RC);
   Register ShiftedCmpVal = RegInfo.createVirtualRegister(RC);
   Register ShiftedNewVal = RegInfo.createVirtualRegister(RC);
   Register MaskLSB2 = RegInfo.createVirtualRegister(RCp);
   Register PtrLSB2 = RegInfo.createVirtualRegister(RC);
   Register MaskUpper = RegInfo.createVirtualRegister(RC);
   Register MaskedCmpVal = RegInfo.createVirtualRegister(RC);
   Register MaskedNewVal = RegInfo.createVirtualRegister(RC);
   unsigned AtomicOp = MI.getOpcode() == Mips::ATOMIC_CMP_SWAP_I8
                           ? Mips::ATOMIC_CMP_SWAP_I8_POSTRA
                           : Mips::ATOMIC_CMP_SWAP_I16_POSTRA;
 
   // The scratch registers here with the EarlyClobber | Define | Dead | Implicit
   // flags are used to coerce the register allocator and the machine verifier to
   // accept the usage of these registers.
   // The EarlyClobber flag has the semantic properties that the operand it is
   // attached to is clobbered before the rest of the inputs are read. Hence it
   // must be unique among the operands to the instruction.
   // The Define flag is needed to coerce the machine verifier that an Undef
   // value isn't a problem.
   // The Dead flag is needed as the value in scratch isn't used by any other
   // instruction. Kill isn't used as Dead is more precise.
   Register Scratch = RegInfo.createVirtualRegister(RC);
   Register Scratch2 = RegInfo.createVirtualRegister(RC);
 
   // insert new blocks after the current block
   const BasicBlock *LLVM_BB = BB->getBasicBlock();
   MachineBasicBlock *exitMBB = MF->CreateMachineBasicBlock(LLVM_BB);
   MachineFunction::iterator It = ++BB->getIterator();
   MF->insert(It, exitMBB);
 
   // Transfer the remainder of BB and its successor edges to exitMBB.
   exitMBB->splice(exitMBB->begin(), BB,
                   std::next(MachineBasicBlock::iterator(MI)), BB->end());
   exitMBB->transferSuccessorsAndUpdatePHIs(BB);
 
   BB->addSuccessor(exitMBB, BranchProbability::getOne());
 
   //  thisMBB:
   //    addiu   masklsb2,$0,-4                # 0xfffffffc
   //    and     alignedaddr,ptr,masklsb2
   //    andi    ptrlsb2,ptr,3
   //    xori    ptrlsb2,ptrlsb2,3              # Only for BE
   //    sll     shiftamt,ptrlsb2,3
   //    ori     maskupper,$0,255               # 0xff
   //    sll     mask,maskupper,shiftamt
   //    nor     mask2,$0,mask
   //    andi    maskedcmpval,cmpval,255
   //    sll     shiftedcmpval,maskedcmpval,shiftamt
   //    andi    maskednewval,newval,255
   //    sll     shiftednewval,maskednewval,shiftamt
   int64_t MaskImm = (Size == 1) ? 255 : 65535;
   BuildMI(BB, DL, TII->get(ArePtrs64bit ? Mips::DADDiu : Mips::ADDiu), MaskLSB2)
     .addReg(ABI.GetNullPtr()).addImm(-4);
   BuildMI(BB, DL, TII->get(ArePtrs64bit ? Mips::AND64 : Mips::AND), AlignedAddr)
     .addReg(Ptr).addReg(MaskLSB2);
   BuildMI(BB, DL, TII->get(Mips::ANDi), PtrLSB2)
       .addReg(Ptr, 0, ArePtrs64bit ? Mips::sub_32 : 0).addImm(3);
   if (Subtarget.isLittle()) {
     BuildMI(BB, DL, TII->get(Mips::SLL), ShiftAmt).addReg(PtrLSB2).addImm(3);
   } else {
     Register Off = RegInfo.createVirtualRegister(RC);
     BuildMI(BB, DL, TII->get(Mips::XORi), Off)
       .addReg(PtrLSB2).addImm((Size == 1) ? 3 : 2);
     BuildMI(BB, DL, TII->get(Mips::SLL), ShiftAmt).addReg(Off).addImm(3);
   }
   BuildMI(BB, DL, TII->get(Mips::ORi), MaskUpper)
     .addReg(Mips::ZERO).addImm(MaskImm);
   BuildMI(BB, DL, TII->get(Mips::SLLV), Mask)
     .addReg(MaskUpper).addReg(ShiftAmt);
   BuildMI(BB, DL, TII->get(Mips::NOR), Mask2).addReg(Mips::ZERO).addReg(Mask);
   BuildMI(BB, DL, TII->get(Mips::ANDi), MaskedCmpVal)
     .addReg(CmpVal).addImm(MaskImm);
   BuildMI(BB, DL, TII->get(Mips::SLLV), ShiftedCmpVal)
     .addReg(MaskedCmpVal).addReg(ShiftAmt);
   BuildMI(BB, DL, TII->get(Mips::ANDi), MaskedNewVal)
     .addReg(NewVal).addImm(MaskImm);
   BuildMI(BB, DL, TII->get(Mips::SLLV), ShiftedNewVal)
     .addReg(MaskedNewVal).addReg(ShiftAmt);
 
   // The purposes of the flags on the scratch registers are explained in
   // emitAtomicBinary. In summary, we need a scratch register which is going to
   // be undef, that is unique among the register chosen for the instruction.
 
   BuildMI(BB, DL, TII->get(AtomicOp))
       .addReg(Dest, RegState::Define | RegState::EarlyClobber)
       .addReg(AlignedAddr)
       .addReg(Mask)
       .addReg(ShiftedCmpVal)
       .addReg(Mask2)
       .addReg(ShiftedNewVal)
       .addReg(ShiftAmt)
       .addReg(Scratch, RegState::EarlyClobber | RegState::Define |
                            RegState::Dead | RegState::Implicit)
       .addReg(Scratch2, RegState::EarlyClobber | RegState::Define |
                             RegState::Dead | RegState::Implicit);
 
   MI.eraseFromParent(); // The instruction is gone now.
 
   return exitMBB;
 }
 
 SDValue MipsTargetLowering::lowerBRCOND(SDValue Op, SelectionDAG &DAG) const {
   // The first operand is the chain, the second is the condition, the third is
   // the block to branch to if the condition is true.
   SDValue Chain = Op.getOperand(0);
   SDValue Dest = Op.getOperand(2);
   SDLoc DL(Op);
 
   assert(!Subtarget.hasMips32r6() && !Subtarget.hasMips64r6());
   SDValue CondRes = createFPCmp(DAG, Op.getOperand(1));
 
   // Return if flag is not set by a floating point comparison.
   if (CondRes.getOpcode() != MipsISD::FPCmp)
     return Op;
 
   SDValue CCNode  = CondRes.getOperand(2);
   Mips::CondCode CC =
     (Mips::CondCode)cast<ConstantSDNode>(CCNode)->getZExtValue();
   unsigned Opc = invertFPCondCodeUser(CC) ? Mips::BRANCH_F : Mips::BRANCH_T;
   SDValue BrCode = DAG.getConstant(Opc, DL, MVT::i32);
   SDValue FCC0 = DAG.getRegister(Mips::FCC0, MVT::i32);
   return DAG.getNode(MipsISD::FPBrcond, DL, Op.getValueType(), Chain, BrCode,
                      FCC0, Dest, CondRes);
 }
 
 SDValue MipsTargetLowering::
 lowerSELECT(SDValue Op, SelectionDAG &DAG) const
 {
   assert(!Subtarget.hasMips32r6() && !Subtarget.hasMips64r6());
   SDValue Cond = createFPCmp(DAG, Op.getOperand(0));
 
   // Return if flag is not set by a floating point comparison.
   if (Cond.getOpcode() != MipsISD::FPCmp)
     return Op;
 
   return createCMovFP(DAG, Cond, Op.getOperand(1), Op.getOperand(2),
                       SDLoc(Op));
 }
 
 SDValue MipsTargetLowering::lowerSETCC(SDValue Op, SelectionDAG &DAG) const {
   assert(!Subtarget.hasMips32r6() && !Subtarget.hasMips64r6());
   SDValue Cond = createFPCmp(DAG, Op);
 
   assert(Cond.getOpcode() == MipsISD::FPCmp &&
          "Floating point operand expected.");
 
   SDLoc DL(Op);
   SDValue True  = DAG.getConstant(1, DL, MVT::i32);
   SDValue False = DAG.getConstant(0, DL, MVT::i32);
 
   return createCMovFP(DAG, Cond, True, False, DL);
 }
 
 SDValue MipsTargetLowering::lowerGlobalAddress(SDValue Op,
                                                SelectionDAG &DAG) const {
   EVT Ty = Op.getValueType();
   GlobalAddressSDNode *N = cast<GlobalAddressSDNode>(Op);
   const GlobalValue *GV = N->getGlobal();
 
   if (!isPositionIndependent()) {
     const MipsTargetObjectFile *TLOF =
         static_cast<const MipsTargetObjectFile *>(
             getTargetMachine().getObjFileLowering());
     const GlobalObject *GO = GV->getBaseObject();
     if (GO && TLOF->IsGlobalInSmallSection(GO, getTargetMachine()))
       // %gp_rel relocation
       return getAddrGPRel(N, SDLoc(N), Ty, DAG, ABI.IsN64());
 
                                 // %hi/%lo relocation
     return Subtarget.hasSym32() ? getAddrNonPIC(N, SDLoc(N), Ty, DAG)
                                 // %highest/%higher/%hi/%lo relocation
                                 : getAddrNonPICSym64(N, SDLoc(N), Ty, DAG);
   }
 
   // Every other architecture would use shouldAssumeDSOLocal in here, but
   // mips is special.
   // * In PIC code mips requires got loads even for local statics!
   // * To save on got entries, for local statics the got entry contains the
   //   page and an additional add instruction takes care of the low bits.
   // * It is legal to access a hidden symbol with a non hidden undefined,
   //   so one cannot guarantee that all access to a hidden symbol will know
   //   it is hidden.
   // * Mips linkers don't support creating a page and a full got entry for
   //   the same symbol.
   // * Given all that, we have to use a full got entry for hidden symbols :-(
   if (GV->hasLocalLinkage())
     return getAddrLocal(N, SDLoc(N), Ty, DAG, ABI.IsN32() || ABI.IsN64());
 
   if (Subtarget.useXGOT())
     return getAddrGlobalLargeGOT(
         N, SDLoc(N), Ty, DAG, MipsII::MO_GOT_HI16, MipsII::MO_GOT_LO16,
         DAG.getEntryNode(),
         MachinePointerInfo::getGOT(DAG.getMachineFunction()));
 
   return getAddrGlobal(
       N, SDLoc(N), Ty, DAG,
       (ABI.IsN32() || ABI.IsN64()) ? MipsII::MO_GOT_DISP : MipsII::MO_GOT,
       DAG.getEntryNode(), MachinePointerInfo::getGOT(DAG.getMachineFunction()));
 }
 
 SDValue MipsTargetLowering::lowerBlockAddress(SDValue Op,
                                               SelectionDAG &DAG) const {
   BlockAddressSDNode *N = cast<BlockAddressSDNode>(Op);
   EVT Ty = Op.getValueType();
 
   if (!isPositionIndependent())
     return Subtarget.hasSym32() ? getAddrNonPIC(N, SDLoc(N), Ty, DAG)
                                 : getAddrNonPICSym64(N, SDLoc(N), Ty, DAG);
 
   return getAddrLocal(N, SDLoc(N), Ty, DAG, ABI.IsN32() || ABI.IsN64());
 }
 
 SDValue MipsTargetLowering::
 lowerGlobalTLSAddress(SDValue Op, SelectionDAG &DAG) const
 {
   // If the relocation model is PIC, use the General Dynamic TLS Model or
   // Local Dynamic TLS model, otherwise use the Initial Exec or
   // Local Exec TLS Model.
 
   GlobalAddressSDNode *GA = cast<GlobalAddressSDNode>(Op);
   if (DAG.getTarget().useEmulatedTLS())
     return LowerToTLSEmulatedModel(GA, DAG);
 
   SDLoc DL(GA);
   const GlobalValue *GV = GA->getGlobal();
   EVT PtrVT = getPointerTy(DAG.getDataLayout());
 
   TLSModel::Model model = getTargetMachine().getTLSModel(GV);
 
   if (model == TLSModel::GeneralDynamic || model == TLSModel::LocalDynamic) {
     // General Dynamic and Local Dynamic TLS Model.
     unsigned Flag = (model == TLSModel::LocalDynamic) ? MipsII::MO_TLSLDM
                                                       : MipsII::MO_TLSGD;
 
     SDValue TGA = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0, Flag);
     SDValue Argument = DAG.getNode(MipsISD::Wrapper, DL, PtrVT,
                                    getGlobalReg(DAG, PtrVT), TGA);
     unsigned PtrSize = PtrVT.getSizeInBits();
     IntegerType *PtrTy = Type::getIntNTy(*DAG.getContext(), PtrSize);
 
     SDValue TlsGetAddr = DAG.getExternalSymbol("__tls_get_addr", PtrVT);
 
     ArgListTy Args;
     ArgListEntry Entry;
     Entry.Node = Argument;
     Entry.Ty = PtrTy;
     Args.push_back(Entry);
 
     TargetLowering::CallLoweringInfo CLI(DAG);
     CLI.setDebugLoc(DL)
         .setChain(DAG.getEntryNode())
         .setLibCallee(CallingConv::C, PtrTy, TlsGetAddr, std::move(Args));
     std::pair<SDValue, SDValue> CallResult = LowerCallTo(CLI);
 
     SDValue Ret = CallResult.first;
 
     if (model != TLSModel::LocalDynamic)
       return Ret;
 
     SDValue TGAHi = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0,
                                                MipsII::MO_DTPREL_HI);
     SDValue Hi = DAG.getNode(MipsISD::TlsHi, DL, PtrVT, TGAHi);
     SDValue TGALo = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0,
                                                MipsII::MO_DTPREL_LO);
     SDValue Lo = DAG.getNode(MipsISD::Lo, DL, PtrVT, TGALo);
     SDValue Add = DAG.getNode(ISD::ADD, DL, PtrVT, Hi, Ret);
     return DAG.getNode(ISD::ADD, DL, PtrVT, Add, Lo);
   }
 
   SDValue Offset;
   if (model == TLSModel::InitialExec) {
     // Initial Exec TLS Model
     SDValue TGA = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0,
                                              MipsII::MO_GOTTPREL);
     TGA = DAG.getNode(MipsISD::Wrapper, DL, PtrVT, getGlobalReg(DAG, PtrVT),
                       TGA);
     Offset =
         DAG.getLoad(PtrVT, DL, DAG.getEntryNode(), TGA, MachinePointerInfo());
   } else {
     // Local Exec TLS Model
     assert(model == TLSModel::LocalExec);
     SDValue TGAHi = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0,
                                                MipsII::MO_TPREL_HI);
     SDValue TGALo = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0,
                                                MipsII::MO_TPREL_LO);
     SDValue Hi = DAG.getNode(MipsISD::TlsHi, DL, PtrVT, TGAHi);
     SDValue Lo = DAG.getNode(MipsISD::Lo, DL, PtrVT, TGALo);
     Offset = DAG.getNode(ISD::ADD, DL, PtrVT, Hi, Lo);
   }
 
   SDValue ThreadPointer = DAG.getNode(MipsISD::ThreadPointer, DL, PtrVT);
   return DAG.getNode(ISD::ADD, DL, PtrVT, ThreadPointer, Offset);
 }
 
 SDValue MipsTargetLowering::
 lowerJumpTable(SDValue Op, SelectionDAG &DAG) const
 {
   JumpTableSDNode *N = cast<JumpTableSDNode>(Op);
   EVT Ty = Op.getValueType();
 
   if (!isPositionIndependent())
     return Subtarget.hasSym32() ? getAddrNonPIC(N, SDLoc(N), Ty, DAG)
                                 : getAddrNonPICSym64(N, SDLoc(N), Ty, DAG);
 
   return getAddrLocal(N, SDLoc(N), Ty, DAG, ABI.IsN32() || ABI.IsN64());
 }
 
 SDValue MipsTargetLowering::
 lowerConstantPool(SDValue Op, SelectionDAG &DAG) const
 {
   ConstantPoolSDNode *N = cast<ConstantPoolSDNode>(Op);
   EVT Ty = Op.getValueType();
 
   if (!isPositionIndependent()) {
     const MipsTargetObjectFile *TLOF =
         static_cast<const MipsTargetObjectFile *>(
             getTargetMachine().getObjFileLowering());
 
     if (TLOF->IsConstantInSmallSection(DAG.getDataLayout(), N->getConstVal(),
                                        getTargetMachine()))
       // %gp_rel relocation
       return getAddrGPRel(N, SDLoc(N), Ty, DAG, ABI.IsN64());
 
     return Subtarget.hasSym32() ? getAddrNonPIC(N, SDLoc(N), Ty, DAG)
                                 : getAddrNonPICSym64(N, SDLoc(N), Ty, DAG);
   }
 
  return getAddrLocal(N, SDLoc(N), Ty, DAG, ABI.IsN32() || ABI.IsN64());
 }
 
 SDValue MipsTargetLowering::lowerVASTART(SDValue Op, SelectionDAG &DAG) const {
   MachineFunction &MF = DAG.getMachineFunction();
   MipsFunctionInfo *FuncInfo = MF.getInfo<MipsFunctionInfo>();
 
   SDLoc DL(Op);
   SDValue FI = DAG.getFrameIndex(FuncInfo->getVarArgsFrameIndex(),
                                  getPointerTy(MF.getDataLayout()));
 
   // vastart just stores the address of the VarArgsFrameIndex slot into the
   // memory location argument.
   const Value *SV = cast<SrcValueSDNode>(Op.getOperand(2))->getValue();
   return DAG.getStore(Op.getOperand(0), DL, FI, Op.getOperand(1),
                       MachinePointerInfo(SV));
 }
 
 SDValue MipsTargetLowering::lowerVAARG(SDValue Op, SelectionDAG &DAG) const {
   SDNode *Node = Op.getNode();
   EVT VT = Node->getValueType(0);
   SDValue Chain = Node->getOperand(0);
   SDValue VAListPtr = Node->getOperand(1);
   const Align Align =
       llvm::MaybeAlign(Node->getConstantOperandVal(3)).valueOrOne();
   const Value *SV = cast<SrcValueSDNode>(Node->getOperand(2))->getValue();
   SDLoc DL(Node);
   unsigned ArgSlotSizeInBytes = (ABI.IsN32() || ABI.IsN64()) ? 8 : 4;
 
   SDValue VAListLoad = DAG.getLoad(getPointerTy(DAG.getDataLayout()), DL, Chain,
                                    VAListPtr, MachinePointerInfo(SV));
   SDValue VAList = VAListLoad;
 
   // Re-align the pointer if necessary.
   // It should only ever be necessary for 64-bit types on O32 since the minimum
   // argument alignment is the same as the maximum type alignment for N32/N64.
   //
   // FIXME: We currently align too often. The code generator doesn't notice
   //        when the pointer is still aligned from the last va_arg (or pair of
   //        va_args for the i64 on O32 case).
   if (Align > getMinStackArgumentAlignment()) {
     VAList = DAG.getNode(
         ISD::ADD, DL, VAList.getValueType(), VAList,
         DAG.getConstant(Align.value() - 1, DL, VAList.getValueType()));
 
     VAList = DAG.getNode(
         ISD::AND, DL, VAList.getValueType(), VAList,
         DAG.getConstant(-(int64_t)Align.value(), DL, VAList.getValueType()));
   }
 
   // Increment the pointer, VAList, to the next vaarg.
   auto &TD = DAG.getDataLayout();
   unsigned ArgSizeInBytes =
       TD.getTypeAllocSize(VT.getTypeForEVT(*DAG.getContext()));
   SDValue Tmp3 =
       DAG.getNode(ISD::ADD, DL, VAList.getValueType(), VAList,
                   DAG.getConstant(alignTo(ArgSizeInBytes, ArgSlotSizeInBytes),
                                   DL, VAList.getValueType()));
   // Store the incremented VAList to the legalized pointer
   Chain = DAG.getStore(VAListLoad.getValue(1), DL, Tmp3, VAListPtr,
                        MachinePointerInfo(SV));
 
   // In big-endian mode we must adjust the pointer when the load size is smaller
   // than the argument slot size. We must also reduce the known alignment to
   // match. For example in the N64 ABI, we must add 4 bytes to the offset to get
   // the correct half of the slot, and reduce the alignment from 8 (slot
   // alignment) down to 4 (type alignment).
   if (!Subtarget.isLittle() && ArgSizeInBytes < ArgSlotSizeInBytes) {
     unsigned Adjustment = ArgSlotSizeInBytes - ArgSizeInBytes;
     VAList = DAG.getNode(ISD::ADD, DL, VAListPtr.getValueType(), VAList,
                          DAG.getIntPtrConstant(Adjustment, DL));
   }
   // Load the actual argument out of the pointer VAList
   return DAG.getLoad(VT, DL, Chain, VAList, MachinePointerInfo());
 }
 
 static SDValue lowerFCOPYSIGN32(SDValue Op, SelectionDAG &DAG,
                                 bool HasExtractInsert) {
   EVT TyX = Op.getOperand(0).getValueType();
   EVT TyY = Op.getOperand(1).getValueType();
   SDLoc DL(Op);
   SDValue Const1 = DAG.getConstant(1, DL, MVT::i32);
   SDValue Const31 = DAG.getConstant(31, DL, MVT::i32);
   SDValue Res;
 
   // If operand is of type f64, extract the upper 32-bit. Otherwise, bitcast it
   // to i32.
   SDValue X = (TyX == MVT::f32) ?
     DAG.getNode(ISD::BITCAST, DL, MVT::i32, Op.getOperand(0)) :
     DAG.getNode(MipsISD::ExtractElementF64, DL, MVT::i32, Op.getOperand(0),
                 Const1);
   SDValue Y = (TyY == MVT::f32) ?
     DAG.getNode(ISD::BITCAST, DL, MVT::i32, Op.getOperand(1)) :
     DAG.getNode(MipsISD::ExtractElementF64, DL, MVT::i32, Op.getOperand(1),
                 Const1);
 
   if (HasExtractInsert) {
     // ext  E, Y, 31, 1  ; extract bit31 of Y
     // ins  X, E, 31, 1  ; insert extracted bit at bit31 of X
     SDValue E = DAG.getNode(MipsISD::Ext, DL, MVT::i32, Y, Const31, Const1);
     Res = DAG.getNode(MipsISD::Ins, DL, MVT::i32, E, Const31, Const1, X);
   } else {
     // sll SllX, X, 1
     // srl SrlX, SllX, 1
     // srl SrlY, Y, 31
     // sll SllY, SrlX, 31
     // or  Or, SrlX, SllY
     SDValue SllX = DAG.getNode(ISD::SHL, DL, MVT::i32, X, Const1);
     SDValue SrlX = DAG.getNode(ISD::SRL, DL, MVT::i32, SllX, Const1);
     SDValue SrlY = DAG.getNode(ISD::SRL, DL, MVT::i32, Y, Const31);
     SDValue SllY = DAG.getNode(ISD::SHL, DL, MVT::i32, SrlY, Const31);
     Res = DAG.getNode(ISD::OR, DL, MVT::i32, SrlX, SllY);
   }
 
   if (TyX == MVT::f32)
     return DAG.getNode(ISD::BITCAST, DL, Op.getOperand(0).getValueType(), Res);
 
   SDValue LowX = DAG.getNode(MipsISD::ExtractElementF64, DL, MVT::i32,
                              Op.getOperand(0),
                              DAG.getConstant(0, DL, MVT::i32));
   return DAG.getNode(MipsISD::BuildPairF64, DL, MVT::f64, LowX, Res);
 }
 
 static SDValue lowerFCOPYSIGN64(SDValue Op, SelectionDAG &DAG,
                                 bool HasExtractInsert) {
   unsigned WidthX = Op.getOperand(0).getValueSizeInBits();
   unsigned WidthY = Op.getOperand(1).getValueSizeInBits();
   EVT TyX = MVT::getIntegerVT(WidthX), TyY = MVT::getIntegerVT(WidthY);
   SDLoc DL(Op);
   SDValue Const1 = DAG.getConstant(1, DL, MVT::i32);
 
   // Bitcast to integer nodes.
   SDValue X = DAG.getNode(ISD::BITCAST, DL, TyX, Op.getOperand(0));
   SDValue Y = DAG.getNode(ISD::BITCAST, DL, TyY, Op.getOperand(1));
 
   if (HasExtractInsert) {
     // ext  E, Y, width(Y) - 1, 1  ; extract bit width(Y)-1 of Y
     // ins  X, E, width(X) - 1, 1  ; insert extracted bit at bit width(X)-1 of X
     SDValue E = DAG.getNode(MipsISD::Ext, DL, TyY, Y,
                             DAG.getConstant(WidthY - 1, DL, MVT::i32), Const1);
 
     if (WidthX > WidthY)
       E = DAG.getNode(ISD::ZERO_EXTEND, DL, TyX, E);
     else if (WidthY > WidthX)
       E = DAG.getNode(ISD::TRUNCATE, DL, TyX, E);
 
     SDValue I = DAG.getNode(MipsISD::Ins, DL, TyX, E,
                             DAG.getConstant(WidthX - 1, DL, MVT::i32), Const1,
                             X);
     return DAG.getNode(ISD::BITCAST, DL, Op.getOperand(0).getValueType(), I);
   }
 
   // (d)sll SllX, X, 1
   // (d)srl SrlX, SllX, 1
   // (d)srl SrlY, Y, width(Y)-1
   // (d)sll SllY, SrlX, width(Y)-1
   // or     Or, SrlX, SllY
   SDValue SllX = DAG.getNode(ISD::SHL, DL, TyX, X, Const1);
   SDValue SrlX = DAG.getNode(ISD::SRL, DL, TyX, SllX, Const1);
   SDValue SrlY = DAG.getNode(ISD::SRL, DL, TyY, Y,
                              DAG.getConstant(WidthY - 1, DL, MVT::i32));
 
   if (WidthX > WidthY)
     SrlY = DAG.getNode(ISD::ZERO_EXTEND, DL, TyX, SrlY);
   else if (WidthY > WidthX)
     SrlY = DAG.getNode(ISD::TRUNCATE, DL, TyX, SrlY);
 
   SDValue SllY = DAG.getNode(ISD::SHL, DL, TyX, SrlY,
                              DAG.getConstant(WidthX - 1, DL, MVT::i32));
   SDValue Or = DAG.getNode(ISD::OR, DL, TyX, SrlX, SllY);
   return DAG.getNode(ISD::BITCAST, DL, Op.getOperand(0).getValueType(), Or);
 }
 
 SDValue
 MipsTargetLowering::lowerFCOPYSIGN(SDValue Op, SelectionDAG &DAG) const {
   if (Subtarget.isGP64bit())
     return lowerFCOPYSIGN64(Op, DAG, Subtarget.hasExtractInsert());
 
   return lowerFCOPYSIGN32(Op, DAG, Subtarget.hasExtractInsert());
 }
 
 static SDValue lowerFABS32(SDValue Op, SelectionDAG &DAG,
                            bool HasExtractInsert) {
   SDLoc DL(Op);
   SDValue Res, Const1 = DAG.getConstant(1, DL, MVT::i32);
 
   // If operand is of type f64, extract the upper 32-bit. Otherwise, bitcast it
   // to i32.
   SDValue X = (Op.getValueType() == MVT::f32)
                   ? DAG.getNode(ISD::BITCAST, DL, MVT::i32, Op.getOperand(0))
                   : DAG.getNode(MipsISD::ExtractElementF64, DL, MVT::i32,
                                 Op.getOperand(0), Const1);
 
   // Clear MSB.
   if (HasExtractInsert)
     Res = DAG.getNode(MipsISD::Ins, DL, MVT::i32,
                       DAG.getRegister(Mips::ZERO, MVT::i32),
                       DAG.getConstant(31, DL, MVT::i32), Const1, X);
   else {
     // TODO: Provide DAG patterns which transform (and x, cst)
     // back to a (shl (srl x (clz cst)) (clz cst)) sequence.
     SDValue SllX = DAG.getNode(ISD::SHL, DL, MVT::i32, X, Const1);
     Res = DAG.getNode(ISD::SRL, DL, MVT::i32, SllX, Const1);
   }
 
   if (Op.getValueType() == MVT::f32)
     return DAG.getNode(ISD::BITCAST, DL, MVT::f32, Res);
 
   // FIXME: For mips32r2, the sequence of (BuildPairF64 (ins (ExtractElementF64
   // Op 1), $zero, 31 1) (ExtractElementF64 Op 0)) and the Op has one use, we
   // should be able to drop the usage of mfc1/mtc1 and rewrite the register in
   // place.
   SDValue LowX =
       DAG.getNode(MipsISD::ExtractElementF64, DL, MVT::i32, Op.getOperand(0),
                   DAG.getConstant(0, DL, MVT::i32));
   return DAG.getNode(MipsISD::BuildPairF64, DL, MVT::f64, LowX, Res);
 }
 
 static SDValue lowerFABS64(SDValue Op, SelectionDAG &DAG,
                            bool HasExtractInsert) {
   SDLoc DL(Op);
   SDValue Res, Const1 = DAG.getConstant(1, DL, MVT::i32);
 
   // Bitcast to integer node.
   SDValue X = DAG.getNode(ISD::BITCAST, DL, MVT::i64, Op.getOperand(0));
 
   // Clear MSB.
   if (HasExtractInsert)
     Res = DAG.getNode(MipsISD::Ins, DL, MVT::i64,
                       DAG.getRegister(Mips::ZERO_64, MVT::i64),
                       DAG.getConstant(63, DL, MVT::i32), Const1, X);
   else {
     SDValue SllX = DAG.getNode(ISD::SHL, DL, MVT::i64, X, Const1);
     Res = DAG.getNode(ISD::SRL, DL, MVT::i64, SllX, Const1);
   }
 
   return DAG.getNode(ISD::BITCAST, DL, MVT::f64, Res);
 }
 
 SDValue MipsTargetLowering::lowerFABS(SDValue Op, SelectionDAG &DAG) const {
   if ((ABI.IsN32() || ABI.IsN64()) && (Op.getValueType() == MVT::f64))
     return lowerFABS64(Op, DAG, Subtarget.hasExtractInsert());
 
   return lowerFABS32(Op, DAG, Subtarget.hasExtractInsert());
 }
 
 SDValue MipsTargetLowering::
 lowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const {
   // check the depth
   if (cast<ConstantSDNode>(Op.getOperand(0))->getZExtValue() != 0) {
     DAG.getContext()->emitError(
         "return address can be determined only for current frame");
     return SDValue();
   }
 
   MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();
   MFI.setFrameAddressIsTaken(true);
   EVT VT = Op.getValueType();
   SDLoc DL(Op);
   SDValue FrameAddr = DAG.getCopyFromReg(
       DAG.getEntryNode(), DL, ABI.IsN64() ? Mips::FP_64 : Mips::FP, VT);
   return FrameAddr;
 }
 
 SDValue MipsTargetLowering::lowerRETURNADDR(SDValue Op,
                                             SelectionDAG &DAG) const {
   if (verifyReturnAddressArgumentIsConstant(Op, DAG))
     return SDValue();
 
   // check the depth
   if (cast<ConstantSDNode>(Op.getOperand(0))->getZExtValue() != 0) {
     DAG.getContext()->emitError(
         "return address can be determined only for current frame");
     return SDValue();
   }
 
   MachineFunction &MF = DAG.getMachineFunction();
   MachineFrameInfo &MFI = MF.getFrameInfo();
   MVT VT = Op.getSimpleValueType();
   unsigned RA = ABI.IsN64() ? Mips::RA_64 : Mips::RA;
   MFI.setReturnAddressIsTaken(true);
 
   // Return RA, which contains the return address. Mark it an implicit live-in.
   unsigned Reg = MF.addLiveIn(RA, getRegClassFor(VT));
   return DAG.getCopyFromReg(DAG.getEntryNode(), SDLoc(Op), Reg, VT);
 }
 
 // An EH_RETURN is the result of lowering llvm.eh.return which in turn is
 // generated from __builtin_eh_return (offset, handler)
 // The effect of this is to adjust the stack pointer by "offset"
 // and then branch to "handler".
 SDValue MipsTargetLowering::lowerEH_RETURN(SDValue Op, SelectionDAG &DAG)
                                                                      const {
   MachineFunction &MF = DAG.getMachineFunction();
   MipsFunctionInfo *MipsFI = MF.getInfo<MipsFunctionInfo>();
 
   MipsFI->setCallsEhReturn();
   SDValue Chain     = Op.getOperand(0);
   SDValue Offset    = Op.getOperand(1);
   SDValue Handler   = Op.getOperand(2);
   SDLoc DL(Op);
   EVT Ty = ABI.IsN64() ? MVT::i64 : MVT::i32;
 
   // Store stack offset in V1, store jump target in V0. Glue CopyToReg and
   // EH_RETURN nodes, so that instructions are emitted back-to-back.
   unsigned OffsetReg = ABI.IsN64() ? Mips::V1_64 : Mips::V1;
   unsigned AddrReg = ABI.IsN64() ? Mips::V0_64 : Mips::V0;
   Chain = DAG.getCopyToReg(Chain, DL, OffsetReg, Offset, SDValue());
   Chain = DAG.getCopyToReg(Chain, DL, AddrReg, Handler, Chain.getValue(1));
   return DAG.getNode(MipsISD::EH_RETURN, DL, MVT::Other, Chain,
                      DAG.getRegister(OffsetReg, Ty),
                      DAG.getRegister(AddrReg, getPointerTy(MF.getDataLayout())),
                      Chain.getValue(1));
 }
 
 SDValue MipsTargetLowering::lowerATOMIC_FENCE(SDValue Op,
                                               SelectionDAG &DAG) const {
   // FIXME: Need pseudo-fence for 'singlethread' fences
   // FIXME: Set SType for weaker fences where supported/appropriate.
   unsigned SType = 0;
   SDLoc DL(Op);
   return DAG.getNode(MipsISD::Sync, DL, MVT::Other, Op.getOperand(0),
                      DAG.getConstant(SType, DL, MVT::i32));
 }
 
 SDValue MipsTargetLowering::lowerShiftLeftParts(SDValue Op,
                                                 SelectionDAG &DAG) const {
   SDLoc DL(Op);
   MVT VT = Subtarget.isGP64bit() ? MVT::i64 : MVT::i32;
 
   SDValue Lo = Op.getOperand(0), Hi = Op.getOperand(1);
   SDValue Shamt = Op.getOperand(2);
   // if shamt < (VT.bits):
   //  lo = (shl lo, shamt)
   //  hi = (or (shl hi, shamt) (srl (srl lo, 1), ~shamt))
   // else:
   //  lo = 0
   //  hi = (shl lo, shamt[4:0])
   SDValue Not = DAG.getNode(ISD::XOR, DL, MVT::i32, Shamt,
                             DAG.getConstant(-1, DL, MVT::i32));
   SDValue ShiftRight1Lo = DAG.getNode(ISD::SRL, DL, VT, Lo,
                                       DAG.getConstant(1, DL, VT));
   SDValue ShiftRightLo = DAG.getNode(ISD::SRL, DL, VT, ShiftRight1Lo, Not);
   SDValue ShiftLeftHi = DAG.getNode(ISD::SHL, DL, VT, Hi, Shamt);
   SDValue Or = DAG.getNode(ISD::OR, DL, VT, ShiftLeftHi, ShiftRightLo);
   SDValue ShiftLeftLo = DAG.getNode(ISD::SHL, DL, VT, Lo, Shamt);
   SDValue Cond = DAG.getNode(ISD::AND, DL, MVT::i32, Shamt,
                              DAG.getConstant(VT.getSizeInBits(), DL, MVT::i32));
   Lo = DAG.getNode(ISD::SELECT, DL, VT, Cond,
                    DAG.getConstant(0, DL, VT), ShiftLeftLo);
   Hi = DAG.getNode(ISD::SELECT, DL, VT, Cond, ShiftLeftLo, Or);
 
   SDValue Ops[2] = {Lo, Hi};
   return DAG.getMergeValues(Ops, DL);
 }
 
 SDValue MipsTargetLowering::lowerShiftRightParts(SDValue Op, SelectionDAG &DAG,
                                                  bool IsSRA) const {
   SDLoc DL(Op);
   SDValue Lo = Op.getOperand(0), Hi = Op.getOperand(1);
   SDValue Shamt = Op.getOperand(2);
   MVT VT = Subtarget.isGP64bit() ? MVT::i64 : MVT::i32;
 
   // if shamt < (VT.bits):
   //  lo = (or (shl (shl hi, 1), ~shamt) (srl lo, shamt))
   //  if isSRA:
   //    hi = (sra hi, shamt)
   //  else:
   //    hi = (srl hi, shamt)
   // else:
   //  if isSRA:
   //   lo = (sra hi, shamt[4:0])
   //   hi = (sra hi, 31)
   //  else:
   //   lo = (srl hi, shamt[4:0])
   //   hi = 0
   SDValue Not = DAG.getNode(ISD::XOR, DL, MVT::i32, Shamt,
                             DAG.getConstant(-1, DL, MVT::i32));
   SDValue ShiftLeft1Hi = DAG.getNode(ISD::SHL, DL, VT, Hi,
                                      DAG.getConstant(1, DL, VT));
   SDValue ShiftLeftHi = DAG.getNode(ISD::SHL, DL, VT, ShiftLeft1Hi, Not);
   SDValue ShiftRightLo = DAG.getNode(ISD::SRL, DL, VT, Lo, Shamt);
   SDValue Or = DAG.getNode(ISD::OR, DL, VT, ShiftLeftHi, ShiftRightLo);
   SDValue ShiftRightHi = DAG.getNode(IsSRA ? ISD::SRA : ISD::SRL,
                                      DL, VT, Hi, Shamt);
   SDValue Cond = DAG.getNode(ISD::AND, DL, MVT::i32, Shamt,
                              DAG.getConstant(VT.getSizeInBits(), DL, MVT::i32));
   SDValue Ext = DAG.getNode(ISD::SRA, DL, VT, Hi,
                             DAG.getConstant(VT.getSizeInBits() - 1, DL, VT));
 
   if (!(Subtarget.hasMips4() || Subtarget.hasMips32())) {
     SDVTList VTList = DAG.getVTList(VT, VT);
     return DAG.getNode(Subtarget.isGP64bit() ? Mips::PseudoD_SELECT_I64
                                              : Mips::PseudoD_SELECT_I,
                        DL, VTList, Cond, ShiftRightHi,
                        IsSRA ? Ext : DAG.getConstant(0, DL, VT), Or,
                        ShiftRightHi);
   }
 
   Lo = DAG.getNode(ISD::SELECT, DL, VT, Cond, ShiftRightHi, Or);
   Hi = DAG.getNode(ISD::SELECT, DL, VT, Cond,
                    IsSRA ? Ext : DAG.getConstant(0, DL, VT), ShiftRightHi);
 
   SDValue Ops[2] = {Lo, Hi};
   return DAG.getMergeValues(Ops, DL);
 }
 
 static SDValue createLoadLR(unsigned Opc, SelectionDAG &DAG, LoadSDNode *LD,
                             SDValue Chain, SDValue Src, unsigned Offset) {
   SDValue Ptr = LD->getBasePtr();
   EVT VT = LD->getValueType(0), MemVT = LD->getMemoryVT();
   EVT BasePtrVT = Ptr.getValueType();
   SDLoc DL(LD);
   SDVTList VTList = DAG.getVTList(VT, MVT::Other);
 
   if (Offset)
     Ptr = DAG.getNode(ISD::ADD, DL, BasePtrVT, Ptr,
                       DAG.getConstant(Offset, DL, BasePtrVT));
 
   SDValue Ops[] = { Chain, Ptr, Src };
   return DAG.getMemIntrinsicNode(Opc, DL, VTList, Ops, MemVT,
                                  LD->getMemOperand());
 }
 
 // Expand an unaligned 32 or 64-bit integer load node.
 SDValue MipsTargetLowering::lowerLOAD(SDValue Op, SelectionDAG &DAG) const {
   LoadSDNode *LD = cast<LoadSDNode>(Op);
   EVT MemVT = LD->getMemoryVT();
 
   if (Subtarget.systemSupportsUnalignedAccess())
     return Op;
 
   // Return if load is aligned or if MemVT is neither i32 nor i64.
   if ((LD->getAlignment() >= MemVT.getSizeInBits() / 8) ||
       ((MemVT != MVT::i32) && (MemVT != MVT::i64)))
     return SDValue();
 
   bool IsLittle = Subtarget.isLittle();
   EVT VT = Op.getValueType();
   ISD::LoadExtType ExtType = LD->getExtensionType();
   SDValue Chain = LD->getChain(), Undef = DAG.getUNDEF(VT);
 
   assert((VT == MVT::i32) || (VT == MVT::i64));
 
   // Expand
   //  (set dst, (i64 (load baseptr)))
   // to
   //  (set tmp, (ldl (add baseptr, 7), undef))
   //  (set dst, (ldr baseptr, tmp))
   if ((VT == MVT::i64) && (ExtType == ISD::NON_EXTLOAD)) {
     SDValue LDL = createLoadLR(MipsISD::LDL, DAG, LD, Chain, Undef,
                                IsLittle ? 7 : 0);
     return createLoadLR(MipsISD::LDR, DAG, LD, LDL.getValue(1), LDL,
                         IsLittle ? 0 : 7);
   }
 
   SDValue LWL = createLoadLR(MipsISD::LWL, DAG, LD, Chain, Undef,
                              IsLittle ? 3 : 0);
   SDValue LWR = createLoadLR(MipsISD::LWR, DAG, LD, LWL.getValue(1), LWL,
                              IsLittle ? 0 : 3);
 
   // Expand
   //  (set dst, (i32 (load baseptr))) or
   //  (set dst, (i64 (sextload baseptr))) or
   //  (set dst, (i64 (extload baseptr)))
   // to
   //  (set tmp, (lwl (add baseptr, 3), undef))
   //  (set dst, (lwr baseptr, tmp))
   if ((VT == MVT::i32) || (ExtType == ISD::SEXTLOAD) ||
       (ExtType == ISD::EXTLOAD))
     return LWR;
 
   assert((VT == MVT::i64) && (ExtType == ISD::ZEXTLOAD));
 
   // Expand
   //  (set dst, (i64 (zextload baseptr)))
   // to
   //  (set tmp0, (lwl (add baseptr, 3), undef))
   //  (set tmp1, (lwr baseptr, tmp0))
   //  (set tmp2, (shl tmp1, 32))
   //  (set dst, (srl tmp2, 32))
   SDLoc DL(LD);
   SDValue Const32 = DAG.getConstant(32, DL, MVT::i32);
   SDValue SLL = DAG.getNode(ISD::SHL, DL, MVT::i64, LWR, Const32);
   SDValue SRL = DAG.getNode(ISD::SRL, DL, MVT::i64, SLL, Const32);
   SDValue Ops[] = { SRL, LWR.getValue(1) };
   return DAG.getMergeValues(Ops, DL);
 }
 
 static SDValue createStoreLR(unsigned Opc, SelectionDAG &DAG, StoreSDNode *SD,
                              SDValue Chain, unsigned Offset) {
   SDValue Ptr = SD->getBasePtr(), Value = SD->getValue();
   EVT MemVT = SD->getMemoryVT(), BasePtrVT = Ptr.getValueType();
   SDLoc DL(SD);
   SDVTList VTList = DAG.getVTList(MVT::Other);
 
   if (Offset)
     Ptr = DAG.getNode(ISD::ADD, DL, BasePtrVT, Ptr,
                       DAG.getConstant(Offset, DL, BasePtrVT));
 
   SDValue Ops[] = { Chain, Value, Ptr };
   return DAG.getMemIntrinsicNode(Opc, DL, VTList, Ops, MemVT,
                                  SD->getMemOperand());
 }
 
 // Expand an unaligned 32 or 64-bit integer store node.
 static SDValue lowerUnalignedIntStore(StoreSDNode *SD, SelectionDAG &DAG,
                                       bool IsLittle) {
   SDValue Value = SD->getValue(), Chain = SD->getChain();
   EVT VT = Value.getValueType();
 
   // Expand
   //  (store val, baseptr) or
   //  (truncstore val, baseptr)
   // to
   //  (swl val, (add baseptr, 3))
   //  (swr val, baseptr)
   if ((VT == MVT::i32) || SD->isTruncatingStore()) {
     SDValue SWL = createStoreLR(MipsISD::SWL, DAG, SD, Chain,
                                 IsLittle ? 3 : 0);
     return createStoreLR(MipsISD::SWR, DAG, SD, SWL, IsLittle ? 0 : 3);
   }
 
   assert(VT == MVT::i64);
 
   // Expand
   //  (store val, baseptr)
   // to
   //  (sdl val, (add baseptr, 7))
   //  (sdr val, baseptr)
   SDValue SDL = createStoreLR(MipsISD::SDL, DAG, SD, Chain, IsLittle ? 7 : 0);
   return createStoreLR(MipsISD::SDR, DAG, SD, SDL, IsLittle ? 0 : 7);
 }
 
 // Lower (store (fp_to_sint $fp) $ptr) to (store (TruncIntFP $fp), $ptr).
 static SDValue lowerFP_TO_SINT_STORE(StoreSDNode *SD, SelectionDAG &DAG,
                                      bool SingleFloat) {
   SDValue Val = SD->getValue();
 
   if (Val.getOpcode() != ISD::FP_TO_SINT ||
       (Val.getValueSizeInBits() > 32 && SingleFloat))
     return SDValue();
 
   EVT FPTy = EVT::getFloatingPointVT(Val.getValueSizeInBits());
   SDValue Tr = DAG.getNode(MipsISD::TruncIntFP, SDLoc(Val), FPTy,
                            Val.getOperand(0));
   return DAG.getStore(SD->getChain(), SDLoc(SD), Tr, SD->getBasePtr(),
                       SD->getPointerInfo(), SD->getAlignment(),
                       SD->getMemOperand()->getFlags());
 }
 
 SDValue MipsTargetLowering::lowerSTORE(SDValue Op, SelectionDAG &DAG) const {
   StoreSDNode *SD = cast<StoreSDNode>(Op);
   EVT MemVT = SD->getMemoryVT();
 
   // Lower unaligned integer stores.
   if (!Subtarget.systemSupportsUnalignedAccess() &&
       (SD->getAlignment() < MemVT.getSizeInBits() / 8) &&
       ((MemVT == MVT::i32) || (MemVT == MVT::i64)))
     return lowerUnalignedIntStore(SD, DAG, Subtarget.isLittle());
 
   return lowerFP_TO_SINT_STORE(SD, DAG, Subtarget.isSingleFloat());
 }
 
 SDValue MipsTargetLowering::lowerEH_DWARF_CFA(SDValue Op,
                                               SelectionDAG &DAG) const {
 
   // Return a fixed StackObject with offset 0 which points to the old stack
   // pointer.
   MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();
   EVT ValTy = Op->getValueType(0);
   int FI = MFI.CreateFixedObject(Op.getValueSizeInBits() / 8, 0, false);
   return DAG.getFrameIndex(FI, ValTy);
 }
 
 SDValue MipsTargetLowering::lowerFP_TO_SINT(SDValue Op,
                                             SelectionDAG &DAG) const {
   if (Op.getValueSizeInBits() > 32 && Subtarget.isSingleFloat())
     return SDValue();
 
   EVT FPTy = EVT::getFloatingPointVT(Op.getValueSizeInBits());
   SDValue Trunc = DAG.getNode(MipsISD::TruncIntFP, SDLoc(Op), FPTy,
                               Op.getOperand(0));
   return DAG.getNode(ISD::BITCAST, SDLoc(Op), Op.getValueType(), Trunc);
 }
 
 //===----------------------------------------------------------------------===//
 //                      Calling Convention Implementation
 //===----------------------------------------------------------------------===//
 
 //===----------------------------------------------------------------------===//
 // TODO: Implement a generic logic using tblgen that can support this.
 // Mips O32 ABI rules:
 // ---
 // i32 - Passed in A0, A1, A2, A3 and stack
 // f32 - Only passed in f32 registers if no int reg has been used yet to hold
 //       an argument. Otherwise, passed in A1, A2, A3 and stack.
 // f64 - Only passed in two aliased f32 registers if no int reg has been used
 //       yet to hold an argument. Otherwise, use A2, A3 and stack. If A1 is
 //       not used, it must be shadowed. If only A3 is available, shadow it and
 //       go to stack.
 // vXiX - Received as scalarized i32s, passed in A0 - A3 and the stack.
 // vXf32 - Passed in either a pair of registers {A0, A1}, {A2, A3} or {A0 - A3}
 //         with the remainder spilled to the stack.
 // vXf64 - Passed in either {A0, A1, A2, A3} or {A2, A3} and in both cases
 //         spilling the remainder to the stack.
 //
 //  For vararg functions, all arguments are passed in A0, A1, A2, A3 and stack.
 //===----------------------------------------------------------------------===//
 
 static bool CC_MipsO32(unsigned ValNo, MVT ValVT, MVT LocVT,
                        CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
                        CCState &State, ArrayRef<MCPhysReg> F64Regs) {
   const MipsSubtarget &Subtarget = static_cast<const MipsSubtarget &>(
       State.getMachineFunction().getSubtarget());
 
   static const MCPhysReg IntRegs[] = { Mips::A0, Mips::A1, Mips::A2, Mips::A3 };
 
   const MipsCCState * MipsState = static_cast<MipsCCState *>(&State);
 
   static const MCPhysReg F32Regs[] = { Mips::F12, Mips::F14 };
 
   static const MCPhysReg FloatVectorIntRegs[] = { Mips::A0, Mips::A2 };
 
   // Do not process byval args here.
   if (ArgFlags.isByVal())
     return true;
 
   // Promote i8 and i16
   if (ArgFlags.isInReg() && !Subtarget.isLittle()) {
     if (LocVT == MVT::i8 || LocVT == MVT::i16 || LocVT == MVT::i32) {
       LocVT = MVT::i32;
       if (ArgFlags.isSExt())
         LocInfo = CCValAssign::SExtUpper;
       else if (ArgFlags.isZExt())
         LocInfo = CCValAssign::ZExtUpper;
       else
         LocInfo = CCValAssign::AExtUpper;
     }
   }
 
   // Promote i8 and i16
   if (LocVT == MVT::i8 || LocVT == MVT::i16) {
     LocVT = MVT::i32;
     if (ArgFlags.isSExt())
       LocInfo = CCValAssign::SExt;
     else if (ArgFlags.isZExt())
       LocInfo = CCValAssign::ZExt;
     else
       LocInfo = CCValAssign::AExt;
   }
 
   unsigned Reg;
 
   // f32 and f64 are allocated in A0, A1, A2, A3 when either of the following
   // is true: function is vararg, argument is 3rd or higher, there is previous
   // argument which is not f32 or f64.
   bool AllocateFloatsInIntReg = State.isVarArg() || ValNo > 1 ||
                                 State.getFirstUnallocated(F32Regs) != ValNo;
   Align OrigAlign = ArgFlags.getNonZeroOrigAlign();
   bool isI64 = (ValVT == MVT::i32 && OrigAlign == Align(8));
   bool isVectorFloat = MipsState->WasOriginalArgVectorFloat(ValNo);
 
   // The MIPS vector ABI for floats passes them in a pair of registers
   if (ValVT == MVT::i32 && isVectorFloat) {
     // This is the start of an vector that was scalarized into an unknown number
     // of components. It doesn't matter how many there are. Allocate one of the
     // notional 8 byte aligned registers which map onto the argument stack, and
     // shadow the register lost to alignment requirements.
     if (ArgFlags.isSplit()) {
       Reg = State.AllocateReg(FloatVectorIntRegs);
       if (Reg == Mips::A2)
         State.AllocateReg(Mips::A1);
       else if (Reg == 0)
         State.AllocateReg(Mips::A3);
     } else {
       // If we're an intermediate component of the split, we can just attempt to
       // allocate a register directly.
       Reg = State.AllocateReg(IntRegs);
     }
   } else if (ValVT == MVT::i32 ||
              (ValVT == MVT::f32 && AllocateFloatsInIntReg)) {
     Reg = State.AllocateReg(IntRegs);
     // If this is the first part of an i64 arg,
     // the allocated register must be either A0 or A2.
     if (isI64 && (Reg == Mips::A1 || Reg == Mips::A3))
       Reg = State.AllocateReg(IntRegs);
     LocVT = MVT::i32;
   } else if (ValVT == MVT::f64 && AllocateFloatsInIntReg) {
     // Allocate int register and shadow next int register. If first
     // available register is Mips::A1 or Mips::A3, shadow it too.
     Reg = State.AllocateReg(IntRegs);
     if (Reg == Mips::A1 || Reg == Mips::A3)
       Reg = State.AllocateReg(IntRegs);
     State.AllocateReg(IntRegs);
     LocVT = MVT::i32;
   } else if (ValVT.isFloatingPoint() && !AllocateFloatsInIntReg) {
     // we are guaranteed to find an available float register
     if (ValVT == MVT::f32) {
       Reg = State.AllocateReg(F32Regs);
       // Shadow int register
       State.AllocateReg(IntRegs);
     } else {
       Reg = State.AllocateReg(F64Regs);
       // Shadow int registers
       unsigned Reg2 = State.AllocateReg(IntRegs);
       if (Reg2 == Mips::A1 || Reg2 == Mips::A3)
         State.AllocateReg(IntRegs);
       State.AllocateReg(IntRegs);
     }
   } else
     llvm_unreachable("Cannot handle this ValVT.");
 
   if (!Reg) {
     unsigned Offset = State.AllocateStack(ValVT.getStoreSize(), OrigAlign);
     State.addLoc(CCValAssign::getMem(ValNo, ValVT, Offset, LocVT, LocInfo));
   } else
     State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));
 
   return false;
 }
 
 static bool CC_MipsO32_FP32(unsigned ValNo, MVT ValVT,
                             MVT LocVT, CCValAssign::LocInfo LocInfo,
                             ISD::ArgFlagsTy ArgFlags, CCState &State) {
   static const MCPhysReg F64Regs[] = { Mips::D6, Mips::D7 };
 
   return CC_MipsO32(ValNo, ValVT, LocVT, LocInfo, ArgFlags, State, F64Regs);
 }
 
 static bool CC_MipsO32_FP64(unsigned ValNo, MVT ValVT,
                             MVT LocVT, CCValAssign::LocInfo LocInfo,
                             ISD::ArgFlagsTy ArgFlags, CCState &State) {
   static const MCPhysReg F64Regs[] = { Mips::D12_64, Mips::D14_64 };
 
   return CC_MipsO32(ValNo, ValVT, LocVT, LocInfo, ArgFlags, State, F64Regs);
 }
 
 static bool CC_MipsO32(unsigned ValNo, MVT ValVT, MVT LocVT,
                        CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
                        CCState &State) LLVM_ATTRIBUTE_UNUSED;
 
 #include "MipsGenCallingConv.inc"
 
  CCAssignFn *MipsTargetLowering::CCAssignFnForCall() const{
    return CC_Mips_FixedArg;
  }
 
  CCAssignFn *MipsTargetLowering::CCAssignFnForReturn() const{
    return RetCC_Mips;
  }
 //===----------------------------------------------------------------------===//
 //                  Call Calling Convention Implementation
 //===----------------------------------------------------------------------===//
 
 // Return next O32 integer argument register.
 static unsigned getNextIntArgReg(unsigned Reg) {
   assert((Reg == Mips::A0) || (Reg == Mips::A2));
   return (Reg == Mips::A0) ? Mips::A1 : Mips::A3;
 }
 
 SDValue MipsTargetLowering::passArgOnStack(SDValue StackPtr, unsigned Offset,
                                            SDValue Chain, SDValue Arg,
                                            const SDLoc &DL, bool IsTailCall,
                                            SelectionDAG &DAG) const {
   if (!IsTailCall) {
     SDValue PtrOff =
         DAG.getNode(ISD::ADD, DL, getPointerTy(DAG.getDataLayout()), StackPtr,
                     DAG.getIntPtrConstant(Offset, DL));
     return DAG.getStore(Chain, DL, Arg, PtrOff, MachinePointerInfo());
   }
 
   MachineFrameInfo &MFI = DAG.getMachineFunction().getFrameInfo();
   int FI = MFI.CreateFixedObject(Arg.getValueSizeInBits() / 8, Offset, false);
   SDValue FIN = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));
   return DAG.getStore(Chain, DL, Arg, FIN, MachinePointerInfo(), MaybeAlign(),
                       MachineMemOperand::MOVolatile);
 }
 
 void MipsTargetLowering::
 getOpndList(SmallVectorImpl<SDValue> &Ops,
             std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
             bool IsPICCall, bool GlobalOrExternal, bool InternalLinkage,
             bool IsCallReloc, CallLoweringInfo &CLI, SDValue Callee,
             SDValue Chain) const {
   // Insert node "GP copy globalreg" before call to function.
   //
   // R_MIPS_CALL* operators (emitted when non-internal functions are called
   // in PIC mode) allow symbols to be resolved via lazy binding.
   // The lazy binding stub requires GP to point to the GOT.
   // Note that we don't need GP to point to the GOT for indirect calls
   // (when R_MIPS_CALL* is not used for the call) because Mips linker generates
   // lazy binding stub for a function only when R_MIPS_CALL* are the only relocs
   // used for the function (that is, Mips linker doesn't generate lazy binding
   // stub for a function whose address is taken in the program).
   if (IsPICCall && !InternalLinkage && IsCallReloc) {
     unsigned GPReg = ABI.IsN64() ? Mips::GP_64 : Mips::GP;
     EVT Ty = ABI.IsN64() ? MVT::i64 : MVT::i32;
     RegsToPass.push_back(std::make_pair(GPReg, getGlobalReg(CLI.DAG, Ty)));
   }
 
   // Build a sequence of copy-to-reg nodes chained together with token
   // chain and flag operands which copy the outgoing args into registers.
   // The InFlag in necessary since all emitted instructions must be
   // stuck together.
   SDValue InFlag;
 
   for (unsigned i = 0, e = RegsToPass.size(); i != e; ++i) {
     Chain = CLI.DAG.getCopyToReg(Chain, CLI.DL, RegsToPass[i].first,
                                  RegsToPass[i].second, InFlag);
     InFlag = Chain.getValue(1);
   }
 
   // Add argument registers to the end of the list so that they are
   // known live into the call.
   for (unsigned i = 0, e = RegsToPass.size(); i != e; ++i)
     Ops.push_back(CLI.DAG.getRegister(RegsToPass[i].first,
                                       RegsToPass[i].second.getValueType()));
 
   // Add a register mask operand representing the call-preserved registers.
   const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo();
   const uint32_t *Mask =
       TRI->getCallPreservedMask(CLI.DAG.getMachineFunction(), CLI.CallConv);
   assert(Mask && "Missing call preserved mask for calling convention");
   if (Subtarget.inMips16HardFloat()) {
     if (GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(CLI.Callee)) {
       StringRef Sym = G->getGlobal()->getName();
       Function *F = G->getGlobal()->getParent()->getFunction(Sym);
       if (F && F->hasFnAttribute("__Mips16RetHelper")) {
         Mask = MipsRegisterInfo::getMips16RetHelperMask();
       }
     }
   }
   Ops.push_back(CLI.DAG.getRegisterMask(Mask));
 
   if (InFlag.getNode())
     Ops.push_back(InFlag);
 }
 
 void MipsTargetLowering::AdjustInstrPostInstrSelection(MachineInstr &MI,
                                                        SDNode *Node) const {
   switch (MI.getOpcode()) {
     default:
       return;
     case Mips::JALR:
     case Mips::JALRPseudo:
     case Mips::JALR64:
     case Mips::JALR64Pseudo:
     case Mips::JALR16_MM:
     case Mips::JALRC16_MMR6:
     case Mips::TAILCALLREG:
     case Mips::TAILCALLREG64:
     case Mips::TAILCALLR6REG:
     case Mips::TAILCALL64R6REG:
     case Mips::TAILCALLREG_MM:
     case Mips::TAILCALLREG_MMR6: {
       if (!EmitJalrReloc ||
           Subtarget.inMips16Mode() ||
           !isPositionIndependent() ||
           Node->getNumOperands() < 1 ||
           Node->getOperand(0).getNumOperands() < 2) {
         return;
       }
       // We are after the callee address, set by LowerCall().
       // If added to MI, asm printer will emit .reloc R_MIPS_JALR for the
       // symbol.
       const SDValue TargetAddr = Node->getOperand(0).getOperand(1);
       StringRef Sym;
       if (const GlobalAddressSDNode *G =
               dyn_cast_or_null<const GlobalAddressSDNode>(TargetAddr)) {
         // We must not emit the R_MIPS_JALR relocation against data symbols
         // since this will cause run-time crashes if the linker replaces the
         // call instruction with a relative branch to the data symbol.
         if (!isa<Function>(G->getGlobal())) {
           LLVM_DEBUG(dbgs() << "Not adding R_MIPS_JALR against data symbol "
                             << G->getGlobal()->getName() << "\n");
           return;
         }
         Sym = G->getGlobal()->getName();
       }
       else if (const ExternalSymbolSDNode *ES =
                    dyn_cast_or_null<const ExternalSymbolSDNode>(TargetAddr)) {
         Sym = ES->getSymbol();
       }
 
       if (Sym.empty())
         return;
 
       MachineFunction *MF = MI.getParent()->getParent();
       MCSymbol *S = MF->getContext().getOrCreateSymbol(Sym);
       LLVM_DEBUG(dbgs() << "Adding R_MIPS_JALR against " << Sym << "\n");
       MI.addOperand(MachineOperand::CreateMCSymbol(S, MipsII::MO_JALR));
     }
   }
 }
 
 /// LowerCall - functions arguments are copied from virtual regs to
 /// (physical regs)/(stack frame), CALLSEQ_START and CALLSEQ_END are emitted.
 SDValue
 MipsTargetLowering::LowerCall(TargetLowering::CallLoweringInfo &CLI,
                               SmallVectorImpl<SDValue> &InVals) const {
   SelectionDAG &DAG                     = CLI.DAG;
   SDLoc DL                              = CLI.DL;
   SmallVectorImpl<ISD::OutputArg> &Outs = CLI.Outs;
   SmallVectorImpl<SDValue> &OutVals     = CLI.OutVals;
   SmallVectorImpl<ISD::InputArg> &Ins   = CLI.Ins;
   SDValue Chain                         = CLI.Chain;
   SDValue Callee                        = CLI.Callee;
   bool &IsTailCall                      = CLI.IsTailCall;
   CallingConv::ID CallConv              = CLI.CallConv;
   bool IsVarArg                         = CLI.IsVarArg;
 
   MachineFunction &MF = DAG.getMachineFunction();
   MachineFrameInfo &MFI = MF.getFrameInfo();
   const TargetFrameLowering *TFL = Subtarget.getFrameLowering();
   MipsFunctionInfo *FuncInfo = MF.getInfo<MipsFunctionInfo>();
   bool IsPIC = isPositionIndependent();
 
   // Analyze operands of the call, assigning locations to each operand.
   SmallVector<CCValAssign, 16> ArgLocs;
   MipsCCState CCInfo(
       CallConv, IsVarArg, DAG.getMachineFunction(), ArgLocs, *DAG.getContext(),
       MipsCCState::getSpecialCallingConvForCallee(Callee.getNode(), Subtarget));
 
   const ExternalSymbolSDNode *ES =
       dyn_cast_or_null<const ExternalSymbolSDNode>(Callee.getNode());
 
   // There is one case where CALLSEQ_START..CALLSEQ_END can be nested, which
   // is during the lowering of a call with a byval argument which produces
   // a call to memcpy. For the O32 case, this causes the caller to allocate
   // stack space for the reserved argument area for the callee, then recursively
   // again for the memcpy call. In the NEWABI case, this doesn't occur as those
   // ABIs mandate that the callee allocates the reserved argument area. We do
   // still produce nested CALLSEQ_START..CALLSEQ_END with zero space though.
   //
   // If the callee has a byval argument and memcpy is used, we are mandated
   // to already have produced a reserved argument area for the callee for O32.
   // Therefore, the reserved argument area can be reused for both calls.
   //
   // Other cases of calling memcpy cannot have a chain with a CALLSEQ_START
   // present, as we have yet to hook that node onto the chain.
   //
   // Hence, the CALLSEQ_START and CALLSEQ_END nodes can be eliminated in this
   // case. GCC does a similar trick, in that wherever possible, it calculates
   // the maximum out going argument area (including the reserved area), and
   // preallocates the stack space on entrance to the caller.
   //
   // FIXME: We should do the same for efficiency and space.
 
   // Note: The check on the calling convention below must match
   //       MipsABIInfo::GetCalleeAllocdArgSizeInBytes().
   bool MemcpyInByVal = ES &&
                        StringRef(ES->getSymbol()) == StringRef("memcpy") &&
                        CallConv != CallingConv::Fast &&
                        Chain.getOpcode() == ISD::CALLSEQ_START;
 
   // Allocate the reserved argument area. It seems strange to do this from the
   // caller side but removing it breaks the frame size calculation.
   unsigned ReservedArgArea =
       MemcpyInByVal ? 0 : ABI.GetCalleeAllocdArgSizeInBytes(CallConv);
   CCInfo.AllocateStack(ReservedArgArea, Align(1));
 
   CCInfo.AnalyzeCallOperands(Outs, CC_Mips, CLI.getArgs(),
                              ES ? ES->getSymbol() : nullptr);
 
   // Get a count of how many bytes are to be pushed on the stack.
   unsigned NextStackOffset = CCInfo.getNextStackOffset();
 
   // Call site info for function parameters tracking.
   MachineFunction::CallSiteInfo CSInfo;
 
   // Check if it's really possible to do a tail call. Restrict it to functions
   // that are part of this compilation unit.
   bool InternalLinkage = false;
   if (IsTailCall) {
     IsTailCall = isEligibleForTailCallOptimization(
         CCInfo, NextStackOffset, *MF.getInfo<MipsFunctionInfo>());
      if (GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(Callee)) {
       InternalLinkage = G->getGlobal()->hasInternalLinkage();
       IsTailCall &= (InternalLinkage || G->getGlobal()->hasLocalLinkage() ||
                      G->getGlobal()->hasPrivateLinkage() ||
                      G->getGlobal()->hasHiddenVisibility() ||
                      G->getGlobal()->hasProtectedVisibility());
      }
   }
   if (!IsTailCall && CLI.CB && CLI.CB->isMustTailCall())
     report_fatal_error("failed to perform tail call elimination on a call "
                        "site marked musttail");
 
   if (IsTailCall)
     ++NumTailCalls;
 
   // Chain is the output chain of the last Load/Store or CopyToReg node.
   // ByValChain is the output chain of the last Memcpy node created for copying
   // byval arguments to the stack.
   unsigned StackAlignment = TFL->getStackAlignment();
   NextStackOffset = alignTo(NextStackOffset, StackAlignment);
   SDValue NextStackOffsetVal = DAG.getIntPtrConstant(NextStackOffset, DL, true);
 
   if (!(IsTailCall || MemcpyInByVal))
     Chain = DAG.getCALLSEQ_START(Chain, NextStackOffset, 0, DL);
 
   SDValue StackPtr =
       DAG.getCopyFromReg(Chain, DL, ABI.IsN64() ? Mips::SP_64 : Mips::SP,
                          getPointerTy(DAG.getDataLayout()));
 
   std::deque<std::pair<unsigned, SDValue>> RegsToPass;
   SmallVector<SDValue, 8> MemOpChains;
 
   CCInfo.rewindByValRegsInfo();
 
   // Walk the register/memloc assignments, inserting copies/loads.
   for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {
     SDValue Arg = OutVals[i];
     CCValAssign &VA = ArgLocs[i];
     MVT ValVT = VA.getValVT(), LocVT = VA.getLocVT();
     ISD::ArgFlagsTy Flags = Outs[i].Flags;
     bool UseUpperBits = false;
 
     // ByVal Arg.
     if (Flags.isByVal()) {
       unsigned FirstByValReg, LastByValReg;
       unsigned ByValIdx = CCInfo.getInRegsParamsProcessed();
       CCInfo.getInRegsParamInfo(ByValIdx, FirstByValReg, LastByValReg);
 
       assert(Flags.getByValSize() &&
              "ByVal args of size 0 should have been ignored by front-end.");
       assert(ByValIdx < CCInfo.getInRegsParamsCount());
       assert(!IsTailCall &&
              "Do not tail-call optimize if there is a byval argument.");
       passByValArg(Chain, DL, RegsToPass, MemOpChains, StackPtr, MFI, DAG, Arg,
                    FirstByValReg, LastByValReg, Flags, Subtarget.isLittle(),
                    VA);
       CCInfo.nextInRegsParam();
       continue;
     }
 
     // Promote the value if needed.
     switch (VA.getLocInfo()) {
     default:
       llvm_unreachable("Unknown loc info!");
     case CCValAssign::Full:
       if (VA.isRegLoc()) {
         if ((ValVT == MVT::f32 && LocVT == MVT::i32) ||
             (ValVT == MVT::f64 && LocVT == MVT::i64) ||
             (ValVT == MVT::i64 && LocVT == MVT::f64))
           Arg = DAG.getNode(ISD::BITCAST, DL, LocVT, Arg);
         else if (ValVT == MVT::f64 && LocVT == MVT::i32) {
           SDValue Lo = DAG.getNode(MipsISD::ExtractElementF64, DL, MVT::i32,
                                    Arg, DAG.getConstant(0, DL, MVT::i32));
           SDValue Hi = DAG.getNode(MipsISD::ExtractElementF64, DL, MVT::i32,
                                    Arg, DAG.getConstant(1, DL, MVT::i32));
           if (!Subtarget.isLittle())
             std::swap(Lo, Hi);
           Register LocRegLo = VA.getLocReg();
           unsigned LocRegHigh = getNextIntArgReg(LocRegLo);
           RegsToPass.push_back(std::make_pair(LocRegLo, Lo));
           RegsToPass.push_back(std::make_pair(LocRegHigh, Hi));
           continue;
         }
       }
       break;
     case CCValAssign::BCvt:
       Arg = DAG.getNode(ISD::BITCAST, DL, LocVT, Arg);
       break;
     case CCValAssign::SExtUpper:
       UseUpperBits = true;
       LLVM_FALLTHROUGH;
     case CCValAssign::SExt:
       Arg = DAG.getNode(ISD::SIGN_EXTEND, DL, LocVT, Arg);
       break;
     case CCValAssign::ZExtUpper:
       UseUpperBits = true;
       LLVM_FALLTHROUGH;
     case CCValAssign::ZExt:
       Arg = DAG.getNode(ISD::ZERO_EXTEND, DL, LocVT, Arg);
       break;
     case CCValAssign::AExtUpper:
       UseUpperBits = true;
       LLVM_FALLTHROUGH;
     case CCValAssign::AExt:
       Arg = DAG.getNode(ISD::ANY_EXTEND, DL, LocVT, Arg);
       break;
     }
 
     if (UseUpperBits) {
       unsigned ValSizeInBits = Outs[i].ArgVT.getSizeInBits();
       unsigned LocSizeInBits = VA.getLocVT().getSizeInBits();
       Arg = DAG.getNode(
           ISD::SHL, DL, VA.getLocVT(), Arg,
           DAG.getConstant(LocSizeInBits - ValSizeInBits, DL, VA.getLocVT()));
     }
 
     // Arguments that can be passed on register must be kept at
     // RegsToPass vector
     if (VA.isRegLoc()) {
       RegsToPass.push_back(std::make_pair(VA.getLocReg(), Arg));
 
       // If the parameter is passed through reg $D, which splits into
       // two physical registers, avoid creating call site info.
       if (Mips::AFGR64RegClass.contains(VA.getLocReg()))
         continue;
 
       // Collect CSInfo about which register passes which parameter.
       const TargetOptions &Options = DAG.getTarget().Options;
       if (Options.SupportsDebugEntryValues)
         CSInfo.emplace_back(VA.getLocReg(), i);
 
       continue;
     }
 
     // Register can't get to this point...
     assert(VA.isMemLoc());
 
     // emit ISD::STORE whichs stores the
     // parameter value to a stack Location
     MemOpChains.push_back(passArgOnStack(StackPtr, VA.getLocMemOffset(),
                                          Chain, Arg, DL, IsTailCall, DAG));
   }
 
   // Transform all store nodes into one single node because all store
   // nodes are independent of each other.
   if (!MemOpChains.empty())
     Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, MemOpChains);
 
   // If the callee is a GlobalAddress/ExternalSymbol node (quite common, every
   // direct call is) turn it into a TargetGlobalAddress/TargetExternalSymbol
   // node so that legalize doesn't hack it.
 
   EVT Ty = Callee.getValueType();
   bool GlobalOrExternal = false, IsCallReloc = false;
 
   // The long-calls feature is ignored in case of PIC.
   // While we do not support -mshared / -mno-shared properly,
   // ignore long-calls in case of -mabicalls too.
   if (!Subtarget.isABICalls() && !IsPIC) {
     // If the function should be called using "long call",
     // get its address into a register to prevent using
     // of the `jal` instruction for the direct call.
     if (auto *N = dyn_cast<ExternalSymbolSDNode>(Callee)) {
       if (Subtarget.useLongCalls())
         Callee = Subtarget.hasSym32()
                      ? getAddrNonPIC(N, SDLoc(N), Ty, DAG)
                      : getAddrNonPICSym64(N, SDLoc(N), Ty, DAG);
     } else if (auto *N = dyn_cast<GlobalAddressSDNode>(Callee)) {
       bool UseLongCalls = Subtarget.useLongCalls();
       // If the function has long-call/far/near attribute
       // it overrides command line switch pased to the backend.
       if (auto *F = dyn_cast<Function>(N->getGlobal())) {
         if (F->hasFnAttribute("long-call"))
           UseLongCalls = true;
         else if (F->hasFnAttribute("short-call"))
           UseLongCalls = false;
       }
       if (UseLongCalls)
         Callee = Subtarget.hasSym32()
                      ? getAddrNonPIC(N, SDLoc(N), Ty, DAG)
                      : getAddrNonPICSym64(N, SDLoc(N), Ty, DAG);
     }
   }
 
   if (GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(Callee)) {
     if (IsPIC) {
       const GlobalValue *Val = G->getGlobal();
       InternalLinkage = Val->hasInternalLinkage();
 
       if (InternalLinkage)
         Callee = getAddrLocal(G, DL, Ty, DAG, ABI.IsN32() || ABI.IsN64());
       else if (Subtarget.useXGOT()) {
         Callee = getAddrGlobalLargeGOT(G, DL, Ty, DAG, MipsII::MO_CALL_HI16,
                                        MipsII::MO_CALL_LO16, Chain,
                                        FuncInfo->callPtrInfo(MF, Val));
         IsCallReloc = true;
       } else {
         Callee = getAddrGlobal(G, DL, Ty, DAG, MipsII::MO_GOT_CALL, Chain,
                                FuncInfo->callPtrInfo(MF, Val));
         IsCallReloc = true;
       }
     } else
       Callee = DAG.getTargetGlobalAddress(G->getGlobal(), DL,
                                           getPointerTy(DAG.getDataLayout()), 0,
                                           MipsII::MO_NO_FLAG);
     GlobalOrExternal = true;
   }
   else if (ExternalSymbolSDNode *S = dyn_cast<ExternalSymbolSDNode>(Callee)) {
     const char *Sym = S->getSymbol();
 
     if (!IsPIC) // static
       Callee = DAG.getTargetExternalSymbol(
           Sym, getPointerTy(DAG.getDataLayout()), MipsII::MO_NO_FLAG);
     else if (Subtarget.useXGOT()) {
       Callee = getAddrGlobalLargeGOT(S, DL, Ty, DAG, MipsII::MO_CALL_HI16,
                                      MipsII::MO_CALL_LO16, Chain,
                                      FuncInfo->callPtrInfo(MF, Sym));
       IsCallReloc = true;
     } else { // PIC
       Callee = getAddrGlobal(S, DL, Ty, DAG, MipsII::MO_GOT_CALL, Chain,
                              FuncInfo->callPtrInfo(MF, Sym));
       IsCallReloc = true;
     }
 
     GlobalOrExternal = true;
   }
 
   SmallVector<SDValue, 8> Ops(1, Chain);
   SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);
 
   getOpndList(Ops, RegsToPass, IsPIC, GlobalOrExternal, InternalLinkage,
               IsCallReloc, CLI, Callee, Chain);
 
   if (IsTailCall) {
     MF.getFrameInfo().setHasTailCall();
     SDValue Ret = DAG.getNode(MipsISD::TailCall, DL, MVT::Other, Ops);
     DAG.addCallSiteInfo(Ret.getNode(), std::move(CSInfo));
     return Ret;
   }
 
   Chain = DAG.getNode(MipsISD::JmpLink, DL, NodeTys, Ops);
   SDValue InFlag = Chain.getValue(1);
 
   DAG.addCallSiteInfo(Chain.getNode(), std::move(CSInfo));
 
   // Create the CALLSEQ_END node in the case of where it is not a call to
   // memcpy.
   if (!(MemcpyInByVal)) {
     Chain = DAG.getCALLSEQ_END(Chain, NextStackOffsetVal,
                                DAG.getIntPtrConstant(0, DL, true), InFlag, DL);
     InFlag = Chain.getValue(1);
   }
 
   // Handle result values, copying them out of physregs into vregs that we
   // return.
   return LowerCallResult(Chain, InFlag, CallConv, IsVarArg, Ins, DL, DAG,
                          InVals, CLI);
 }
 
 /// LowerCallResult - Lower the result values of a call into the
 /// appropriate copies out of appropriate physical registers.
 SDValue MipsTargetLowering::LowerCallResult(
     SDValue Chain, SDValue InFlag, CallingConv::ID CallConv, bool IsVarArg,
     const SmallVectorImpl<ISD::InputArg> &Ins, const SDLoc &DL,
     SelectionDAG &DAG, SmallVectorImpl<SDValue> &InVals,
     TargetLowering::CallLoweringInfo &CLI) const {
   // Assign locations to each value returned by this call.
   SmallVector<CCValAssign, 16> RVLocs;
   MipsCCState CCInfo(CallConv, IsVarArg, DAG.getMachineFunction(), RVLocs,
                      *DAG.getContext());
 
   const ExternalSymbolSDNode *ES =
       dyn_cast_or_null<const ExternalSymbolSDNode>(CLI.Callee.getNode());
   CCInfo.AnalyzeCallResult(Ins, RetCC_Mips, CLI.RetTy,
                            ES ? ES->getSymbol() : nullptr);
 
   // Copy all of the result registers out of their specified physreg.
   for (unsigned i = 0; i != RVLocs.size(); ++i) {
     CCValAssign &VA = RVLocs[i];
     assert(VA.isRegLoc() && "Can only return in registers!");
 
     SDValue Val = DAG.getCopyFromReg(Chain, DL, RVLocs[i].getLocReg(),
                                      RVLocs[i].getLocVT(), InFlag);
     Chain = Val.getValue(1);
     InFlag = Val.getValue(2);
 
     if (VA.isUpperBitsInLoc()) {
       unsigned ValSizeInBits = Ins[i].ArgVT.getSizeInBits();
       unsigned LocSizeInBits = VA.getLocVT().getSizeInBits();
       unsigned Shift =
           VA.getLocInfo() == CCValAssign::ZExtUpper ? ISD::SRL : ISD::SRA;
       Val = DAG.getNode(
           Shift, DL, VA.getLocVT(), Val,
           DAG.getConstant(LocSizeInBits - ValSizeInBits, DL, VA.getLocVT()));
     }
 
     switch (VA.getLocInfo()) {
     default:
       llvm_unreachable("Unknown loc info!");
     case CCValAssign::Full:
       break;
     case CCValAssign::BCvt:
       Val = DAG.getNode(ISD::BITCAST, DL, VA.getValVT(), Val);
       break;
     case CCValAssign::AExt:
     case CCValAssign::AExtUpper:
       Val = DAG.getNode(ISD::TRUNCATE, DL, VA.getValVT(), Val);
       break;
     case CCValAssign::ZExt:
     case CCValAssign::ZExtUpper:
       Val = DAG.getNode(ISD::AssertZext, DL, VA.getLocVT(), Val,
                         DAG.getValueType(VA.getValVT()));
       Val = DAG.getNode(ISD::TRUNCATE, DL, VA.getValVT(), Val);
       break;
     case CCValAssign::SExt:
     case CCValAssign::SExtUpper:
       Val = DAG.getNode(ISD::AssertSext, DL, VA.getLocVT(), Val,
                         DAG.getValueType(VA.getValVT()));
       Val = DAG.getNode(ISD::TRUNCATE, DL, VA.getValVT(), Val);
       break;
     }
 
     InVals.push_back(Val);
   }
 
   return Chain;
 }
 
 static SDValue UnpackFromArgumentSlot(SDValue Val, const CCValAssign &VA,
                                       EVT ArgVT, const SDLoc &DL,
                                       SelectionDAG &DAG) {
   MVT LocVT = VA.getLocVT();
   EVT ValVT = VA.getValVT();
 
   // Shift into the upper bits if necessary.
   switch (VA.getLocInfo()) {
   default:
     break;
   case CCValAssign::AExtUpper:
   case CCValAssign::SExtUpper:
   case CCValAssign::ZExtUpper: {
     unsigned ValSizeInBits = ArgVT.getSizeInBits();
     unsigned LocSizeInBits = VA.getLocVT().getSizeInBits();
     unsigned Opcode =
         VA.getLocInfo() == CCValAssign::ZExtUpper ? ISD::SRL : ISD::SRA;
     Val = DAG.getNode(
         Opcode, DL, VA.getLocVT(), Val,
         DAG.getConstant(LocSizeInBits - ValSizeInBits, DL, VA.getLocVT()));
     break;
   }
   }
 
   // If this is an value smaller than the argument slot size (32-bit for O32,
   // 64-bit for N32/N64), it has been promoted in some way to the argument slot
   // size. Extract the value and insert any appropriate assertions regarding
   // sign/zero extension.
   switch (VA.getLocInfo()) {
   default:
     llvm_unreachable("Unknown loc info!");
   case CCValAssign::Full:
     break;
   case CCValAssign::AExtUpper:
   case CCValAssign::AExt:
     Val = DAG.getNode(ISD::TRUNCATE, DL, ValVT, Val);
     break;
   case CCValAssign::SExtUpper:
   case CCValAssign::SExt:
     Val = DAG.getNode(ISD::AssertSext, DL, LocVT, Val, DAG.getValueType(ValVT));
     Val = DAG.getNode(ISD::TRUNCATE, DL, ValVT, Val);
     break;
   case CCValAssign::ZExtUpper:
   case CCValAssign::ZExt:
     Val = DAG.getNode(ISD::AssertZext, DL, LocVT, Val, DAG.getValueType(ValVT));
     Val = DAG.getNode(ISD::TRUNCATE, DL, ValVT, Val);
     break;
   case CCValAssign::BCvt:
     Val = DAG.getNode(ISD::BITCAST, DL, ValVT, Val);
     break;
   }
 
   return Val;
 }
 
 //===----------------------------------------------------------------------===//
 //             Formal Arguments Calling Convention Implementation
 //===----------------------------------------------------------------------===//
 /// LowerFormalArguments - transform physical registers into virtual registers
 /// and generate load operations for arguments places on the stack.
 SDValue MipsTargetLowering::LowerFormalArguments(
     SDValue Chain, CallingConv::ID CallConv, bool IsVarArg,
     const SmallVectorImpl<ISD::InputArg> &Ins, const SDLoc &DL,
     SelectionDAG &DAG, SmallVectorImpl<SDValue> &InVals) const {
   MachineFunction &MF = DAG.getMachineFunction();
   MachineFrameInfo &MFI = MF.getFrameInfo();
   MipsFunctionInfo *MipsFI = MF.getInfo<MipsFunctionInfo>();
 
   MipsFI->setVarArgsFrameIndex(0);
 
   // Used with vargs to acumulate store chains.
   std::vector<SDValue> OutChains;
 
   // Assign locations to all of the incoming arguments.
   SmallVector<CCValAssign, 16> ArgLocs;
   MipsCCState CCInfo(CallConv, IsVarArg, DAG.getMachineFunction(), ArgLocs,
                      *DAG.getContext());
   CCInfo.AllocateStack(ABI.GetCalleeAllocdArgSizeInBytes(CallConv), Align(1));
   const Function &Func = DAG.getMachineFunction().getFunction();
   Function::const_arg_iterator FuncArg = Func.arg_begin();
 
   if (Func.hasFnAttribute("interrupt") && !Func.arg_empty())
     report_fatal_error(
         "Functions with the interrupt attribute cannot have arguments!");
 
   CCInfo.AnalyzeFormalArguments(Ins, CC_Mips_FixedArg);
   MipsFI->setFormalArgInfo(CCInfo.getNextStackOffset(),
                            CCInfo.getInRegsParamsCount() > 0);
 
   unsigned CurArgIdx = 0;
   CCInfo.rewindByValRegsInfo();
 
   for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {
     CCValAssign &VA = ArgLocs[i];
     if (Ins[i].isOrigArg()) {
       std::advance(FuncArg, Ins[i].getOrigArgIndex() - CurArgIdx);
       CurArgIdx = Ins[i].getOrigArgIndex();
     }
     EVT ValVT = VA.getValVT();
     ISD::ArgFlagsTy Flags = Ins[i].Flags;
     bool IsRegLoc = VA.isRegLoc();
 
     if (Flags.isByVal()) {
       assert(Ins[i].isOrigArg() && "Byval arguments cannot be implicit");
       unsigned FirstByValReg, LastByValReg;
       unsigned ByValIdx = CCInfo.getInRegsParamsProcessed();
       CCInfo.getInRegsParamInfo(ByValIdx, FirstByValReg, LastByValReg);
 
       assert(Flags.getByValSize() &&
              "ByVal args of size 0 should have been ignored by front-end.");
       assert(ByValIdx < CCInfo.getInRegsParamsCount());
       copyByValRegs(Chain, DL, OutChains, DAG, Flags, InVals, &*FuncArg,
                     FirstByValReg, LastByValReg, VA, CCInfo);
       CCInfo.nextInRegsParam();
       continue;
     }
 
     // Arguments stored on registers
     if (IsRegLoc) {
       MVT RegVT = VA.getLocVT();
       Register ArgReg = VA.getLocReg();
       const TargetRegisterClass *RC = getRegClassFor(RegVT);
 
       // Transform the arguments stored on
       // physical registers into virtual ones
       unsigned Reg = addLiveIn(DAG.getMachineFunction(), ArgReg, RC);
       SDValue ArgValue = DAG.getCopyFromReg(Chain, DL, Reg, RegVT);
 
       ArgValue = UnpackFromArgumentSlot(ArgValue, VA, Ins[i].ArgVT, DL, DAG);
 
       // Handle floating point arguments passed in integer registers and
       // long double arguments passed in floating point registers.
       if ((RegVT == MVT::i32 && ValVT == MVT::f32) ||
           (RegVT == MVT::i64 && ValVT == MVT::f64) ||
           (RegVT == MVT::f64 && ValVT == MVT::i64))
         ArgValue = DAG.getNode(ISD::BITCAST, DL, ValVT, ArgValue);
       else if (ABI.IsO32() && RegVT == MVT::i32 &&
                ValVT == MVT::f64) {
         unsigned Reg2 = addLiveIn(DAG.getMachineFunction(),
                                   getNextIntArgReg(ArgReg), RC);
         SDValue ArgValue2 = DAG.getCopyFromReg(Chain, DL, Reg2, RegVT);
         if (!Subtarget.isLittle())
           std::swap(ArgValue, ArgValue2);
         ArgValue = DAG.getNode(MipsISD::BuildPairF64, DL, MVT::f64,
                                ArgValue, ArgValue2);
       }
 
       InVals.push_back(ArgValue);
     } else { // VA.isRegLoc()
       MVT LocVT = VA.getLocVT();
 
       if (ABI.IsO32()) {
         // We ought to be able to use LocVT directly but O32 sets it to i32
         // when allocating floating point values to integer registers.
         // This shouldn't influence how we load the value into registers unless
         // we are targeting softfloat.
         if (VA.getValVT().isFloatingPoint() && !Subtarget.useSoftFloat())
           LocVT = VA.getValVT();
       }
 
       // sanity check
       assert(VA.isMemLoc());
 
       // The stack pointer offset is relative to the caller stack frame.
       int FI = MFI.CreateFixedObject(LocVT.getSizeInBits() / 8,
                                      VA.getLocMemOffset(), true);
 
       // Create load nodes to retrieve arguments from the stack
       SDValue FIN = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));
       SDValue ArgValue = DAG.getLoad(
           LocVT, DL, Chain, FIN,
           MachinePointerInfo::getFixedStack(DAG.getMachineFunction(), FI));
       OutChains.push_back(ArgValue.getValue(1));
 
       ArgValue = UnpackFromArgumentSlot(ArgValue, VA, Ins[i].ArgVT, DL, DAG);
 
       InVals.push_back(ArgValue);
     }
   }
 
   for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {
     // The mips ABIs for returning structs by value requires that we copy
     // the sret argument into $v0 for the return. Save the argument into
     // a virtual register so that we can access it from the return points.
     if (Ins[i].Flags.isSRet()) {
       unsigned Reg = MipsFI->getSRetReturnReg();
       if (!Reg) {
         Reg = MF.getRegInfo().createVirtualRegister(
             getRegClassFor(ABI.IsN64() ? MVT::i64 : MVT::i32));
         MipsFI->setSRetReturnReg(Reg);
       }
       SDValue Copy = DAG.getCopyToReg(DAG.getEntryNode(), DL, Reg, InVals[i]);
       Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, Copy, Chain);
       break;
     }
   }
 
   if (IsVarArg)
     writeVarArgRegs(OutChains, Chain, DL, DAG, CCInfo);
 
   // All stores are grouped in one node to allow the matching between
   // the size of Ins and InVals. This only happens when on varg functions
   if (!OutChains.empty()) {
     OutChains.push_back(Chain);
     Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, OutChains);
   }
 
   return Chain;
 }
 
 //===----------------------------------------------------------------------===//
 //               Return Value Calling Convention Implementation
 //===----------------------------------------------------------------------===//
 
 bool
 MipsTargetLowering::CanLowerReturn(CallingConv::ID CallConv,
                                    MachineFunction &MF, bool IsVarArg,
                                    const SmallVectorImpl<ISD::OutputArg> &Outs,
                                    LLVMContext &Context) const {
   SmallVector<CCValAssign, 16> RVLocs;
   MipsCCState CCInfo(CallConv, IsVarArg, MF, RVLocs, Context);
   return CCInfo.CheckReturn(Outs, RetCC_Mips);
 }
 
 bool MipsTargetLowering::shouldSignExtendTypeInLibCall(EVT Type,
                                                        bool IsSigned) const {
   if ((ABI.IsN32() || ABI.IsN64()) && Type == MVT::i32)
       return true;
 
   return IsSigned;
 }
 
 SDValue
 MipsTargetLowering::LowerInterruptReturn(SmallVectorImpl<SDValue> &RetOps,
                                          const SDLoc &DL,
                                          SelectionDAG &DAG) const {
   MachineFunction &MF = DAG.getMachineFunction();
   MipsFunctionInfo *MipsFI = MF.getInfo<MipsFunctionInfo>();
 
   MipsFI->setISR();
 
   return DAG.getNode(MipsISD::ERet, DL, MVT::Other, RetOps);
 }
 
 SDValue
 MipsTargetLowering::LowerReturn(SDValue Chain, CallingConv::ID CallConv,
                                 bool IsVarArg,
                                 const SmallVectorImpl<ISD::OutputArg> &Outs,
                                 const SmallVectorImpl<SDValue> &OutVals,
                                 const SDLoc &DL, SelectionDAG &DAG) const {
   // CCValAssign - represent the assignment of
   // the return value to a location
   SmallVector<CCValAssign, 16> RVLocs;
   MachineFunction &MF = DAG.getMachineFunction();
 
   // CCState - Info about the registers and stack slot.
   MipsCCState CCInfo(CallConv, IsVarArg, MF, RVLocs, *DAG.getContext());
 
   // Analyze return values.
   CCInfo.AnalyzeReturn(Outs, RetCC_Mips);
 
   SDValue Flag;
   SmallVector<SDValue, 4> RetOps(1, Chain);
 
   // Copy the result values into the output registers.
   for (unsigned i = 0; i != RVLocs.size(); ++i) {
     SDValue Val = OutVals[i];
     CCValAssign &VA = RVLocs[i];
     assert(VA.isRegLoc() && "Can only return in registers!");
     bool UseUpperBits = false;
 
     switch (VA.getLocInfo()) {
     default:
       llvm_unreachable("Unknown loc info!");
     case CCValAssign::Full:
       break;
     case CCValAssign::BCvt:
       Val = DAG.getNode(ISD::BITCAST, DL, VA.getLocVT(), Val);
       break;
     case CCValAssign::AExtUpper:
       UseUpperBits = true;
       LLVM_FALLTHROUGH;
     case CCValAssign::AExt:
       Val = DAG.getNode(ISD::ANY_EXTEND, DL, VA.getLocVT(), Val);
       break;
     case CCValAssign::ZExtUpper:
       UseUpperBits = true;
       LLVM_FALLTHROUGH;
     case CCValAssign::ZExt:
       Val = DAG.getNode(ISD::ZERO_EXTEND, DL, VA.getLocVT(), Val);
       break;
     case CCValAssign::SExtUpper:
       UseUpperBits = true;
       LLVM_FALLTHROUGH;
     case CCValAssign::SExt:
       Val = DAG.getNode(ISD::SIGN_EXTEND, DL, VA.getLocVT(), Val);
       break;
     }
 
     if (UseUpperBits) {
       unsigned ValSizeInBits = Outs[i].ArgVT.getSizeInBits();
       unsigned LocSizeInBits = VA.getLocVT().getSizeInBits();
       Val = DAG.getNode(
           ISD::SHL, DL, VA.getLocVT(), Val,
           DAG.getConstant(LocSizeInBits - ValSizeInBits, DL, VA.getLocVT()));
     }
 
     Chain = DAG.getCopyToReg(Chain, DL, VA.getLocReg(), Val, Flag);
 
     // Guarantee that all emitted copies are stuck together with flags.
     Flag = Chain.getValue(1);
     RetOps.push_back(DAG.getRegister(VA.getLocReg(), VA.getLocVT()));
   }
 
   // The mips ABIs for returning structs by value requires that we copy
   // the sret argument into $v0 for the return. We saved the argument into
   // a virtual register in the entry block, so now we copy the value out
   // and into $v0.
   if (MF.getFunction().hasStructRetAttr()) {
     MipsFunctionInfo *MipsFI = MF.getInfo<MipsFunctionInfo>();
     unsigned Reg = MipsFI->getSRetReturnReg();
 
     if (!Reg)
       llvm_unreachable("sret virtual register not created in the entry block");
     SDValue Val =
         DAG.getCopyFromReg(Chain, DL, Reg, getPointerTy(DAG.getDataLayout()));
     unsigned V0 = ABI.IsN64() ? Mips::V0_64 : Mips::V0;
 
     Chain = DAG.getCopyToReg(Chain, DL, V0, Val, Flag);
     Flag = Chain.getValue(1);
     RetOps.push_back(DAG.getRegister(V0, getPointerTy(DAG.getDataLayout())));
   }
 
   RetOps[0] = Chain;  // Update chain.
 
   // Add the flag if we have it.
   if (Flag.getNode())
     RetOps.push_back(Flag);
 
   // ISRs must use "eret".
   if (DAG.getMachineFunction().getFunction().hasFnAttribute("interrupt"))
     return LowerInterruptReturn(RetOps, DL, DAG);
 
   // Standard return on Mips is a "jr $ra"
   return DAG.getNode(MipsISD::Ret, DL, MVT::Other, RetOps);
 }
 
 //===----------------------------------------------------------------------===//
 //                           Mips Inline Assembly Support
 //===----------------------------------------------------------------------===//
 
 /// getConstraintType - Given a constraint letter, return the type of
 /// constraint it is for this target.
 MipsTargetLowering::ConstraintType
 MipsTargetLowering::getConstraintType(StringRef Constraint) const {
   // Mips specific constraints
   // GCC config/mips/constraints.md
   //
   // 'd' : An address register. Equivalent to r
   //       unless generating MIPS16 code.
   // 'y' : Equivalent to r; retained for
   //       backwards compatibility.
   // 'c' : A register suitable for use in an indirect
   //       jump. This will always be $25 for -mabicalls.
   // 'l' : The lo register. 1 word storage.
   // 'x' : The hilo register pair. Double word storage.
   if (Constraint.size() == 1) {
     switch (Constraint[0]) {
       default : break;
       case 'd':
       case 'y':
       case 'f':
       case 'c':
       case 'l':
       case 'x':
         return C_RegisterClass;
       case 'R':
         return C_Memory;
     }
   }
 
   if (Constraint == "ZC")
     return C_Memory;
 
   return TargetLowering::getConstraintType(Constraint);
 }
 
 /// Examine constraint type and operand type and determine a weight value.
 /// This object must already have been set up with the operand type
 /// and the current alternative constraint selected.
 TargetLowering::ConstraintWeight
 MipsTargetLowering::getSingleConstraintMatchWeight(
     AsmOperandInfo &info, const char *constraint) const {
   ConstraintWeight weight = CW_Invalid;
   Value *CallOperandVal = info.CallOperandVal;
     // If we don't have a value, we can't do a match,
     // but allow it at the lowest weight.
   if (!CallOperandVal)
     return CW_Default;
   Type *type = CallOperandVal->getType();
   // Look at the constraint type.
   switch (*constraint) {
   default:
     weight = TargetLowering::getSingleConstraintMatchWeight(info, constraint);
     break;
   case 'd':
   case 'y':
     if (type->isIntegerTy())
       weight = CW_Register;
     break;
   case 'f': // FPU or MSA register
     if (Subtarget.hasMSA() && type->isVectorTy() &&
         type->getPrimitiveSizeInBits().getFixedSize() == 128)
       weight = CW_Register;
     else if (type->isFloatTy())
       weight = CW_Register;
     break;
   case 'c': // $25 for indirect jumps
   case 'l': // lo register
   case 'x': // hilo register pair
     if (type->isIntegerTy())
       weight = CW_SpecificReg;
     break;
   case 'I': // signed 16 bit immediate
   case 'J': // integer zero
   case 'K': // unsigned 16 bit immediate
   case 'L': // signed 32 bit immediate where lower 16 bits are 0
   case 'N': // immediate in the range of -65535 to -1 (inclusive)
   case 'O': // signed 15 bit immediate (+- 16383)
   case 'P': // immediate in the range of 65535 to 1 (inclusive)
     if (isa<ConstantInt>(CallOperandVal))
       weight = CW_Constant;
     break;
   case 'R':
     weight = CW_Memory;
     break;
   }
   return weight;
 }
 
 /// This is a helper function to parse a physical register string and split it
 /// into non-numeric and numeric parts (Prefix and Reg). The first boolean flag
 /// that is returned indicates whether parsing was successful. The second flag
 /// is true if the numeric part exists.
 static std::pair<bool, bool> parsePhysicalReg(StringRef C, StringRef &Prefix,
                                               unsigned long long &Reg) {
   if (C.front() != '{' || C.back() != '}')
     return std::make_pair(false, false);
 
   // Search for the first numeric character.
   StringRef::const_iterator I, B = C.begin() + 1, E = C.end() - 1;
   I = std::find_if(B, E, isdigit);
 
   Prefix = StringRef(B, I - B);
 
   // The second flag is set to false if no numeric characters were found.
   if (I == E)
     return std::make_pair(true, false);
 
   // Parse the numeric characters.
   return std::make_pair(!getAsUnsignedInteger(StringRef(I, E - I), 10, Reg),
                         true);
 }
 
 EVT MipsTargetLowering::getTypeForExtReturn(LLVMContext &Context, EVT VT,
                                             ISD::NodeType) const {
   bool Cond = !Subtarget.isABI_O32() && VT.getSizeInBits() == 32;
   EVT MinVT = getRegisterType(Context, Cond ? MVT::i64 : MVT::i32);
   return VT.bitsLT(MinVT) ? MinVT : VT;
 }
 
 std::pair<unsigned, const TargetRegisterClass *> MipsTargetLowering::
 parseRegForInlineAsmConstraint(StringRef C, MVT VT) const {
   const TargetRegisterInfo *TRI =
       Subtarget.getRegisterInfo();
   const TargetRegisterClass *RC;
   StringRef Prefix;
   unsigned long long Reg;
 
   std::pair<bool, bool> R = parsePhysicalReg(C, Prefix, Reg);
 
   if (!R.first)
     return std::make_pair(0U, nullptr);
 
   if ((Prefix == "hi" || Prefix == "lo")) { // Parse hi/lo.
     // No numeric characters follow "hi" or "lo".
     if (R.second)
       return std::make_pair(0U, nullptr);
 
     RC = TRI->getRegClass(Prefix == "hi" ?
                           Mips::HI32RegClassID : Mips::LO32RegClassID);
     return std::make_pair(*(RC->begin()), RC);
   } else if (Prefix.startswith("$msa")) {
     // Parse $msa(ir|csr|access|save|modify|request|map|unmap)
 
     // No numeric characters follow the name.
     if (R.second)
       return std::make_pair(0U, nullptr);
 
     Reg = StringSwitch<unsigned long long>(Prefix)
               .Case("$msair", Mips::MSAIR)
               .Case("$msacsr", Mips::MSACSR)
               .Case("$msaaccess", Mips::MSAAccess)
               .Case("$msasave", Mips::MSASave)
               .Case("$msamodify", Mips::MSAModify)
               .Case("$msarequest", Mips::MSARequest)
               .Case("$msamap", Mips::MSAMap)
               .Case("$msaunmap", Mips::MSAUnmap)
               .Default(0);
 
     if (!Reg)
       return std::make_pair(0U, nullptr);
 
     RC = TRI->getRegClass(Mips::MSACtrlRegClassID);
     return std::make_pair(Reg, RC);
   }
 
   if (!R.second)
     return std::make_pair(0U, nullptr);
 
   if (Prefix == "$f") { // Parse $f0-$f31.
     // If the size of FP registers is 64-bit or Reg is an even number, select
     // the 64-bit register class. Otherwise, select the 32-bit register class.
     if (VT == MVT::Other)
       VT = (Subtarget.isFP64bit() || !(Reg % 2)) ? MVT::f64 : MVT::f32;
 
     RC = getRegClassFor(VT);
 
     if (RC == &Mips::AFGR64RegClass) {
       assert(Reg % 2 == 0);
       Reg >>= 1;
     }
   } else if (Prefix == "$fcc") // Parse $fcc0-$fcc7.
     RC = TRI->getRegClass(Mips::FCCRegClassID);
   else if (Prefix == "$w") { // Parse $w0-$w31.
     RC = getRegClassFor((VT == MVT::Other) ? MVT::v16i8 : VT);
   } else { // Parse $0-$31.
     assert(Prefix == "$");
     RC = getRegClassFor((VT == MVT::Other) ? MVT::i32 : VT);
   }
 
   assert(Reg < RC->getNumRegs());
   return std::make_pair(*(RC->begin() + Reg), RC);
 }
 
 /// Given a register class constraint, like 'r', if this corresponds directly
 /// to an LLVM register class, return a register of 0 and the register class
 /// pointer.
 std::pair<unsigned, const TargetRegisterClass *>
 MipsTargetLowering::getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI,
                                                  StringRef Constraint,
                                                  MVT VT) const {
   if (Constraint.size() == 1) {
     switch (Constraint[0]) {
     case 'd': // Address register. Same as 'r' unless generating MIPS16 code.
     case 'y': // Same as 'r'. Exists for compatibility.
     case 'r':
       if (VT == MVT::i32 || VT == MVT::i16 || VT == MVT::i8) {
         if (Subtarget.inMips16Mode())
           return std::make_pair(0U, &Mips::CPU16RegsRegClass);
         return std::make_pair(0U, &Mips::GPR32RegClass);
       }
       if (VT == MVT::i64 && !Subtarget.isGP64bit())
         return std::make_pair(0U, &Mips::GPR32RegClass);
       if (VT == MVT::i64 && Subtarget.isGP64bit())
         return std::make_pair(0U, &Mips::GPR64RegClass);
       // This will generate an error message
       return std::make_pair(0U, nullptr);
     case 'f': // FPU or MSA register
       if (VT == MVT::v16i8)
         return std::make_pair(0U, &Mips::MSA128BRegClass);
       else if (VT == MVT::v8i16 || VT == MVT::v8f16)
         return std::make_pair(0U, &Mips::MSA128HRegClass);
       else if (VT == MVT::v4i32 || VT == MVT::v4f32)
         return std::make_pair(0U, &Mips::MSA128WRegClass);
       else if (VT == MVT::v2i64 || VT == MVT::v2f64)
         return std::make_pair(0U, &Mips::MSA128DRegClass);
       else if (VT == MVT::f32)
         return std::make_pair(0U, &Mips::FGR32RegClass);
       else if ((VT == MVT::f64) && (!Subtarget.isSingleFloat())) {
         if (Subtarget.isFP64bit())
           return std::make_pair(0U, &Mips::FGR64RegClass);
         return std::make_pair(0U, &Mips::AFGR64RegClass);
       }
       break;
     case 'c': // register suitable for indirect jump
       if (VT == MVT::i32)
         return std::make_pair((unsigned)Mips::T9, &Mips::GPR32RegClass);
       if (VT == MVT::i64)
         return std::make_pair((unsigned)Mips::T9_64, &Mips::GPR64RegClass);
       // This will generate an error message
       return std::make_pair(0U, nullptr);
     case 'l': // use the `lo` register to store values
               // that are no bigger than a word
       if (VT == MVT::i32 || VT == MVT::i16 || VT == MVT::i8)
         return std::make_pair((unsigned)Mips::LO0, &Mips::LO32RegClass);
       return std::make_pair((unsigned)Mips::LO0_64, &Mips::LO64RegClass);
     case 'x': // use the concatenated `hi` and `lo` registers
               // to store doubleword values
       // Fixme: Not triggering the use of both hi and low
       // This will generate an error message
       return std::make_pair(0U, nullptr);
     }
   }
 
   if (!Constraint.empty()) {
     std::pair<unsigned, const TargetRegisterClass *> R;
     R = parseRegForInlineAsmConstraint(Constraint, VT);
 
     if (R.second)
       return R;
   }
 
   return TargetLowering::getRegForInlineAsmConstraint(TRI, Constraint, VT);
 }
 
 /// LowerAsmOperandForConstraint - Lower the specified operand into the Ops
 /// vector.  If it is invalid, don't add anything to Ops.
 void MipsTargetLowering::LowerAsmOperandForConstraint(SDValue Op,
                                                      std::string &Constraint,
                                                      std::vector<SDValue>&Ops,
                                                      SelectionDAG &DAG) const {
   SDLoc DL(Op);
   SDValue Result;
 
   // Only support length 1 constraints for now.
   if (Constraint.length() > 1) return;
 
   char ConstraintLetter = Constraint[0];
   switch (ConstraintLetter) {
   default: break; // This will fall through to the generic implementation
   case 'I': // Signed 16 bit constant
     // If this fails, the parent routine will give an error
     if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
       EVT Type = Op.getValueType();
       int64_t Val = C->getSExtValue();
       if (isInt<16>(Val)) {
         Result = DAG.getTargetConstant(Val, DL, Type);
         break;
       }
     }
     return;
   case 'J': // integer zero
     if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
       EVT Type = Op.getValueType();
       int64_t Val = C->getZExtValue();
       if (Val == 0) {
         Result = DAG.getTargetConstant(0, DL, Type);
         break;
       }
     }
     return;
   case 'K': // unsigned 16 bit immediate
     if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
       EVT Type = Op.getValueType();
       uint64_t Val = (uint64_t)C->getZExtValue();
       if (isUInt<16>(Val)) {
         Result = DAG.getTargetConstant(Val, DL, Type);
         break;
       }
     }
     return;
   case 'L': // signed 32 bit immediate where lower 16 bits are 0
     if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
       EVT Type = Op.getValueType();
       int64_t Val = C->getSExtValue();
       if ((isInt<32>(Val)) && ((Val & 0xffff) == 0)){
         Result = DAG.getTargetConstant(Val, DL, Type);
         break;
       }
     }
     return;
   case 'N': // immediate in the range of -65535 to -1 (inclusive)
     if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
       EVT Type = Op.getValueType();
       int64_t Val = C->getSExtValue();
       if ((Val >= -65535) && (Val <= -1)) {
         Result = DAG.getTargetConstant(Val, DL, Type);
         break;
       }
     }
     return;
   case 'O': // signed 15 bit immediate
     if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
       EVT Type = Op.getValueType();
       int64_t Val = C->getSExtValue();
       if ((isInt<15>(Val))) {
         Result = DAG.getTargetConstant(Val, DL, Type);
         break;
       }
     }
     return;
   case 'P': // immediate in the range of 1 to 65535 (inclusive)
     if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op)) {
       EVT Type = Op.getValueType();
       int64_t Val = C->getSExtValue();
       if ((Val <= 65535) && (Val >= 1)) {
         Result = DAG.getTargetConstant(Val, DL, Type);
         break;
       }
     }
     return;
   }
 
   if (Result.getNode()) {
     Ops.push_back(Result);
     return;
   }
 
   TargetLowering::LowerAsmOperandForConstraint(Op, Constraint, Ops, DAG);
 }
 
 bool MipsTargetLowering::isLegalAddressingMode(const DataLayout &DL,
                                                const AddrMode &AM, Type *Ty,
                                                unsigned AS,
                                                Instruction *I) const {
   // No global is ever allowed as a base.
   if (AM.BaseGV)
     return false;
 
   switch (AM.Scale) {
   case 0: // "r+i" or just "i", depending on HasBaseReg.
     break;
   case 1:
     if (!AM.HasBaseReg) // allow "r+i".
       break;
     return false; // disallow "r+r" or "r+r+i".
   default:
     return false;
   }
 
   return true;
 }
 
 bool
 MipsTargetLowering::isOffsetFoldingLegal(const GlobalAddressSDNode *GA) const {
   // The Mips target isn't yet aware of offsets.
   return false;
 }
 
 EVT MipsTargetLowering::getOptimalMemOpType(
     const MemOp &Op, const AttributeList &FuncAttributes) const {
   if (Subtarget.hasMips64())
     return MVT::i64;
 
   return MVT::i32;
 }
 
 bool MipsTargetLowering::isFPImmLegal(const APFloat &Imm, EVT VT,
                                       bool ForCodeSize) const {
   if (VT != MVT::f32 && VT != MVT::f64)
     return false;
   if (Imm.isNegZero())
     return false;
   return Imm.isZero();
 }
 
 unsigned MipsTargetLowering::getJumpTableEncoding() const {
 
   // FIXME: For space reasons this should be: EK_GPRel32BlockAddress.
   if (ABI.IsN64() && isPositionIndependent())
     return MachineJumpTableInfo::EK_GPRel64BlockAddress;
 
   return TargetLowering::getJumpTableEncoding();
 }
 
 bool MipsTargetLowering::useSoftFloat() const {
   return Subtarget.useSoftFloat();
 }
 
 void MipsTargetLowering::copyByValRegs(
     SDValue Chain, const SDLoc &DL, std::vector<SDValue> &OutChains,
     SelectionDAG &DAG, const ISD::ArgFlagsTy &Flags,
     SmallVectorImpl<SDValue> &InVals, const Argument *FuncArg,
     unsigned FirstReg, unsigned LastReg, const CCValAssign &VA,
     MipsCCState &State) const {
   MachineFunction &MF = DAG.getMachineFunction();
   MachineFrameInfo &MFI = MF.getFrameInfo();
   unsigned GPRSizeInBytes = Subtarget.getGPRSizeInBytes();
   unsigned NumRegs = LastReg - FirstReg;
   unsigned RegAreaSize = NumRegs * GPRSizeInBytes;
   unsigned FrameObjSize = std::max(Flags.getByValSize(), RegAreaSize);
   int FrameObjOffset;
   ArrayRef<MCPhysReg> ByValArgRegs = ABI.GetByValArgRegs();
 
   if (RegAreaSize)
     FrameObjOffset =
         (int)ABI.GetCalleeAllocdArgSizeInBytes(State.getCallingConv()) -
         (int)((ByValArgRegs.size() - FirstReg) * GPRSizeInBytes);
   else
     FrameObjOffset = VA.getLocMemOffset();
 
   // Create frame object.
   EVT PtrTy = getPointerTy(DAG.getDataLayout());
   // Make the fixed object stored to mutable so that the load instructions
   // referencing it have their memory dependencies added.
   // Set the frame object as isAliased which clears the underlying objects
   // vector in ScheduleDAGInstrs::buildSchedGraph() resulting in addition of all
   // stores as dependencies for loads referencing this fixed object.
   int FI = MFI.CreateFixedObject(FrameObjSize, FrameObjOffset, false, true);
   SDValue FIN = DAG.getFrameIndex(FI, PtrTy);
   InVals.push_back(FIN);
 
   if (!NumRegs)
     return;
 
   // Copy arg registers.
   MVT RegTy = MVT::getIntegerVT(GPRSizeInBytes * 8);
   const TargetRegisterClass *RC = getRegClassFor(RegTy);
 
   for (unsigned I = 0; I < NumRegs; ++I) {
     unsigned ArgReg = ByValArgRegs[FirstReg + I];
     unsigned VReg = addLiveIn(MF, ArgReg, RC);
     unsigned Offset = I * GPRSizeInBytes;
     SDValue StorePtr = DAG.getNode(ISD::ADD, DL, PtrTy, FIN,
                                    DAG.getConstant(Offset, DL, PtrTy));
     SDValue Store = DAG.getStore(Chain, DL, DAG.getRegister(VReg, RegTy),
                                  StorePtr, MachinePointerInfo(FuncArg, Offset));
     OutChains.push_back(Store);
   }
 }
 
 // Copy byVal arg to registers and stack.
 void MipsTargetLowering::passByValArg(
     SDValue Chain, const SDLoc &DL,
     std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
     SmallVectorImpl<SDValue> &MemOpChains, SDValue StackPtr,
     MachineFrameInfo &MFI, SelectionDAG &DAG, SDValue Arg, unsigned FirstReg,
     unsigned LastReg, const ISD::ArgFlagsTy &Flags, bool isLittle,
     const CCValAssign &VA) const {
   unsigned ByValSizeInBytes = Flags.getByValSize();
   unsigned OffsetInBytes = 0; // From beginning of struct
   unsigned RegSizeInBytes = Subtarget.getGPRSizeInBytes();
   Align Alignment =
       std::min(Flags.getNonZeroByValAlign(), Align(RegSizeInBytes));
   EVT PtrTy = getPointerTy(DAG.getDataLayout()),
       RegTy = MVT::getIntegerVT(RegSizeInBytes * 8);
   unsigned NumRegs = LastReg - FirstReg;
 
   if (NumRegs) {
     ArrayRef<MCPhysReg> ArgRegs = ABI.GetByValArgRegs();
     bool LeftoverBytes = (NumRegs * RegSizeInBytes > ByValSizeInBytes);
     unsigned I = 0;
 
     // Copy words to registers.
     for (; I < NumRegs - LeftoverBytes; ++I, OffsetInBytes += RegSizeInBytes) {
       SDValue LoadPtr = DAG.getNode(ISD::ADD, DL, PtrTy, Arg,
                                     DAG.getConstant(OffsetInBytes, DL, PtrTy));
       SDValue LoadVal = DAG.getLoad(RegTy, DL, Chain, LoadPtr,
                                     MachinePointerInfo(), Alignment);
       MemOpChains.push_back(LoadVal.getValue(1));
       unsigned ArgReg = ArgRegs[FirstReg + I];
       RegsToPass.push_back(std::make_pair(ArgReg, LoadVal));
     }
 
     // Return if the struct has been fully copied.
     if (ByValSizeInBytes == OffsetInBytes)
       return;
 
     // Copy the remainder of the byval argument with sub-word loads and shifts.
     if (LeftoverBytes) {
       SDValue Val;
 
       for (unsigned LoadSizeInBytes = RegSizeInBytes / 2, TotalBytesLoaded = 0;
            OffsetInBytes < ByValSizeInBytes; LoadSizeInBytes /= 2) {
         unsigned RemainingSizeInBytes = ByValSizeInBytes - OffsetInBytes;
 
         if (RemainingSizeInBytes < LoadSizeInBytes)
           continue;
 
         // Load subword.
         SDValue LoadPtr = DAG.getNode(ISD::ADD, DL, PtrTy, Arg,
                                       DAG.getConstant(OffsetInBytes, DL,
                                                       PtrTy));
         SDValue LoadVal = DAG.getExtLoad(
             ISD::ZEXTLOAD, DL, RegTy, Chain, LoadPtr, MachinePointerInfo(),
             MVT::getIntegerVT(LoadSizeInBytes * 8), Alignment);
         MemOpChains.push_back(LoadVal.getValue(1));
 
         // Shift the loaded value.
         unsigned Shamt;
 
         if (isLittle)
           Shamt = TotalBytesLoaded * 8;
         else
           Shamt = (RegSizeInBytes - (TotalBytesLoaded + LoadSizeInBytes)) * 8;
 
         SDValue Shift = DAG.getNode(ISD::SHL, DL, RegTy, LoadVal,
                                     DAG.getConstant(Shamt, DL, MVT::i32));
 
         if (Val.getNode())
           Val = DAG.getNode(ISD::OR, DL, RegTy, Val, Shift);
         else
           Val = Shift;
 
         OffsetInBytes += LoadSizeInBytes;
         TotalBytesLoaded += LoadSizeInBytes;
         Alignment = std::min(Alignment, Align(LoadSizeInBytes));
       }
 
       unsigned ArgReg = ArgRegs[FirstReg + I];
       RegsToPass.push_back(std::make_pair(ArgReg, Val));
       return;
     }
   }
 
   // Copy remainder of byval arg to it with memcpy.
   unsigned MemCpySize = ByValSizeInBytes - OffsetInBytes;
   SDValue Src = DAG.getNode(ISD::ADD, DL, PtrTy, Arg,
                             DAG.getConstant(OffsetInBytes, DL, PtrTy));
   SDValue Dst = DAG.getNode(ISD::ADD, DL, PtrTy, StackPtr,
                             DAG.getIntPtrConstant(VA.getLocMemOffset(), DL));
   Chain = DAG.getMemcpy(
       Chain, DL, Dst, Src, DAG.getConstant(MemCpySize, DL, PtrTy),
       Align(Alignment), /*isVolatile=*/false, /*AlwaysInline=*/false,
       /*isTailCall=*/false, MachinePointerInfo(), MachinePointerInfo());
   MemOpChains.push_back(Chain);
 }
 
 void MipsTargetLowering::writeVarArgRegs(std::vector<SDValue> &OutChains,
                                          SDValue Chain, const SDLoc &DL,
                                          SelectionDAG &DAG,
                                          CCState &State) const {
   ArrayRef<MCPhysReg> ArgRegs = ABI.GetVarArgRegs();
   unsigned Idx = State.getFirstUnallocated(ArgRegs);
   unsigned RegSizeInBytes = Subtarget.getGPRSizeInBytes();
   MVT RegTy = MVT::getIntegerVT(RegSizeInBytes * 8);
   const TargetRegisterClass *RC = getRegClassFor(RegTy);
   MachineFunction &MF = DAG.getMachineFunction();
   MachineFrameInfo &MFI = MF.getFrameInfo();
   MipsFunctionInfo *MipsFI = MF.getInfo<MipsFunctionInfo>();
 
   // Offset of the first variable argument from stack pointer.
   int VaArgOffset;
 
   if (ArgRegs.size() == Idx)
     VaArgOffset = alignTo(State.getNextStackOffset(), RegSizeInBytes);
   else {
     VaArgOffset =
         (int)ABI.GetCalleeAllocdArgSizeInBytes(State.getCallingConv()) -
         (int)(RegSizeInBytes * (ArgRegs.size() - Idx));
   }
 
   // Record the frame index of the first variable argument
   // which is a value necessary to VASTART.
   int FI = MFI.CreateFixedObject(RegSizeInBytes, VaArgOffset, true);
   MipsFI->setVarArgsFrameIndex(FI);
 
   // Copy the integer registers that have not been used for argument passing
   // to the argument register save area. For O32, the save area is allocated
   // in the caller's stack frame, while for N32/64, it is allocated in the
   // callee's stack frame.
   for (unsigned I = Idx; I < ArgRegs.size();
        ++I, VaArgOffset += RegSizeInBytes) {
     unsigned Reg = addLiveIn(MF, ArgRegs[I], RC);
     SDValue ArgValue = DAG.getCopyFromReg(Chain, DL, Reg, RegTy);
     FI = MFI.CreateFixedObject(RegSizeInBytes, VaArgOffset, true);
     SDValue PtrOff = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));
     SDValue Store =
         DAG.getStore(Chain, DL, ArgValue, PtrOff, MachinePointerInfo());
     cast<StoreSDNode>(Store.getNode())->getMemOperand()->setValue(
         (Value *)nullptr);
     OutChains.push_back(Store);
   }
 }
 
 void MipsTargetLowering::HandleByVal(CCState *State, unsigned &Size,
                                      Align Alignment) const {
   const TargetFrameLowering *TFL = Subtarget.getFrameLowering();
 
   assert(Size && "Byval argument's size shouldn't be 0.");
 
   Alignment = std::min(Alignment, TFL->getStackAlign());
 
   unsigned FirstReg = 0;
   unsigned NumRegs = 0;
 
   if (State->getCallingConv() != CallingConv::Fast) {
     unsigned RegSizeInBytes = Subtarget.getGPRSizeInBytes();
     ArrayRef<MCPhysReg> IntArgRegs = ABI.GetByValArgRegs();
     // FIXME: The O32 case actually describes no shadow registers.
     const MCPhysReg *ShadowRegs =
         ABI.IsO32() ? IntArgRegs.data() : Mips64DPRegs;
 
     // We used to check the size as well but we can't do that anymore since
     // CCState::HandleByVal() rounds up the size after calling this function.
     assert(
         Alignment >= Align(RegSizeInBytes) &&
         "Byval argument's alignment should be a multiple of RegSizeInBytes.");
 
     FirstReg = State->getFirstUnallocated(IntArgRegs);
 
     // If Alignment > RegSizeInBytes, the first arg register must be even.
     // FIXME: This condition happens to do the right thing but it's not the
     //        right way to test it. We want to check that the stack frame offset
     //        of the register is aligned.
     if ((Alignment > RegSizeInBytes) && (FirstReg % 2)) {
       State->AllocateReg(IntArgRegs[FirstReg], ShadowRegs[FirstReg]);
       ++FirstReg;
     }
 
     // Mark the registers allocated.
     Size = alignTo(Size, RegSizeInBytes);
     for (unsigned I = FirstReg; Size > 0 && (I < IntArgRegs.size());
          Size -= RegSizeInBytes, ++I, ++NumRegs)
       State->AllocateReg(IntArgRegs[I], ShadowRegs[I]);
   }
 
   State->addInRegsParamInfo(FirstReg, FirstReg + NumRegs);
 }
 
 MachineBasicBlock *MipsTargetLowering::emitPseudoSELECT(MachineInstr &MI,
                                                         MachineBasicBlock *BB,
                                                         bool isFPCmp,
                                                         unsigned Opc) const {
   assert(!(Subtarget.hasMips4() || Subtarget.hasMips32()) &&
          "Subtarget already supports SELECT nodes with the use of"
          "conditional-move instructions.");
 
   const TargetInstrInfo *TII =
       Subtarget.getInstrInfo();
   DebugLoc DL = MI.getDebugLoc();
 
   // To "insert" a SELECT instruction, we actually have to insert the
   // diamond control-flow pattern.  The incoming instruction knows the
   // destination vreg to set, the condition code register to branch on, the
   // true/false values to select between, and a branch opcode to use.
   const BasicBlock *LLVM_BB = BB->getBasicBlock();
   MachineFunction::iterator It = ++BB->getIterator();
 
   //  thisMBB:
   //  ...
   //   TrueVal = ...
   //   setcc r1, r2, r3
   //   bNE   r1, r0, copy1MBB
   //   fallthrough --> copy0MBB
   MachineBasicBlock *thisMBB  = BB;
   MachineFunction *F = BB->getParent();
   MachineBasicBlock *copy0MBB = F->CreateMachineBasicBlock(LLVM_BB);
   MachineBasicBlock *sinkMBB  = F->CreateMachineBasicBlock(LLVM_BB);
   F->insert(It, copy0MBB);
   F->insert(It, sinkMBB);
 
   // Transfer the remainder of BB and its successor edges to sinkMBB.
   sinkMBB->splice(sinkMBB->begin(), BB,
                   std::next(MachineBasicBlock::iterator(MI)), BB->end());
   sinkMBB->transferSuccessorsAndUpdatePHIs(BB);
 
   // Next, add the true and fallthrough blocks as its successors.
   BB->addSuccessor(copy0MBB);
   BB->addSuccessor(sinkMBB);
 
   if (isFPCmp) {
     // bc1[tf] cc, sinkMBB
     BuildMI(BB, DL, TII->get(Opc))
         .addReg(MI.getOperand(1).getReg())
         .addMBB(sinkMBB);
   } else {
     // bne rs, $0, sinkMBB
     BuildMI(BB, DL, TII->get(Opc))
         .addReg(MI.getOperand(1).getReg())
         .addReg(Mips::ZERO)
         .addMBB(sinkMBB);
   }
 
   //  copy0MBB:
   //   %FalseValue = ...
   //   # fallthrough to sinkMBB
   BB = copy0MBB;
 
   // Update machine-CFG edges
   BB->addSuccessor(sinkMBB);
 
   //  sinkMBB:
   //   %Result = phi [ %TrueValue, thisMBB ], [ %FalseValue, copy0MBB ]
   //  ...
   BB = sinkMBB;
 
   BuildMI(*BB, BB->begin(), DL, TII->get(Mips::PHI), MI.getOperand(0).getReg())
       .addReg(MI.getOperand(2).getReg())
       .addMBB(thisMBB)
       .addReg(MI.getOperand(3).getReg())
       .addMBB(copy0MBB);
 
   MI.eraseFromParent(); // The pseudo instruction is gone now.
 
   return BB;
 }
 
 MachineBasicBlock *
 MipsTargetLowering::emitPseudoD_SELECT(MachineInstr &MI,
                                        MachineBasicBlock *BB) const {
   assert(!(Subtarget.hasMips4() || Subtarget.hasMips32()) &&
          "Subtarget already supports SELECT nodes with the use of"
          "conditional-move instructions.");
 
   const TargetInstrInfo *TII = Subtarget.getInstrInfo();
   DebugLoc DL = MI.getDebugLoc();
 
   // D_SELECT substitutes two SELECT nodes that goes one after another and
   // have the same condition operand. On machines which don't have
   // conditional-move instruction, it reduces unnecessary branch instructions
   // which are result of using two diamond patterns that are result of two
   // SELECT pseudo instructions.
   const BasicBlock *LLVM_BB = BB->getBasicBlock();
   MachineFunction::iterator It = ++BB->getIterator();
 
   //  thisMBB:
   //  ...
   //   TrueVal = ...
   //   setcc r1, r2, r3
   //   bNE   r1, r0, copy1MBB
   //   fallthrough --> copy0MBB
   MachineBasicBlock *thisMBB = BB;
   MachineFunction *F = BB->getParent();
   MachineBasicBlock *copy0MBB = F->CreateMachineBasicBlock(LLVM_BB);
   MachineBasicBlock *sinkMBB = F->CreateMachineBasicBlock(LLVM_BB);
   F->insert(It, copy0MBB);
   F->insert(It, sinkMBB);
 
   // Transfer the remainder of BB and its successor edges to sinkMBB.
   sinkMBB->splice(sinkMBB->begin(), BB,
                   std::next(MachineBasicBlock::iterator(MI)), BB->end());
   sinkMBB->transferSuccessorsAndUpdatePHIs(BB);
 
   // Next, add the true and fallthrough blocks as its successors.
   BB->addSuccessor(copy0MBB);
   BB->addSuccessor(sinkMBB);
 
   // bne rs, $0, sinkMBB
   BuildMI(BB, DL, TII->get(Mips::BNE))
       .addReg(MI.getOperand(2).getReg())
       .addReg(Mips::ZERO)
       .addMBB(sinkMBB);
 
   //  copy0MBB:
   //   %FalseValue = ...
   //   # fallthrough to sinkMBB
   BB = copy0MBB;
 
   // Update machine-CFG edges
   BB->addSuccessor(sinkMBB);
 
   //  sinkMBB:
   //   %Result = phi [ %TrueValue, thisMBB ], [ %FalseValue, copy0MBB ]
   //  ...
   BB = sinkMBB;
 
   // Use two PHI nodes to select two reults
   BuildMI(*BB, BB->begin(), DL, TII->get(Mips::PHI), MI.getOperand(0).getReg())
       .addReg(MI.getOperand(3).getReg())
       .addMBB(thisMBB)
       .addReg(MI.getOperand(5).getReg())
       .addMBB(copy0MBB);
   BuildMI(*BB, BB->begin(), DL, TII->get(Mips::PHI), MI.getOperand(1).getReg())
       .addReg(MI.getOperand(4).getReg())
       .addMBB(thisMBB)
       .addReg(MI.getOperand(6).getReg())
       .addMBB(copy0MBB);
 
   MI.eraseFromParent(); // The pseudo instruction is gone now.
 
   return BB;
 }
 
 // FIXME? Maybe this could be a TableGen attribute on some registers and
 // this table could be generated automatically from RegInfo.
 Register
 MipsTargetLowering::getRegisterByName(const char *RegName, LLT VT,
                                       const MachineFunction &MF) const {
   // Named registers is expected to be fairly rare. For now, just support $28
   // since the linux kernel uses it.
   if (Subtarget.isGP64bit()) {
     Register Reg = StringSwitch<Register>(RegName)
                          .Case("$28", Mips::GP_64)
                          .Default(Register());
     if (Reg)
       return Reg;
   } else {
     Register Reg = StringSwitch<Register>(RegName)
                          .Case("$28", Mips::GP)
                          .Default(Register());
     if (Reg)
       return Reg;
   }
   report_fatal_error("Invalid register name global variable");
 }
 
 MachineBasicBlock *MipsTargetLowering::emitLDR_W(MachineInstr &MI,
                                                  MachineBasicBlock *BB) const {
   MachineFunction *MF = BB->getParent();
   MachineRegisterInfo &MRI = MF->getRegInfo();
   const TargetInstrInfo *TII = Subtarget.getInstrInfo();
   const bool IsLittle = Subtarget.isLittle();
   DebugLoc DL = MI.getDebugLoc();
 
   Register Dest = MI.getOperand(0).getReg();
   Register Address = MI.getOperand(1).getReg();
   unsigned Imm = MI.getOperand(2).getImm();
 
   MachineBasicBlock::iterator I(MI);
 
   if (Subtarget.hasMips32r6() || Subtarget.hasMips64r6()) {
     // Mips release 6 can load from adress that is not naturally-aligned.
     Register Temp = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     BuildMI(*BB, I, DL, TII->get(Mips::LW))
         .addDef(Temp)
         .addUse(Address)
         .addImm(Imm);
     BuildMI(*BB, I, DL, TII->get(Mips::FILL_W)).addDef(Dest).addUse(Temp);
   } else {
     // Mips release 5 needs to use instructions that can load from an unaligned
     // memory address.
     Register LoadHalf = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     Register LoadFull = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     Register Undef = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     BuildMI(*BB, I, DL, TII->get(Mips::IMPLICIT_DEF)).addDef(Undef);
     BuildMI(*BB, I, DL, TII->get(Mips::LWR))
         .addDef(LoadHalf)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 0 : 3))
         .addUse(Undef);
     BuildMI(*BB, I, DL, TII->get(Mips::LWL))
         .addDef(LoadFull)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 3 : 0))
         .addUse(LoadHalf);
     BuildMI(*BB, I, DL, TII->get(Mips::FILL_W)).addDef(Dest).addUse(LoadFull);
   }
 
   MI.eraseFromParent();
   return BB;
 }
 
 MachineBasicBlock *MipsTargetLowering::emitLDR_D(MachineInstr &MI,
                                                  MachineBasicBlock *BB) const {
   MachineFunction *MF = BB->getParent();
   MachineRegisterInfo &MRI = MF->getRegInfo();
   const TargetInstrInfo *TII = Subtarget.getInstrInfo();
   const bool IsLittle = Subtarget.isLittle();
   DebugLoc DL = MI.getDebugLoc();
 
   Register Dest = MI.getOperand(0).getReg();
   Register Address = MI.getOperand(1).getReg();
   unsigned Imm = MI.getOperand(2).getImm();
 
   MachineBasicBlock::iterator I(MI);
 
   if (Subtarget.hasMips32r6() || Subtarget.hasMips64r6()) {
     // Mips release 6 can load from adress that is not naturally-aligned.
     if (Subtarget.isGP64bit()) {
       Register Temp = MRI.createVirtualRegister(&Mips::GPR64RegClass);
       BuildMI(*BB, I, DL, TII->get(Mips::LD))
           .addDef(Temp)
           .addUse(Address)
           .addImm(Imm);
       BuildMI(*BB, I, DL, TII->get(Mips::FILL_D)).addDef(Dest).addUse(Temp);
     } else {
       Register Wtemp = MRI.createVirtualRegister(&Mips::MSA128WRegClass);
       Register Lo = MRI.createVirtualRegister(&Mips::GPR32RegClass);
       Register Hi = MRI.createVirtualRegister(&Mips::GPR32RegClass);
       BuildMI(*BB, I, DL, TII->get(Mips::LW))
           .addDef(Lo)
           .addUse(Address)
           .addImm(Imm + (IsLittle ? 0 : 4));
       BuildMI(*BB, I, DL, TII->get(Mips::LW))
           .addDef(Hi)
           .addUse(Address)
           .addImm(Imm + (IsLittle ? 4 : 0));
       BuildMI(*BB, I, DL, TII->get(Mips::FILL_W)).addDef(Wtemp).addUse(Lo);
       BuildMI(*BB, I, DL, TII->get(Mips::INSERT_W), Dest)
           .addUse(Wtemp)
           .addUse(Hi)
           .addImm(1);
     }
   } else {
     // Mips release 5 needs to use instructions that can load from an unaligned
     // memory address.
     Register LoHalf = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     Register LoFull = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     Register LoUndef = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     Register HiHalf = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     Register HiFull = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     Register HiUndef = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     Register Wtemp = MRI.createVirtualRegister(&Mips::MSA128WRegClass);
     BuildMI(*BB, I, DL, TII->get(Mips::IMPLICIT_DEF)).addDef(LoUndef);
     BuildMI(*BB, I, DL, TII->get(Mips::LWR))
         .addDef(LoHalf)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 0 : 7))
         .addUse(LoUndef);
     BuildMI(*BB, I, DL, TII->get(Mips::LWL))
         .addDef(LoFull)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 3 : 4))
         .addUse(LoHalf);
     BuildMI(*BB, I, DL, TII->get(Mips::IMPLICIT_DEF)).addDef(HiUndef);
     BuildMI(*BB, I, DL, TII->get(Mips::LWR))
         .addDef(HiHalf)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 4 : 3))
         .addUse(HiUndef);
     BuildMI(*BB, I, DL, TII->get(Mips::LWL))
         .addDef(HiFull)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 7 : 0))
         .addUse(HiHalf);
     BuildMI(*BB, I, DL, TII->get(Mips::FILL_W)).addDef(Wtemp).addUse(LoFull);
     BuildMI(*BB, I, DL, TII->get(Mips::INSERT_W), Dest)
         .addUse(Wtemp)
         .addUse(HiFull)
         .addImm(1);
   }
 
   MI.eraseFromParent();
   return BB;
 }
 
 MachineBasicBlock *MipsTargetLowering::emitSTR_W(MachineInstr &MI,
                                                  MachineBasicBlock *BB) const {
   MachineFunction *MF = BB->getParent();
   MachineRegisterInfo &MRI = MF->getRegInfo();
   const TargetInstrInfo *TII = Subtarget.getInstrInfo();
   const bool IsLittle = Subtarget.isLittle();
   DebugLoc DL = MI.getDebugLoc();
 
   Register StoreVal = MI.getOperand(0).getReg();
   Register Address = MI.getOperand(1).getReg();
   unsigned Imm = MI.getOperand(2).getImm();
 
   MachineBasicBlock::iterator I(MI);
 
   if (Subtarget.hasMips32r6() || Subtarget.hasMips64r6()) {
     // Mips release 6 can store to adress that is not naturally-aligned.
     Register BitcastW = MRI.createVirtualRegister(&Mips::MSA128WRegClass);
     Register Tmp = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     BuildMI(*BB, I, DL, TII->get(Mips::COPY)).addDef(BitcastW).addUse(StoreVal);
     BuildMI(*BB, I, DL, TII->get(Mips::COPY_S_W))
         .addDef(Tmp)
         .addUse(BitcastW)
         .addImm(0);
     BuildMI(*BB, I, DL, TII->get(Mips::SW))
         .addUse(Tmp)
         .addUse(Address)
         .addImm(Imm);
   } else {
     // Mips release 5 needs to use instructions that can store to an unaligned
     // memory address.
     Register Tmp = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     BuildMI(*BB, I, DL, TII->get(Mips::COPY_S_W))
         .addDef(Tmp)
         .addUse(StoreVal)
         .addImm(0);
     BuildMI(*BB, I, DL, TII->get(Mips::SWR))
         .addUse(Tmp)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 0 : 3));
     BuildMI(*BB, I, DL, TII->get(Mips::SWL))
         .addUse(Tmp)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 3 : 0));
   }
 
   MI.eraseFromParent();
 
   return BB;
 }
 
 MachineBasicBlock *MipsTargetLowering::emitSTR_D(MachineInstr &MI,
                                                  MachineBasicBlock *BB) const {
   MachineFunction *MF = BB->getParent();
   MachineRegisterInfo &MRI = MF->getRegInfo();
   const TargetInstrInfo *TII = Subtarget.getInstrInfo();
   const bool IsLittle = Subtarget.isLittle();
   DebugLoc DL = MI.getDebugLoc();
 
   Register StoreVal = MI.getOperand(0).getReg();
   Register Address = MI.getOperand(1).getReg();
   unsigned Imm = MI.getOperand(2).getImm();
 
   MachineBasicBlock::iterator I(MI);
 
   if (Subtarget.hasMips32r6() || Subtarget.hasMips64r6()) {
     // Mips release 6 can store to adress that is not naturally-aligned.
     if (Subtarget.isGP64bit()) {
       Register BitcastD = MRI.createVirtualRegister(&Mips::MSA128DRegClass);
       Register Lo = MRI.createVirtualRegister(&Mips::GPR64RegClass);
       BuildMI(*BB, I, DL, TII->get(Mips::COPY))
           .addDef(BitcastD)
           .addUse(StoreVal);
       BuildMI(*BB, I, DL, TII->get(Mips::COPY_S_D))
           .addDef(Lo)
           .addUse(BitcastD)
           .addImm(0);
       BuildMI(*BB, I, DL, TII->get(Mips::SD))
           .addUse(Lo)
           .addUse(Address)
           .addImm(Imm);
     } else {
       Register BitcastW = MRI.createVirtualRegister(&Mips::MSA128WRegClass);
       Register Lo = MRI.createVirtualRegister(&Mips::GPR32RegClass);
       Register Hi = MRI.createVirtualRegister(&Mips::GPR32RegClass);
       BuildMI(*BB, I, DL, TII->get(Mips::COPY))
           .addDef(BitcastW)
           .addUse(StoreVal);
       BuildMI(*BB, I, DL, TII->get(Mips::COPY_S_W))
           .addDef(Lo)
           .addUse(BitcastW)
           .addImm(0);
       BuildMI(*BB, I, DL, TII->get(Mips::COPY_S_W))
           .addDef(Hi)
           .addUse(BitcastW)
           .addImm(1);
       BuildMI(*BB, I, DL, TII->get(Mips::SW))
           .addUse(Lo)
           .addUse(Address)
           .addImm(Imm + (IsLittle ? 0 : 4));
       BuildMI(*BB, I, DL, TII->get(Mips::SW))
           .addUse(Hi)
           .addUse(Address)
           .addImm(Imm + (IsLittle ? 4 : 0));
     }
   } else {
     // Mips release 5 needs to use instructions that can store to an unaligned
     // memory address.
     Register Bitcast = MRI.createVirtualRegister(&Mips::MSA128WRegClass);
     Register Lo = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     Register Hi = MRI.createVirtualRegister(&Mips::GPR32RegClass);
     BuildMI(*BB, I, DL, TII->get(Mips::COPY)).addDef(Bitcast).addUse(StoreVal);
     BuildMI(*BB, I, DL, TII->get(Mips::COPY_S_W))
         .addDef(Lo)
         .addUse(Bitcast)
         .addImm(0);
     BuildMI(*BB, I, DL, TII->get(Mips::COPY_S_W))
         .addDef(Hi)
         .addUse(Bitcast)
         .addImm(1);
     BuildMI(*BB, I, DL, TII->get(Mips::SWR))
         .addUse(Lo)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 0 : 3));
     BuildMI(*BB, I, DL, TII->get(Mips::SWL))
         .addUse(Lo)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 3 : 0));
     BuildMI(*BB, I, DL, TII->get(Mips::SWR))
         .addUse(Hi)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 4 : 7));
     BuildMI(*BB, I, DL, TII->get(Mips::SWL))
         .addUse(Hi)
         .addUse(Address)
         .addImm(Imm + (IsLittle ? 7 : 4));
   }
 
   MI.eraseFromParent();
   return BB;
 }
diff --git a/llvm/lib/Target/Mips/MipsISelLowering.h b/llvm/lib/Target/Mips/MipsISelLowering.h
index 3820c42ba8aa..a2ee940c1dcf 100644
--- a/llvm/lib/Target/Mips/MipsISelLowering.h
+++ b/llvm/lib/Target/Mips/MipsISelLowering.h
@@ -1,718 +1,720 @@
 //===- MipsISelLowering.h - Mips DAG Lowering Interface ---------*- C++ -*-===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
 //
 // This file defines the interfaces that Mips uses to lower LLVM code into a
 // selection DAG.
 //
 //===----------------------------------------------------------------------===//
 
 #ifndef LLVM_LIB_TARGET_MIPS_MIPSISELLOWERING_H
 #define LLVM_LIB_TARGET_MIPS_MIPSISELLOWERING_H
 
 #include "MCTargetDesc/MipsABIInfo.h"
 #include "MCTargetDesc/MipsBaseInfo.h"
 #include "MCTargetDesc/MipsMCTargetDesc.h"
 #include "Mips.h"
 #include "llvm/CodeGen/CallingConvLower.h"
 #include "llvm/CodeGen/ISDOpcodes.h"
 #include "llvm/CodeGen/MachineMemOperand.h"
 #include "llvm/CodeGen/SelectionDAG.h"
 #include "llvm/CodeGen/SelectionDAGNodes.h"
 #include "llvm/CodeGen/TargetLowering.h"
 #include "llvm/CodeGen/ValueTypes.h"
 #include "llvm/IR/CallingConv.h"
 #include "llvm/IR/InlineAsm.h"
 #include "llvm/IR/Type.h"
 #include "llvm/Support/MachineValueType.h"
 #include "llvm/Target/TargetMachine.h"
 #include <algorithm>
 #include <cassert>
 #include <deque>
 #include <string>
 #include <utility>
 #include <vector>
 
 namespace llvm {
 
 class Argument;
 class FastISel;
 class FunctionLoweringInfo;
 class MachineBasicBlock;
 class MachineFrameInfo;
 class MachineInstr;
 class MipsCCState;
 class MipsFunctionInfo;
 class MipsSubtarget;
 class MipsTargetMachine;
 class TargetLibraryInfo;
 class TargetRegisterClass;
 
   namespace MipsISD {
 
     enum NodeType : unsigned {
       // Start the numbering from where ISD NodeType finishes.
       FIRST_NUMBER = ISD::BUILTIN_OP_END,
 
       // Jump and link (call)
       JmpLink,
 
       // Tail call
       TailCall,
 
       // Get the Highest (63-48) 16 bits from a 64-bit immediate
       Highest,
 
       // Get the Higher (47-32) 16 bits from a 64-bit immediate
       Higher,
 
       // Get the High 16 bits from a 32/64-bit immediate
       // No relation with Mips Hi register
       Hi,
 
       // Get the Lower 16 bits from a 32/64-bit immediate
       // No relation with Mips Lo register
       Lo,
 
       // Get the High 16 bits from a 32 bit immediate for accessing the GOT.
       GotHi,
 
       // Get the High 16 bits from a 32-bit immediate for accessing TLS.
       TlsHi,
 
       // Handle gp_rel (small data/bss sections) relocation.
       GPRel,
 
       // Thread Pointer
       ThreadPointer,
 
       // Vector Floating Point Multiply and Subtract
       FMS,
 
       // Floating Point Branch Conditional
       FPBrcond,
 
       // Floating Point Compare
       FPCmp,
 
       // Floating point select
       FSELECT,
 
       // Node used to generate an MTC1 i32 to f64 instruction
       MTC1_D64,
 
       // Floating Point Conditional Moves
       CMovFP_T,
       CMovFP_F,
 
       // FP-to-int truncation node.
       TruncIntFP,
 
       // Return
       Ret,
 
       // Interrupt, exception, error trap Return
       ERet,
 
       // Software Exception Return.
       EH_RETURN,
 
       // Node used to extract integer from accumulator.
       MFHI,
       MFLO,
 
       // Node used to insert integers to accumulator.
       MTLOHI,
 
       // Mult nodes.
       Mult,
       Multu,
+      Mult16,
+      Multu16,
 
       // MAdd/Sub nodes
       MAdd,
       MAddu,
       MSub,
       MSubu,
 
       // DivRem(u)
       DivRem,
       DivRemU,
       DivRem16,
       DivRemU16,
 
       BuildPairF64,
       ExtractElementF64,
 
       Wrapper,
 
       DynAlloc,
 
       Sync,
 
       Ext,
       Ins,
       CIns,
 
       // EXTR.W instrinsic nodes.
       EXTP,
       EXTPDP,
       EXTR_S_H,
       EXTR_W,
       EXTR_R_W,
       EXTR_RS_W,
       SHILO,
       MTHLIP,
 
       // DPA.W intrinsic nodes.
       MULSAQ_S_W_PH,
       MAQ_S_W_PHL,
       MAQ_S_W_PHR,
       MAQ_SA_W_PHL,
       MAQ_SA_W_PHR,
       DPAU_H_QBL,
       DPAU_H_QBR,
       DPSU_H_QBL,
       DPSU_H_QBR,
       DPAQ_S_W_PH,
       DPSQ_S_W_PH,
       DPAQ_SA_L_W,
       DPSQ_SA_L_W,
       DPA_W_PH,
       DPS_W_PH,
       DPAQX_S_W_PH,
       DPAQX_SA_W_PH,
       DPAX_W_PH,
       DPSX_W_PH,
       DPSQX_S_W_PH,
       DPSQX_SA_W_PH,
       MULSA_W_PH,
 
       MULT,
       MULTU,
       MADD_DSP,
       MADDU_DSP,
       MSUB_DSP,
       MSUBU_DSP,
 
       // DSP shift nodes.
       SHLL_DSP,
       SHRA_DSP,
       SHRL_DSP,
 
       // DSP setcc and select_cc nodes.
       SETCC_DSP,
       SELECT_CC_DSP,
 
       // Vector comparisons.
       // These take a vector and return a boolean.
       VALL_ZERO,
       VANY_ZERO,
       VALL_NONZERO,
       VANY_NONZERO,
 
       // These take a vector and return a vector bitmask.
       VCEQ,
       VCLE_S,
       VCLE_U,
       VCLT_S,
       VCLT_U,
 
       // Vector Shuffle with mask as an operand
       VSHF,  // Generic shuffle
       SHF,   // 4-element set shuffle.
       ILVEV, // Interleave even elements
       ILVOD, // Interleave odd elements
       ILVL,  // Interleave left elements
       ILVR,  // Interleave right elements
       PCKEV, // Pack even elements
       PCKOD, // Pack odd elements
 
       // Vector Lane Copy
       INSVE, // Copy element from one vector to another
 
       // Combined (XOR (OR $a, $b), -1)
       VNOR,
 
       // Extended vector element extraction
       VEXTRACT_SEXT_ELT,
       VEXTRACT_ZEXT_ELT,
 
       // Load/Store Left/Right nodes.
       LWL = ISD::FIRST_TARGET_MEMORY_OPCODE,
       LWR,
       SWL,
       SWR,
       LDL,
       LDR,
       SDL,
       SDR
     };
 
   } // ene namespace MipsISD
 
   //===--------------------------------------------------------------------===//
   // TargetLowering Implementation
   //===--------------------------------------------------------------------===//
 
   class MipsTargetLowering : public TargetLowering  {
     bool isMicroMips;
 
   public:
     explicit MipsTargetLowering(const MipsTargetMachine &TM,
                                 const MipsSubtarget &STI);
 
     static const MipsTargetLowering *create(const MipsTargetMachine &TM,
                                             const MipsSubtarget &STI);
 
     /// createFastISel - This method returns a target specific FastISel object,
     /// or null if the target does not support "fast" ISel.
     FastISel *createFastISel(FunctionLoweringInfo &funcInfo,
                              const TargetLibraryInfo *libInfo) const override;
 
     MVT getScalarShiftAmountTy(const DataLayout &, EVT) const override {
       return MVT::i32;
     }
 
     EVT getTypeForExtReturn(LLVMContext &Context, EVT VT,
                             ISD::NodeType) const override;
 
     bool isCheapToSpeculateCttz() const override;
     bool isCheapToSpeculateCtlz() const override;
     bool shouldFoldConstantShiftPairToMask(const SDNode *N,
                                            CombineLevel Level) const override;
 
     /// Return the register type for a given MVT, ensuring vectors are treated
     /// as a series of gpr sized integers.
     MVT getRegisterTypeForCallingConv(LLVMContext &Context, CallingConv::ID CC,
                                       EVT VT) const override;
 
     /// Return the number of registers for a given MVT, ensuring vectors are
     /// treated as a series of gpr sized integers.
     unsigned getNumRegistersForCallingConv(LLVMContext &Context,
                                            CallingConv::ID CC,
                                            EVT VT) const override;
 
     /// Break down vectors to the correct number of gpr sized integers.
     unsigned getVectorTypeBreakdownForCallingConv(
         LLVMContext &Context, CallingConv::ID CC, EVT VT, EVT &IntermediateVT,
         unsigned &NumIntermediates, MVT &RegisterVT) const override;
 
     /// Return the correct alignment for the current calling convention.
     Align getABIAlignmentForCallingConv(Type *ArgTy,
                                         DataLayout DL) const override {
       const Align ABIAlign = DL.getABITypeAlign(ArgTy);
       if (ArgTy->isVectorTy())
         return std::min(ABIAlign, Align(8));
       return ABIAlign;
     }
 
     ISD::NodeType getExtendForAtomicOps() const override {
       return ISD::SIGN_EXTEND;
     }
 
     /// LowerOperation - Provide custom lowering hooks for some operations.
     SDValue LowerOperation(SDValue Op, SelectionDAG &DAG) const override;
 
     /// ReplaceNodeResults - Replace the results of node with an illegal result
     /// type with new values built out of custom code.
     ///
     void ReplaceNodeResults(SDNode *N, SmallVectorImpl<SDValue>&Results,
                             SelectionDAG &DAG) const override;
 
     /// getTargetNodeName - This method returns the name of a target specific
     //  DAG node.
     const char *getTargetNodeName(unsigned Opcode) const override;
 
     /// getSetCCResultType - get the ISD::SETCC result ValueType
     EVT getSetCCResultType(const DataLayout &DL, LLVMContext &Context,
                            EVT VT) const override;
 
     SDValue PerformDAGCombine(SDNode *N, DAGCombinerInfo &DCI) const override;
 
     MachineBasicBlock *
     EmitInstrWithCustomInserter(MachineInstr &MI,
                                 MachineBasicBlock *MBB) const override;
 
     void AdjustInstrPostInstrSelection(MachineInstr &MI,
                                        SDNode *Node) const override;
 
     void HandleByVal(CCState *, unsigned &, Align) const override;
 
     Register getRegisterByName(const char* RegName, LLT VT,
                                const MachineFunction &MF) const override;
 
     /// If a physical register, this returns the register that receives the
     /// exception address on entry to an EH pad.
     Register
     getExceptionPointerRegister(const Constant *PersonalityFn) const override {
       return ABI.IsN64() ? Mips::A0_64 : Mips::A0;
     }
 
     /// If a physical register, this returns the register that receives the
     /// exception typeid on entry to a landing pad.
     Register
     getExceptionSelectorRegister(const Constant *PersonalityFn) const override {
       return ABI.IsN64() ? Mips::A1_64 : Mips::A1;
     }
 
     bool isJumpTableRelative() const override {
       return getTargetMachine().isPositionIndependent();
     }
 
    CCAssignFn *CCAssignFnForCall() const;
 
    CCAssignFn *CCAssignFnForReturn() const;
 
   protected:
     SDValue getGlobalReg(SelectionDAG &DAG, EVT Ty) const;
 
     // This method creates the following nodes, which are necessary for
     // computing a local symbol's address:
     //
     // (add (load (wrapper $gp, %got(sym)), %lo(sym))
     template <class NodeTy>
     SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty, SelectionDAG &DAG,
                          bool IsN32OrN64) const {
       unsigned GOTFlag = IsN32OrN64 ? MipsII::MO_GOT_PAGE : MipsII::MO_GOT;
       SDValue GOT = DAG.getNode(MipsISD::Wrapper, DL, Ty, getGlobalReg(DAG, Ty),
                                 getTargetNode(N, Ty, DAG, GOTFlag));
       SDValue Load =
           DAG.getLoad(Ty, DL, DAG.getEntryNode(), GOT,
                       MachinePointerInfo::getGOT(DAG.getMachineFunction()));
       unsigned LoFlag = IsN32OrN64 ? MipsII::MO_GOT_OFST : MipsII::MO_ABS_LO;
       SDValue Lo = DAG.getNode(MipsISD::Lo, DL, Ty,
                                getTargetNode(N, Ty, DAG, LoFlag));
       return DAG.getNode(ISD::ADD, DL, Ty, Load, Lo);
     }
 
     // This method creates the following nodes, which are necessary for
     // computing a global symbol's address:
     //
     // (load (wrapper $gp, %got(sym)))
     template <class NodeTy>
     SDValue getAddrGlobal(NodeTy *N, const SDLoc &DL, EVT Ty, SelectionDAG &DAG,
                           unsigned Flag, SDValue Chain,
                           const MachinePointerInfo &PtrInfo) const {
       SDValue Tgt = DAG.getNode(MipsISD::Wrapper, DL, Ty, getGlobalReg(DAG, Ty),
                                 getTargetNode(N, Ty, DAG, Flag));
       return DAG.getLoad(Ty, DL, Chain, Tgt, PtrInfo);
     }
 
     // This method creates the following nodes, which are necessary for
     // computing a global symbol's address in large-GOT mode:
     //
     // (load (wrapper (add %hi(sym), $gp), %lo(sym)))
     template <class NodeTy>
     SDValue getAddrGlobalLargeGOT(NodeTy *N, const SDLoc &DL, EVT Ty,
                                   SelectionDAG &DAG, unsigned HiFlag,
                                   unsigned LoFlag, SDValue Chain,
                                   const MachinePointerInfo &PtrInfo) const {
       SDValue Hi = DAG.getNode(MipsISD::GotHi, DL, Ty,
                                getTargetNode(N, Ty, DAG, HiFlag));
       Hi = DAG.getNode(ISD::ADD, DL, Ty, Hi, getGlobalReg(DAG, Ty));
       SDValue Wrapper = DAG.getNode(MipsISD::Wrapper, DL, Ty, Hi,
                                     getTargetNode(N, Ty, DAG, LoFlag));
       return DAG.getLoad(Ty, DL, Chain, Wrapper, PtrInfo);
     }
 
     // This method creates the following nodes, which are necessary for
     // computing a symbol's address in non-PIC mode:
     //
     // (add %hi(sym), %lo(sym))
     //
     // This method covers O32, N32 and N64 in sym32 mode.
     template <class NodeTy>
     SDValue getAddrNonPIC(NodeTy *N, const SDLoc &DL, EVT Ty,
                           SelectionDAG &DAG) const {
       SDValue Hi = getTargetNode(N, Ty, DAG, MipsII::MO_ABS_HI);
       SDValue Lo = getTargetNode(N, Ty, DAG, MipsII::MO_ABS_LO);
       return DAG.getNode(ISD::ADD, DL, Ty,
                          DAG.getNode(MipsISD::Hi, DL, Ty, Hi),
                          DAG.getNode(MipsISD::Lo, DL, Ty, Lo));
    }
 
    // This method creates the following nodes, which are necessary for
    // computing a symbol's address in non-PIC mode for N64.
    //
    // (add (shl (add (shl (add %highest(sym), %higher(sim)), 16), %high(sym)),
    //            16), %lo(%sym))
    //
    // FIXME: This method is not efficent for (micro)MIPS64R6.
    template <class NodeTy>
    SDValue getAddrNonPICSym64(NodeTy *N, const SDLoc &DL, EVT Ty,
                           SelectionDAG &DAG) const {
       SDValue Hi = getTargetNode(N, Ty, DAG, MipsII::MO_ABS_HI);
       SDValue Lo = getTargetNode(N, Ty, DAG, MipsII::MO_ABS_LO);
 
       SDValue Highest =
           DAG.getNode(MipsISD::Highest, DL, Ty,
                       getTargetNode(N, Ty, DAG, MipsII::MO_HIGHEST));
       SDValue Higher = getTargetNode(N, Ty, DAG, MipsII::MO_HIGHER);
       SDValue HigherPart =
           DAG.getNode(ISD::ADD, DL, Ty, Highest,
                       DAG.getNode(MipsISD::Higher, DL, Ty, Higher));
       SDValue Cst = DAG.getConstant(16, DL, MVT::i32);
       SDValue Shift = DAG.getNode(ISD::SHL, DL, Ty, HigherPart, Cst);
       SDValue Add = DAG.getNode(ISD::ADD, DL, Ty, Shift,
                                 DAG.getNode(MipsISD::Hi, DL, Ty, Hi));
       SDValue Shift2 = DAG.getNode(ISD::SHL, DL, Ty, Add, Cst);
 
       return DAG.getNode(ISD::ADD, DL, Ty, Shift2,
                          DAG.getNode(MipsISD::Lo, DL, Ty, Lo));
    }
 
     // This method creates the following nodes, which are necessary for
     // computing a symbol's address using gp-relative addressing:
     //
     // (add $gp, %gp_rel(sym))
     template <class NodeTy>
     SDValue getAddrGPRel(NodeTy *N, const SDLoc &DL, EVT Ty,
                          SelectionDAG &DAG, bool IsN64) const {
       SDValue GPRel = getTargetNode(N, Ty, DAG, MipsII::MO_GPREL);
       return DAG.getNode(
           ISD::ADD, DL, Ty,
           DAG.getRegister(IsN64 ? Mips::GP_64 : Mips::GP, Ty),
           DAG.getNode(MipsISD::GPRel, DL, DAG.getVTList(Ty), GPRel));
     }
 
     /// This function fills Ops, which is the list of operands that will later
     /// be used when a function call node is created. It also generates
     /// copyToReg nodes to set up argument registers.
     virtual void
     getOpndList(SmallVectorImpl<SDValue> &Ops,
                 std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
                 bool IsPICCall, bool GlobalOrExternal, bool InternalLinkage,
                 bool IsCallReloc, CallLoweringInfo &CLI, SDValue Callee,
                 SDValue Chain) const;
 
   protected:
     SDValue lowerLOAD(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerSTORE(SDValue Op, SelectionDAG &DAG) const;
 
     // Subtarget Info
     const MipsSubtarget &Subtarget;
     // Cache the ABI from the TargetMachine, we use it everywhere.
     const MipsABIInfo &ABI;
 
   private:
     // Create a TargetGlobalAddress node.
     SDValue getTargetNode(GlobalAddressSDNode *N, EVT Ty, SelectionDAG &DAG,
                           unsigned Flag) const;
 
     // Create a TargetExternalSymbol node.
     SDValue getTargetNode(ExternalSymbolSDNode *N, EVT Ty, SelectionDAG &DAG,
                           unsigned Flag) const;
 
     // Create a TargetBlockAddress node.
     SDValue getTargetNode(BlockAddressSDNode *N, EVT Ty, SelectionDAG &DAG,
                           unsigned Flag) const;
 
     // Create a TargetJumpTable node.
     SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,
                           unsigned Flag) const;
 
     // Create a TargetConstantPool node.
     SDValue getTargetNode(ConstantPoolSDNode *N, EVT Ty, SelectionDAG &DAG,
                           unsigned Flag) const;
 
     // Lower Operand helpers
     SDValue LowerCallResult(SDValue Chain, SDValue InFlag,
                             CallingConv::ID CallConv, bool isVarArg,
                             const SmallVectorImpl<ISD::InputArg> &Ins,
                             const SDLoc &dl, SelectionDAG &DAG,
                             SmallVectorImpl<SDValue> &InVals,
                             TargetLowering::CallLoweringInfo &CLI) const;
 
     // Lower Operand specifics
     SDValue lowerBRCOND(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerConstantPool(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerGlobalAddress(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerBlockAddress(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerGlobalTLSAddress(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerJumpTable(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerSELECT(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerSETCC(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerVASTART(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerVAARG(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerFCOPYSIGN(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerFABS(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerRETURNADDR(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerEH_RETURN(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerATOMIC_FENCE(SDValue Op, SelectionDAG& DAG) const;
     SDValue lowerShiftLeftParts(SDValue Op, SelectionDAG& DAG) const;
     SDValue lowerShiftRightParts(SDValue Op, SelectionDAG& DAG,
                                  bool IsSRA) const;
     SDValue lowerEH_DWARF_CFA(SDValue Op, SelectionDAG &DAG) const;
     SDValue lowerFP_TO_SINT(SDValue Op, SelectionDAG &DAG) const;
 
     /// isEligibleForTailCallOptimization - Check whether the call is eligible
     /// for tail call optimization.
     virtual bool
     isEligibleForTailCallOptimization(const CCState &CCInfo,
                                       unsigned NextStackOffset,
                                       const MipsFunctionInfo &FI) const = 0;
 
     /// copyByValArg - Copy argument registers which were used to pass a byval
     /// argument to the stack. Create a stack frame object for the byval
     /// argument.
     void copyByValRegs(SDValue Chain, const SDLoc &DL,
                        std::vector<SDValue> &OutChains, SelectionDAG &DAG,
                        const ISD::ArgFlagsTy &Flags,
                        SmallVectorImpl<SDValue> &InVals,
                        const Argument *FuncArg, unsigned FirstReg,
                        unsigned LastReg, const CCValAssign &VA,
                        MipsCCState &State) const;
 
     /// passByValArg - Pass a byval argument in registers or on stack.
     void passByValArg(SDValue Chain, const SDLoc &DL,
                       std::deque<std::pair<unsigned, SDValue>> &RegsToPass,
                       SmallVectorImpl<SDValue> &MemOpChains, SDValue StackPtr,
                       MachineFrameInfo &MFI, SelectionDAG &DAG, SDValue Arg,
                       unsigned FirstReg, unsigned LastReg,
                       const ISD::ArgFlagsTy &Flags, bool isLittle,
                       const CCValAssign &VA) const;
 
     /// writeVarArgRegs - Write variable function arguments passed in registers
     /// to the stack. Also create a stack frame object for the first variable
     /// argument.
     void writeVarArgRegs(std::vector<SDValue> &OutChains, SDValue Chain,
                          const SDLoc &DL, SelectionDAG &DAG,
                          CCState &State) const;
 
     SDValue
     LowerFormalArguments(SDValue Chain, CallingConv::ID CallConv, bool isVarArg,
                          const SmallVectorImpl<ISD::InputArg> &Ins,
                          const SDLoc &dl, SelectionDAG &DAG,
                          SmallVectorImpl<SDValue> &InVals) const override;
 
     SDValue passArgOnStack(SDValue StackPtr, unsigned Offset, SDValue Chain,
                            SDValue Arg, const SDLoc &DL, bool IsTailCall,
                            SelectionDAG &DAG) const;
 
     SDValue LowerCall(TargetLowering::CallLoweringInfo &CLI,
                       SmallVectorImpl<SDValue> &InVals) const override;
 
     bool CanLowerReturn(CallingConv::ID CallConv, MachineFunction &MF,
                         bool isVarArg,
                         const SmallVectorImpl<ISD::OutputArg> &Outs,
                         LLVMContext &Context) const override;
 
     SDValue LowerReturn(SDValue Chain, CallingConv::ID CallConv, bool isVarArg,
                         const SmallVectorImpl<ISD::OutputArg> &Outs,
                         const SmallVectorImpl<SDValue> &OutVals,
                         const SDLoc &dl, SelectionDAG &DAG) const override;
 
     SDValue LowerInterruptReturn(SmallVectorImpl<SDValue> &RetOps,
                                  const SDLoc &DL, SelectionDAG &DAG) const;
 
     bool shouldSignExtendTypeInLibCall(EVT Type, bool IsSigned) const override;
 
     // Inline asm support
     ConstraintType getConstraintType(StringRef Constraint) const override;
 
     /// Examine constraint string and operand type and determine a weight value.
     /// The operand object must already have been set up with the operand type.
     ConstraintWeight getSingleConstraintMatchWeight(
       AsmOperandInfo &info, const char *constraint) const override;
 
     /// This function parses registers that appear in inline-asm constraints.
     /// It returns pair (0, 0) on failure.
     std::pair<unsigned, const TargetRegisterClass *>
     parseRegForInlineAsmConstraint(StringRef C, MVT VT) const;
 
     std::pair<unsigned, const TargetRegisterClass *>
     getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI,
                                  StringRef Constraint, MVT VT) const override;
 
     /// LowerAsmOperandForConstraint - Lower the specified operand into the Ops
     /// vector.  If it is invalid, don't add anything to Ops. If hasMemory is
     /// true it means one of the asm constraint of the inline asm instruction
     /// being processed is 'm'.
     void LowerAsmOperandForConstraint(SDValue Op,
                                       std::string &Constraint,
                                       std::vector<SDValue> &Ops,
                                       SelectionDAG &DAG) const override;
 
     unsigned
     getInlineAsmMemConstraint(StringRef ConstraintCode) const override {
       if (ConstraintCode == "o")
         return InlineAsm::Constraint_o;
       if (ConstraintCode == "R")
         return InlineAsm::Constraint_R;
       if (ConstraintCode == "ZC")
         return InlineAsm::Constraint_ZC;
       return TargetLowering::getInlineAsmMemConstraint(ConstraintCode);
     }
 
     bool isLegalAddressingMode(const DataLayout &DL, const AddrMode &AM,
                                Type *Ty, unsigned AS,
                                Instruction *I = nullptr) const override;
 
     bool isOffsetFoldingLegal(const GlobalAddressSDNode *GA) const override;
 
     EVT getOptimalMemOpType(const MemOp &Op,
                             const AttributeList &FuncAttributes) const override;
 
     /// isFPImmLegal - Returns true if the target can instruction select the
     /// specified FP immediate natively. If false, the legalizer will
     /// materialize the FP immediate as a load from a constant pool.
     bool isFPImmLegal(const APFloat &Imm, EVT VT,
                       bool ForCodeSize) const override;
 
     unsigned getJumpTableEncoding() const override;
     bool useSoftFloat() const override;
 
     bool shouldInsertFencesForAtomic(const Instruction *I) const override {
       return true;
     }
 
     /// Emit a sign-extension using sll/sra, seb, or seh appropriately.
     MachineBasicBlock *emitSignExtendToI32InReg(MachineInstr &MI,
                                                 MachineBasicBlock *BB,
                                                 unsigned Size, unsigned DstReg,
                                                 unsigned SrcRec) const;
 
     MachineBasicBlock *emitAtomicBinary(MachineInstr &MI,
                                         MachineBasicBlock *BB) const;
     MachineBasicBlock *emitAtomicBinaryPartword(MachineInstr &MI,
                                                 MachineBasicBlock *BB,
                                                 unsigned Size) const;
     MachineBasicBlock *emitAtomicCmpSwap(MachineInstr &MI,
                                          MachineBasicBlock *BB) const;
     MachineBasicBlock *emitAtomicCmpSwapPartword(MachineInstr &MI,
                                                  MachineBasicBlock *BB,
                                                  unsigned Size) const;
     MachineBasicBlock *emitSEL_D(MachineInstr &MI, MachineBasicBlock *BB) const;
     MachineBasicBlock *emitPseudoSELECT(MachineInstr &MI, MachineBasicBlock *BB,
                                         bool isFPCmp, unsigned Opc) const;
     MachineBasicBlock *emitPseudoD_SELECT(MachineInstr &MI,
                                           MachineBasicBlock *BB) const;
     MachineBasicBlock *emitLDR_W(MachineInstr &MI, MachineBasicBlock *BB) const;
     MachineBasicBlock *emitLDR_D(MachineInstr &MI, MachineBasicBlock *BB) const;
     MachineBasicBlock *emitSTR_W(MachineInstr &MI, MachineBasicBlock *BB) const;
     MachineBasicBlock *emitSTR_D(MachineInstr &MI, MachineBasicBlock *BB) const;
   };
 
   /// Create MipsTargetLowering objects.
   const MipsTargetLowering *
   createMips16TargetLowering(const MipsTargetMachine &TM,
                              const MipsSubtarget &STI);
   const MipsTargetLowering *
   createMipsSETargetLowering(const MipsTargetMachine &TM,
                              const MipsSubtarget &STI);
 
 namespace Mips {
 
 FastISel *createFastISel(FunctionLoweringInfo &funcInfo,
                          const TargetLibraryInfo *libInfo);
 
 } // end namespace Mips
 
 } // end namespace llvm
 
 #endif // LLVM_LIB_TARGET_MIPS_MIPSISELLOWERING_H
diff --git a/llvm/lib/Target/Mips/MipsInstrInfo.td b/llvm/lib/Target/Mips/MipsInstrInfo.td
index 089fed9ec0bf..da366e88c865 100644
--- a/llvm/lib/Target/Mips/MipsInstrInfo.td
+++ b/llvm/lib/Target/Mips/MipsInstrInfo.td
@@ -1,3408 +1,3410 @@
 //===- MipsInstrInfo.td - Target Description for Mips Target -*- tablegen -*-=//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
 //
 // This file contains the Mips implementation of the TargetInstrInfo class.
 //
 //===----------------------------------------------------------------------===//
 
 
 //===----------------------------------------------------------------------===//
 // Mips profiles and nodes
 //===----------------------------------------------------------------------===//
 
 def SDT_MipsJmpLink      : SDTypeProfile<0, 1, [SDTCisVT<0, iPTR>]>;
 def SDT_MipsCMov         : SDTypeProfile<1, 4, [SDTCisSameAs<0, 1>,
                                                 SDTCisSameAs<1, 2>,
                                                 SDTCisSameAs<3, 4>,
                                                 SDTCisInt<4>]>;
 def SDT_MipsCallSeqStart : SDCallSeqStart<[SDTCisVT<0, i32>, SDTCisVT<1, i32>]>;
 def SDT_MipsCallSeqEnd   : SDCallSeqEnd<[SDTCisVT<0, i32>, SDTCisVT<1, i32>]>;
 def SDT_MFLOHI : SDTypeProfile<1, 1, [SDTCisInt<0>, SDTCisVT<1, untyped>]>;
 def SDT_MTLOHI : SDTypeProfile<1, 2, [SDTCisVT<0, untyped>,
                                       SDTCisInt<1>, SDTCisSameAs<1, 2>]>;
 def SDT_MipsMultDiv : SDTypeProfile<1, 2, [SDTCisVT<0, untyped>, SDTCisInt<1>,
                                     SDTCisSameAs<1, 2>]>;
 def SDT_MipsMAddMSub : SDTypeProfile<1, 3,
                                      [SDTCisVT<0, untyped>, SDTCisSameAs<0, 3>,
                                       SDTCisVT<1, i32>, SDTCisSameAs<1, 2>]>;
-def SDT_MipsDivRem16 : SDTypeProfile<0, 2, [SDTCisInt<0>, SDTCisSameAs<0, 1>]>;
+def SDT_MipsMultDiv16 : SDTypeProfile<0, 2, [SDTCisInt<0>, SDTCisSameAs<0, 1>]>;
 
 def SDT_MipsThreadPointer : SDTypeProfile<1, 0, [SDTCisPtrTy<0>]>;
 
 def SDT_Sync             : SDTypeProfile<0, 1, [SDTCisVT<0, i32>]>;
 
 def SDT_Ext : SDTypeProfile<1, 3, [SDTCisInt<0>, SDTCisSameAs<0, 1>,
                                    SDTCisVT<2, i32>, SDTCisSameAs<2, 3>]>;
 def SDT_Ins : SDTypeProfile<1, 4, [SDTCisInt<0>, SDTCisSameAs<0, 1>,
                                    SDTCisVT<2, i32>, SDTCisSameAs<2, 3>,
                                    SDTCisSameAs<0, 4>]>;
 
 def SDTMipsLoadLR  : SDTypeProfile<1, 2,
                                    [SDTCisInt<0>, SDTCisPtrTy<1>,
                                     SDTCisSameAs<0, 2>]>;
 
 // Call
 def MipsJmpLink : SDNode<"MipsISD::JmpLink",SDT_MipsJmpLink,
                          [SDNPHasChain, SDNPOutGlue, SDNPOptInGlue,
                           SDNPVariadic]>;
 
 // Tail call
 def MipsTailCall : SDNode<"MipsISD::TailCall", SDT_MipsJmpLink,
                           [SDNPHasChain, SDNPOptInGlue, SDNPVariadic]>;
 
 // Hi and Lo nodes are used to handle global addresses. Used on
 // MipsISelLowering to lower stuff like GlobalAddress, ExternalSymbol
 // static model. (nothing to do with Mips Registers Hi and Lo)
 
 // Hi is the odd node out, on MIPS64 it can expand to either daddiu when
 // using static relocations with 64 bit symbols, or lui when using 32 bit
 // symbols.
 def MipsHigher : SDNode<"MipsISD::Higher", SDTIntUnaryOp>;
 def MipsHighest : SDNode<"MipsISD::Highest", SDTIntUnaryOp>;
 def MipsHi    : SDNode<"MipsISD::Hi", SDTIntUnaryOp>;
 def MipsLo    : SDNode<"MipsISD::Lo", SDTIntUnaryOp>;
 
 def MipsGPRel : SDNode<"MipsISD::GPRel", SDTIntUnaryOp>;
 
 // Hi node for accessing the GOT.
 def MipsGotHi : SDNode<"MipsISD::GotHi", SDTIntUnaryOp>;
 
 // Hi node for handling TLS offsets
 def MipsTlsHi   : SDNode<"MipsISD::TlsHi", SDTIntUnaryOp>;
 
 // Thread pointer
 def MipsThreadPointer: SDNode<"MipsISD::ThreadPointer", SDT_MipsThreadPointer>;
 
 // Return
 def MipsRet : SDNode<"MipsISD::Ret", SDTNone,
                      [SDNPHasChain, SDNPOptInGlue, SDNPVariadic]>;
 
 def MipsERet : SDNode<"MipsISD::ERet", SDTNone,
                       [SDNPHasChain, SDNPOptInGlue, SDNPSideEffect]>;
 
 // These are target-independent nodes, but have target-specific formats.
 def callseq_start : SDNode<"ISD::CALLSEQ_START", SDT_MipsCallSeqStart,
                            [SDNPHasChain, SDNPSideEffect, SDNPOutGlue]>;
 def callseq_end   : SDNode<"ISD::CALLSEQ_END", SDT_MipsCallSeqEnd,
                            [SDNPHasChain, SDNPSideEffect,
                             SDNPOptInGlue, SDNPOutGlue]>;
 
 // Nodes used to extract LO/HI registers.
 def MipsMFHI : SDNode<"MipsISD::MFHI", SDT_MFLOHI>;
 def MipsMFLO : SDNode<"MipsISD::MFLO", SDT_MFLOHI>;
 
 // Node used to insert 32-bit integers to LOHI register pair.
 def MipsMTLOHI : SDNode<"MipsISD::MTLOHI", SDT_MTLOHI>;
 
 // Mult nodes.
-def MipsMult  : SDNode<"MipsISD::Mult", SDT_MipsMultDiv>;
-def MipsMultu : SDNode<"MipsISD::Multu", SDT_MipsMultDiv>;
+def MipsMult    : SDNode<"MipsISD::Mult", SDT_MipsMultDiv>;
+def MipsMultu   : SDNode<"MipsISD::Multu", SDT_MipsMultDiv>;
+def MipsMult16  : SDNode<"MipsISD::Mult16", SDT_MipsMultDiv16>;
+def MipsMultu16 : SDNode<"MipsISD::Multu16", SDT_MipsMultDiv16>;
 
 // MAdd*/MSub* nodes
 def MipsMAdd  : SDNode<"MipsISD::MAdd", SDT_MipsMAddMSub>;
 def MipsMAddu : SDNode<"MipsISD::MAddu", SDT_MipsMAddMSub>;
 def MipsMSub  : SDNode<"MipsISD::MSub", SDT_MipsMAddMSub>;
 def MipsMSubu : SDNode<"MipsISD::MSubu", SDT_MipsMAddMSub>;
 
 // DivRem(u) nodes
 def MipsDivRem    : SDNode<"MipsISD::DivRem", SDT_MipsMultDiv>;
 def MipsDivRemU   : SDNode<"MipsISD::DivRemU", SDT_MipsMultDiv>;
-def MipsDivRem16  : SDNode<"MipsISD::DivRem16", SDT_MipsDivRem16,
+def MipsDivRem16  : SDNode<"MipsISD::DivRem16", SDT_MipsMultDiv16,
                            [SDNPOutGlue]>;
-def MipsDivRemU16 : SDNode<"MipsISD::DivRemU16", SDT_MipsDivRem16,
+def MipsDivRemU16 : SDNode<"MipsISD::DivRemU16", SDT_MipsMultDiv16,
                            [SDNPOutGlue]>;
 
 // Target constant nodes that are not part of any isel patterns and remain
 // unchanged can cause instructions with illegal operands to be emitted.
 // Wrapper node patterns give the instruction selector a chance to replace
 // target constant nodes that would otherwise remain unchanged with ADDiu
 // nodes. Without these wrapper node patterns, the following conditional move
 // instruction is emitted when function cmov2 in test/CodeGen/Mips/cmov.ll is
 // compiled:
 //  movn  %got(d)($gp), %got(c)($gp), $4
 // This instruction is illegal since movn can take only register operands.
 
 def MipsWrapper    : SDNode<"MipsISD::Wrapper", SDTIntBinOp>;
 
 def MipsSync : SDNode<"MipsISD::Sync", SDT_Sync, [SDNPHasChain,SDNPSideEffect]>;
 
 def MipsExt :  SDNode<"MipsISD::Ext", SDT_Ext>;
 def MipsIns :  SDNode<"MipsISD::Ins", SDT_Ins>;
 def MipsCIns : SDNode<"MipsISD::CIns", SDT_Ext>;
 
 def MipsLWL : SDNode<"MipsISD::LWL", SDTMipsLoadLR,
                      [SDNPHasChain, SDNPMayLoad, SDNPMemOperand]>;
 def MipsLWR : SDNode<"MipsISD::LWR", SDTMipsLoadLR,
                      [SDNPHasChain, SDNPMayLoad, SDNPMemOperand]>;
 def MipsSWL : SDNode<"MipsISD::SWL", SDTStore,
                      [SDNPHasChain, SDNPMayStore, SDNPMemOperand]>;
 def MipsSWR : SDNode<"MipsISD::SWR", SDTStore,
                      [SDNPHasChain, SDNPMayStore, SDNPMemOperand]>;
 def MipsLDL : SDNode<"MipsISD::LDL", SDTMipsLoadLR,
                      [SDNPHasChain, SDNPMayLoad, SDNPMemOperand]>;
 def MipsLDR : SDNode<"MipsISD::LDR", SDTMipsLoadLR,
                      [SDNPHasChain, SDNPMayLoad, SDNPMemOperand]>;
 def MipsSDL : SDNode<"MipsISD::SDL", SDTStore,
                      [SDNPHasChain, SDNPMayStore, SDNPMemOperand]>;
 def MipsSDR : SDNode<"MipsISD::SDR", SDTStore,
                      [SDNPHasChain, SDNPMayStore, SDNPMemOperand]>;
 
 //===----------------------------------------------------------------------===//
 // Mips Instruction Predicate Definitions.
 //===----------------------------------------------------------------------===//
 def HasMips2     :    Predicate<"Subtarget->hasMips2()">,
                       AssemblerPredicate<(all_of FeatureMips2)>;
 def HasMips3_32  :    Predicate<"Subtarget->hasMips3_32()">,
                       AssemblerPredicate<(all_of FeatureMips3_32)>;
 def HasMips3_32r2 :   Predicate<"Subtarget->hasMips3_32r2()">,
                       AssemblerPredicate<(all_of FeatureMips3_32r2)>;
 def HasMips3     :    Predicate<"Subtarget->hasMips3()">,
                       AssemblerPredicate<(all_of FeatureMips3)>;
 def NotMips3     :    Predicate<"!Subtarget->hasMips3()">,
                       AssemblerPredicate<(all_of (not FeatureMips3))>;
 def HasMips4_32  :    Predicate<"Subtarget->hasMips4_32()">,
                       AssemblerPredicate<(all_of FeatureMips4_32)>;
 def NotMips4_32  :    Predicate<"!Subtarget->hasMips4_32()">,
                       AssemblerPredicate<(all_of (not FeatureMips4_32))>;
 def HasMips4_32r2 :   Predicate<"Subtarget->hasMips4_32r2()">,
                       AssemblerPredicate<(all_of FeatureMips4_32r2)>;
 def HasMips5_32r2 :   Predicate<"Subtarget->hasMips5_32r2()">,
                       AssemblerPredicate<(all_of FeatureMips5_32r2)>;
 def HasMips32    :    Predicate<"Subtarget->hasMips32()">,
                       AssemblerPredicate<(all_of FeatureMips32)>;
 def HasMips32r2  :    Predicate<"Subtarget->hasMips32r2()">,
                       AssemblerPredicate<(all_of FeatureMips32r2)>;
 def HasMips32r5  :    Predicate<"Subtarget->hasMips32r5()">,
                       AssemblerPredicate<(all_of FeatureMips32r5)>;
 def HasMips32r6  :    Predicate<"Subtarget->hasMips32r6()">,
                       AssemblerPredicate<(all_of FeatureMips32r6)>;
 def NotMips32r6  :    Predicate<"!Subtarget->hasMips32r6()">,
                       AssemblerPredicate<(all_of (not FeatureMips32r6))>;
 def IsGP64bit    :    Predicate<"Subtarget->isGP64bit()">,
                       AssemblerPredicate<(all_of FeatureGP64Bit)>;
 def IsGP32bit    :    Predicate<"!Subtarget->isGP64bit()">,
                       AssemblerPredicate<(all_of (not FeatureGP64Bit))>;
 def IsPTR64bit    :   Predicate<"Subtarget->isABI_N64()">,
                       AssemblerPredicate<(all_of FeaturePTR64Bit)>;
 def IsPTR32bit    :   Predicate<"!Subtarget->isABI_N64()">,
                       AssemblerPredicate<(all_of (not FeaturePTR64Bit))>;
 def HasMips64    :    Predicate<"Subtarget->hasMips64()">,
                       AssemblerPredicate<(all_of FeatureMips64)>;
 def NotMips64    :    Predicate<"!Subtarget->hasMips64()">,
                       AssemblerPredicate<(all_of (not FeatureMips64))>;
 def HasMips64r2  :    Predicate<"Subtarget->hasMips64r2()">,
                       AssemblerPredicate<(all_of FeatureMips64r2)>;
 def HasMips64r5  :    Predicate<"Subtarget->hasMips64r5()">,
                       AssemblerPredicate<(all_of FeatureMips64r5)>;
 def HasMips64r6  :    Predicate<"Subtarget->hasMips64r6()">,
                       AssemblerPredicate<(all_of FeatureMips64r6)>;
 def NotMips64r6  :    Predicate<"!Subtarget->hasMips64r6()">,
                       AssemblerPredicate<(all_of (not FeatureMips64r6))>;
 def InMips16Mode :    Predicate<"Subtarget->inMips16Mode()">,
                       AssemblerPredicate<(all_of FeatureMips16)>;
 def NotInMips16Mode : Predicate<"!Subtarget->inMips16Mode()">,
                       AssemblerPredicate<(all_of (not FeatureMips16))>;
 def HasCnMips    :    Predicate<"Subtarget->hasCnMips()">,
                       AssemblerPredicate<(all_of FeatureCnMips)>;
 def NotCnMips    :    Predicate<"!Subtarget->hasCnMips()">,
                       AssemblerPredicate<(all_of (not FeatureCnMips))>;
 def HasCnMipsP   :    Predicate<"Subtarget->hasCnMipsP()">,
                       AssemblerPredicate<(all_of FeatureCnMipsP)>;
 def NotCnMipsP   :    Predicate<"!Subtarget->hasCnMipsP()">,
                       AssemblerPredicate<(all_of (not FeatureCnMipsP))>;
 def IsSym32     :     Predicate<"Subtarget->hasSym32()">,
                       AssemblerPredicate<(all_of FeatureSym32)>;
 def IsSym64     :     Predicate<"!Subtarget->hasSym32()">,
                       AssemblerPredicate<(all_of (not FeatureSym32))>;
 def IsN64       :     Predicate<"Subtarget->isABI_N64()">;
 def IsNotN64    :     Predicate<"!Subtarget->isABI_N64()">;
 def RelocNotPIC :     Predicate<"!TM.isPositionIndependent()">;
 def RelocPIC    :     Predicate<"TM.isPositionIndependent()">;
 def NoNaNsFPMath :    Predicate<"TM.Options.NoNaNsFPMath">;
 def UseAbs :          Predicate<"Subtarget->inAbs2008Mode() ||"
                                 "TM.Options.NoNaNsFPMath">;
 def HasStdEnc :       Predicate<"Subtarget->hasStandardEncoding()">,
                       AssemblerPredicate<(all_of (not FeatureMips16))>;
 def NotDSP :          Predicate<"!Subtarget->hasDSP()">;
 def InMicroMips    :  Predicate<"Subtarget->inMicroMipsMode()">,
                       AssemblerPredicate<(all_of FeatureMicroMips)>;
 def NotInMicroMips :  Predicate<"!Subtarget->inMicroMipsMode()">,
                       AssemblerPredicate<(all_of (not FeatureMicroMips))>;
 def IsLE           :  Predicate<"Subtarget->isLittle()">;
 def IsBE           :  Predicate<"!Subtarget->isLittle()">;
 def IsNotNaCl    :    Predicate<"!Subtarget->isTargetNaCl()">;
 def UseTCCInDIV    :  AssemblerPredicate<(all_of FeatureUseTCCInDIV)>;
 def HasEVA       :    Predicate<"Subtarget->hasEVA()">,
                       AssemblerPredicate<(all_of FeatureEVA)>;
 def HasMSA : Predicate<"Subtarget->hasMSA()">,
              AssemblerPredicate<(all_of FeatureMSA)>;
 def HasMadd4 : Predicate<"!Subtarget->disableMadd4()">,
                AssemblerPredicate<(all_of (not FeatureNoMadd4))>;
 def HasMT  : Predicate<"Subtarget->hasMT()">,
              AssemblerPredicate<(all_of FeatureMT)>;
 def UseIndirectJumpsHazard : Predicate<"Subtarget->useIndirectJumpsHazard()">,
                             AssemblerPredicate<(all_of FeatureUseIndirectJumpsHazard)>;
 def NoIndirectJumpGuards : Predicate<"!Subtarget->useIndirectJumpsHazard()">,
                            AssemblerPredicate<(all_of (not FeatureUseIndirectJumpsHazard))>;
 def HasCRC   : Predicate<"Subtarget->hasCRC()">,
                AssemblerPredicate<(all_of FeatureCRC)>;
 def HasVirt  : Predicate<"Subtarget->hasVirt()">,
                AssemblerPredicate<(all_of FeatureVirt)>;
 def HasGINV  : Predicate<"Subtarget->hasGINV()">,
                AssemblerPredicate<(all_of FeatureGINV)>;
 // TODO: Add support for FPOpFusion::Standard
 def AllowFPOpFusion : Predicate<"TM.Options.AllowFPOpFusion =="
                                 " FPOpFusion::Fast">;
 //===----------------------------------------------------------------------===//
 // Mips GPR size adjectives.
 // They are mutually exclusive.
 //===----------------------------------------------------------------------===//
 
 class GPR_32 { list<Predicate> GPRPredicates = [IsGP32bit]; }
 class GPR_64 { list<Predicate> GPRPredicates = [IsGP64bit]; }
 
 class PTR_32 { list<Predicate> PTRPredicates = [IsPTR32bit]; }
 class PTR_64 { list<Predicate> PTRPredicates = [IsPTR64bit]; }
 
 //===----------------------------------------------------------------------===//
 // Mips Symbol size adjectives.
 // They are mutally exculsive.
 //===----------------------------------------------------------------------===//
 
 class SYM_32 { list<Predicate> SYMPredicates = [IsSym32]; }
 class SYM_64 { list<Predicate> SYMPredicates = [IsSym64]; }
 
 //===----------------------------------------------------------------------===//
 // Mips ISA/ASE membership and instruction group membership adjectives.
 // They are mutually exclusive.
 //===----------------------------------------------------------------------===//
 
 // FIXME: I'd prefer to use additive predicates to build the instruction sets
 //        but we are short on assembler feature bits at the moment. Using a
 //        subtractive predicate will hopefully keep us under the 32 predicate
 //        limit long enough to develop an alternative way to handle P1||P2
 //        predicates.
 class ISA_MIPS1 {
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS1_NOT_MIPS3 {
   list<Predicate> InsnPredicates = [NotMips3];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS1_NOT_4_32 {
   list<Predicate> InsnPredicates = [NotMips4_32];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS1_NOT_32R6_64R6 {
   list<Predicate> InsnPredicates = [NotMips32r6, NotMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS2 {
   list<Predicate> InsnPredicates = [HasMips2];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS2_NOT_32R6_64R6 {
   list<Predicate> InsnPredicates = [HasMips2, NotMips32r6, NotMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS3 {
   list<Predicate> InsnPredicates = [HasMips3];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS3_NOT_32R6_64R6 {
   list<Predicate> InsnPredicates = [HasMips3, NotMips32r6, NotMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS32 {
   list<Predicate> InsnPredicates = [HasMips32];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS32_NOT_32R6_64R6 {
   list<Predicate> InsnPredicates = [HasMips32, NotMips32r6, NotMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS32R2 {
   list<Predicate> InsnPredicates = [HasMips32r2];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS32R2_NOT_32R6_64R6 {
   list<Predicate> InsnPredicates = [HasMips32r2, NotMips32r6, NotMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS32R5 {
   list<Predicate> InsnPredicates = [HasMips32r5];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS64 {
   list<Predicate> InsnPredicates = [HasMips64];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS64_NOT_64R6 {
   list<Predicate> InsnPredicates = [HasMips64, NotMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS64R2 {
   list<Predicate> InsnPredicates = [HasMips64r2];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS64R5 {
   list<Predicate> InsnPredicates = [HasMips64r5];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS32R6 {
   list<Predicate> InsnPredicates = [HasMips32r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MIPS64R6 {
   list<Predicate> InsnPredicates = [HasMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 class ISA_MICROMIPS {
   list<Predicate> EncodingPredicates = [InMicroMips];
 }
 class ISA_MICROMIPS32R5 {
   list<Predicate> InsnPredicates = [HasMips32r5];
   list<Predicate> EncodingPredicates = [InMicroMips];
 }
 class ISA_MICROMIPS32R6 {
   list<Predicate> InsnPredicates = [HasMips32r6];
   list<Predicate> EncodingPredicates = [InMicroMips];
 }
 class ISA_MICROMIPS64R6 {
   list<Predicate> InsnPredicates = [HasMips64r6];
   list<Predicate> EncodingPredicates = [InMicroMips];
 }
 class ISA_MICROMIPS32_NOT_MIPS32R6 {
   list<Predicate> InsnPredicates = [NotMips32r6];
   list<Predicate> EncodingPredicates = [InMicroMips];
 }
 class ASE_EVA { list<Predicate> ASEPredicate = [HasEVA]; }
 
 // The portions of MIPS-III that were also added to MIPS32
 class INSN_MIPS3_32 {
   list<Predicate> InsnPredicates = [HasMips3_32];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 
 // The portions of MIPS-III that were also added to MIPS32 but were removed in
 // MIPS32r6 and MIPS64r6.
 class INSN_MIPS3_32_NOT_32R6_64R6 {
   list<Predicate> InsnPredicates = [HasMips3_32, NotMips32r6, NotMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 
 // The portions of MIPS-III that were also added to MIPS32
 class INSN_MIPS3_32R2 {
   list<Predicate> InsnPredicates = [HasMips3_32r2];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 
 // The portions of MIPS-IV that were also added to MIPS32.
 class INSN_MIPS4_32 {
   list <Predicate> InsnPredicates = [HasMips4_32];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 
 // The portions of MIPS-IV that were also added to MIPS32 but were removed in
 // MIPS32r6 and MIPS64r6.
 class INSN_MIPS4_32_NOT_32R6_64R6 {
   list<Predicate> InsnPredicates = [HasMips4_32, NotMips32r6, NotMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 
 // The portions of MIPS-IV that were also added to MIPS32r2 but were removed in
 // MIPS32r6 and MIPS64r6.
 class INSN_MIPS4_32R2_NOT_32R6_64R6 {
   list<Predicate> InsnPredicates = [HasMips4_32r2, NotMips32r6, NotMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 
 // The portions of MIPS-IV that were also added to MIPS32r2.
 class INSN_MIPS4_32R2 {
   list<Predicate> InsnPredicates = [HasMips4_32r2];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 
 // The portions of MIPS-V that were also added to MIPS32r2 but were removed in
 // MIPS32r6 and MIPS64r6.
 class INSN_MIPS5_32R2_NOT_32R6_64R6 {
   list<Predicate> InsnPredicates = [HasMips5_32r2, NotMips32r6, NotMips64r6];
   list<Predicate> EncodingPredicates = [HasStdEnc];
 }
 
 class ASE_CNMIPS {
   list<Predicate> ASEPredicate = [HasCnMips];
 }
 
 class NOT_ASE_CNMIPS {
   list<Predicate> ASEPredicate = [NotCnMips];
 }
 
 class ASE_CNMIPSP {
   list<Predicate> ASEPredicate = [HasCnMipsP];
 }
 
 class NOT_ASE_CNMIPSP {
   list<Predicate> ASEPredicate = [NotCnMipsP];
 }
 
 class ASE_MIPS64_CNMIPS {
   list<Predicate> ASEPredicate = [HasMips64, HasCnMips];
 }
 
 class ASE_MSA {
   list<Predicate> ASEPredicate = [HasMSA];
 }
 
 class ASE_MSA_NOT_MSA64 {
   list<Predicate> ASEPredicate = [HasMSA, NotMips64];
 }
 
 class ASE_MSA64 {
   list<Predicate> ASEPredicate = [HasMSA, HasMips64];
 }
 
 class ASE_MT {
   list <Predicate> ASEPredicate = [HasMT];
 }
 
 class ASE_CRC {
   list <Predicate> ASEPredicate = [HasCRC];
 }
 
 class ASE_VIRT {
   list <Predicate> ASEPredicate = [HasVirt];
 }
 
 class ASE_GINV {
   list <Predicate> ASEPredicate = [HasGINV];
 }
 
 // Class used for separating microMIPSr6 and microMIPS (r3) instruction.
 // It can be used only on instructions that doesn't inherit PredicateControl.
 class ISA_MICROMIPS_NOT_32R6 : PredicateControl {
   let InsnPredicates = [NotMips32r6];
   let EncodingPredicates = [InMicroMips];
 }
 
 class ASE_NOT_DSP {
   list<Predicate> ASEPredicate = [NotDSP];
 }
 
 class MADD4 {
   list<Predicate> AdditionalPredicates = [HasMadd4];
 }
 
 // Classes used for separating expansions that differ based on the ABI in
 // use.
 class ABI_N64 {
   list<Predicate> AdditionalPredicates = [IsN64];
 }
 
 class ABI_NOT_N64 {
   list<Predicate> AdditionalPredicates = [IsNotN64];
 }
 
 class FPOP_FUSION_FAST {
   list <Predicate> AdditionalPredicates = [AllowFPOpFusion];
 }
 
 //===----------------------------------------------------------------------===//
 
 class MipsPat<dag pattern, dag result> : Pat<pattern, result>, PredicateControl;
 
 class MipsInstAlias<string Asm, dag Result, bit Emit = 0b1> :
   InstAlias<Asm, Result, Emit>, PredicateControl;
 
 class IsCommutable {
   bit isCommutable = 1;
 }
 
 class IsBranch {
   bit isBranch = 1;
   bit isCTI = 1;
 }
 
 class IsReturn {
   bit isReturn = 1;
   bit isCTI = 1;
 }
 
 class IsCall {
   bit isCall = 1;
   bit isCTI = 1;
 }
 
 class IsTailCall {
   bit isCall = 1;
   bit isTerminator = 1;
   bit isReturn = 1;
   bit isBarrier = 1;
   bit hasExtraSrcRegAllocReq = 1;
   bit isCodeGenOnly = 1;
   bit isCTI = 1;
 }
 
 class IsAsCheapAsAMove {
   bit isAsCheapAsAMove = 1;
 }
 
 class NeverHasSideEffects {
   bit hasSideEffects = 0;
 }
 
 //===----------------------------------------------------------------------===//
 // Instruction format superclass
 //===----------------------------------------------------------------------===//
 
 include "MipsInstrFormats.td"
 
 //===----------------------------------------------------------------------===//
 // Mips Operand, Complex Patterns and Transformations Definitions.
 //===----------------------------------------------------------------------===//
 
 class ConstantSImmAsmOperandClass<int Bits, list<AsmOperandClass> Supers = [],
                                   int Offset = 0> : AsmOperandClass {
   let Name = "ConstantSImm" # Bits # "_" # Offset;
   let RenderMethod = "addConstantSImmOperands<" # Bits # ", " # Offset # ">";
   let PredicateMethod = "isConstantSImm<" # Bits # ", " # Offset # ">";
   let SuperClasses = Supers;
   let DiagnosticType = "SImm" # Bits # "_" # Offset;
 }
 
 class SimmLslAsmOperandClass<int Bits, list<AsmOperandClass> Supers = [],
                                   int Shift = 0> : AsmOperandClass {
   let Name = "Simm" # Bits # "_Lsl" # Shift;
   let RenderMethod = "addImmOperands";
   let PredicateMethod = "isScaledSImm<" # Bits # ", " # Shift # ">";
   let SuperClasses = Supers;
   let DiagnosticType = "SImm" # Bits # "_Lsl" # Shift;
 }
 
 class ConstantUImmAsmOperandClass<int Bits, list<AsmOperandClass> Supers = [],
                                   int Offset = 0> : AsmOperandClass {
   let Name = "ConstantUImm" # Bits # "_" # Offset;
   let RenderMethod = "addConstantUImmOperands<" # Bits # ", " # Offset # ">";
   let PredicateMethod = "isConstantUImm<" # Bits # ", " # Offset # ">";
   let SuperClasses = Supers;
   let DiagnosticType = "UImm" # Bits # "_" # Offset;
 }
 
 class ConstantUImmRangeAsmOperandClass<int Bottom, int Top,
                                        list<AsmOperandClass> Supers = []>
     : AsmOperandClass {
   let Name = "ConstantUImmRange" # Bottom # "_" # Top;
   let RenderMethod = "addImmOperands";
   let PredicateMethod = "isConstantUImmRange<" # Bottom # ", " # Top # ">";
   let SuperClasses = Supers;
   let DiagnosticType = "UImmRange" # Bottom # "_" # Top;
 }
 
 class SImmAsmOperandClass<int Bits, list<AsmOperandClass> Supers = []>
     : AsmOperandClass {
   let Name = "SImm" # Bits;
   let RenderMethod = "addSImmOperands<" # Bits # ">";
   let PredicateMethod = "isSImm<" # Bits # ">";
   let SuperClasses = Supers;
   let DiagnosticType = "SImm" # Bits;
 }
 
 class UImmAsmOperandClass<int Bits, list<AsmOperandClass> Supers = []>
     : AsmOperandClass {
   let Name = "UImm" # Bits;
   let RenderMethod = "addUImmOperands<" # Bits # ">";
   let PredicateMethod = "isUImm<" # Bits # ">";
   let SuperClasses = Supers;
   let DiagnosticType = "UImm" # Bits;
 }
 
 // Generic case - only to support certain assembly pseudo instructions.
 class UImmAnyAsmOperandClass<int Bits, list<AsmOperandClass> Supers = []>
     : AsmOperandClass {
   let Name = "ImmAny";
   let RenderMethod = "addConstantUImmOperands<32>";
   let PredicateMethod = "isSImm<" # Bits # ">";
   let SuperClasses = Supers;
   let DiagnosticType = "ImmAny";
 }
 
 // AsmOperandClasses require a strict ordering which is difficult to manage
 // as a hierarchy. Instead, we use a linear ordering and impose an order that
 // is in some places arbitrary.
 //
 // Here the rules that are in use:
 // * Wider immediates are a superset of narrower immediates:
 //     uimm4 < uimm5 < uimm6
 // * For the same bit-width, unsigned immediates are a superset of signed
 //   immediates::
 //     simm4 < uimm4 < simm5 < uimm5
 // * For the same upper-bound, signed immediates are a superset of unsigned
 //   immediates:
 //     uimm3 < simm4 < uimm4 < simm4
 // * Modified immediates are a superset of ordinary immediates:
 //     uimm5 < uimm5_plus1 (1..32) < uimm5_plus32 (32..63) < uimm6
 //   The term 'superset' starts to break down here since the uimm5_plus* classes
 //   are not true supersets of uimm5 (but they are still subsets of uimm6).
 // * 'Relaxed' immediates are supersets of the corresponding unsigned immediate.
 //     uimm16 < uimm16_relaxed
 // * The codeGen pattern type is arbitrarily ordered.
 //     uimm5 < uimm5_64, and uimm5 < vsplat_uimm5
 //   This is entirely arbitrary. We need an ordering and what we pick is
 //   unimportant since only one is possible for a given mnemonic.
 
 def UImm32CoercedAsmOperandClass : UImmAnyAsmOperandClass<33, []> {
   let Name = "UImm32_Coerced";
   let DiagnosticType = "UImm32_Coerced";
 }
 def SImm32RelaxedAsmOperandClass
     : SImmAsmOperandClass<32, [UImm32CoercedAsmOperandClass]> {
   let Name = "SImm32_Relaxed";
   let PredicateMethod = "isAnyImm<33>";
   let DiagnosticType = "SImm32_Relaxed";
 }
 def SImm32AsmOperandClass
     : SImmAsmOperandClass<32, [SImm32RelaxedAsmOperandClass]>;
 def ConstantUImm26AsmOperandClass
     : ConstantUImmAsmOperandClass<26, [SImm32AsmOperandClass]>;
 def ConstantUImm20AsmOperandClass
     : ConstantUImmAsmOperandClass<20, [ConstantUImm26AsmOperandClass]>;
 def ConstantSImm19Lsl2AsmOperandClass : AsmOperandClass {
   let Name = "SImm19Lsl2";
   let RenderMethod = "addImmOperands";
   let PredicateMethod = "isScaledSImm<19, 2>";
   let SuperClasses = [ConstantUImm20AsmOperandClass];
   let DiagnosticType = "SImm19_Lsl2";
 }
 def UImm16RelaxedAsmOperandClass
     : UImmAsmOperandClass<16, [ConstantUImm20AsmOperandClass]> {
   let Name = "UImm16_Relaxed";
   let PredicateMethod = "isAnyImm<16>";
   let DiagnosticType = "UImm16_Relaxed";
 }
 // Similar to the relaxed classes which take an SImm and render it as
 // an UImm, this takes a UImm and renders it as an SImm.
 def UImm16AltRelaxedAsmOperandClass
     : SImmAsmOperandClass<16, [UImm16RelaxedAsmOperandClass]> {
   let Name = "UImm16_AltRelaxed";
   let PredicateMethod = "isUImm<16>";
   let DiagnosticType = "UImm16_AltRelaxed";
 }
 // FIXME: One of these should probably have UImm16AsmOperandClass as the
 //        superclass instead of UImm16RelaxedasmOPerandClass.
 def UImm16AsmOperandClass
     : UImmAsmOperandClass<16, [UImm16RelaxedAsmOperandClass]>;
 def SImm16RelaxedAsmOperandClass
     : SImmAsmOperandClass<16, [UImm16RelaxedAsmOperandClass]> {
   let Name = "SImm16_Relaxed";
   let PredicateMethod = "isAnyImm<16>";
   let DiagnosticType = "SImm16_Relaxed";
 }
 def SImm16AsmOperandClass
     : SImmAsmOperandClass<16, [SImm16RelaxedAsmOperandClass]>;
 def ConstantSImm10Lsl3AsmOperandClass : AsmOperandClass {
   let Name = "SImm10Lsl3";
   let RenderMethod = "addImmOperands";
   let PredicateMethod = "isScaledSImm<10, 3>";
   let SuperClasses = [SImm16AsmOperandClass];
   let DiagnosticType = "SImm10_Lsl3";
 }
 def ConstantSImm10Lsl2AsmOperandClass : AsmOperandClass {
   let Name = "SImm10Lsl2";
   let RenderMethod = "addImmOperands";
   let PredicateMethod = "isScaledSImm<10, 2>";
   let SuperClasses = [ConstantSImm10Lsl3AsmOperandClass];
   let DiagnosticType = "SImm10_Lsl2";
 }
 def ConstantSImm11AsmOperandClass
     : ConstantSImmAsmOperandClass<11, [ConstantSImm10Lsl2AsmOperandClass]>;
 def ConstantSImm10Lsl1AsmOperandClass : AsmOperandClass {
   let Name = "SImm10Lsl1";
   let RenderMethod = "addImmOperands";
   let PredicateMethod = "isScaledSImm<10, 1>";
   let SuperClasses = [ConstantSImm11AsmOperandClass];
   let DiagnosticType = "SImm10_Lsl1";
 }
 def ConstantUImm10AsmOperandClass
     : ConstantUImmAsmOperandClass<10, [ConstantSImm10Lsl1AsmOperandClass]>;
 def ConstantSImm10AsmOperandClass
     : ConstantSImmAsmOperandClass<10, [ConstantUImm10AsmOperandClass]>;
 def ConstantSImm9AsmOperandClass
     : ConstantSImmAsmOperandClass<9, [ConstantSImm10AsmOperandClass]>;
 def ConstantSImm7Lsl2AsmOperandClass : AsmOperandClass {
   let Name = "SImm7Lsl2";
   let RenderMethod = "addImmOperands";
   let PredicateMethod = "isScaledSImm<7, 2>";
   let SuperClasses = [ConstantSImm9AsmOperandClass];
   let DiagnosticType = "SImm7_Lsl2";
 }
 def ConstantUImm8AsmOperandClass
     : ConstantUImmAsmOperandClass<8, [ConstantSImm7Lsl2AsmOperandClass]>;
 def ConstantUImm7Sub1AsmOperandClass
     : ConstantUImmAsmOperandClass<7, [ConstantUImm8AsmOperandClass], -1> {
   // Specify the names since the -1 offset causes invalid identifiers otherwise.
   let Name = "UImm7_N1";
   let DiagnosticType = "UImm7_N1";
 }
 def ConstantUImm7AsmOperandClass
     : ConstantUImmAsmOperandClass<7, [ConstantUImm7Sub1AsmOperandClass]>;
 def ConstantUImm6Lsl2AsmOperandClass : AsmOperandClass {
   let Name = "UImm6Lsl2";
   let RenderMethod = "addImmOperands";
   let PredicateMethod = "isScaledUImm<6, 2>";
   let SuperClasses = [ConstantUImm7AsmOperandClass];
   let DiagnosticType = "UImm6_Lsl2";
 }
 def ConstantUImm6AsmOperandClass
     : ConstantUImmAsmOperandClass<6, [ConstantUImm6Lsl2AsmOperandClass]>;
 def ConstantSImm6AsmOperandClass
     : ConstantSImmAsmOperandClass<6, [ConstantUImm6AsmOperandClass]>;
 def ConstantUImm5Lsl2AsmOperandClass : AsmOperandClass {
   let Name = "UImm5Lsl2";
   let RenderMethod = "addImmOperands";
   let PredicateMethod = "isScaledUImm<5, 2>";
   let SuperClasses = [ConstantSImm6AsmOperandClass];
   let DiagnosticType = "UImm5_Lsl2";
 }
 def ConstantUImm5_Range2_64AsmOperandClass
     : ConstantUImmRangeAsmOperandClass<2, 64, [ConstantUImm5Lsl2AsmOperandClass]>;
 def ConstantUImm5Plus33AsmOperandClass
     : ConstantUImmAsmOperandClass<5, [ConstantUImm5_Range2_64AsmOperandClass],
                                   33>;
 def ConstantUImm5ReportUImm6AsmOperandClass
     : ConstantUImmAsmOperandClass<5, [ConstantUImm5Plus33AsmOperandClass]> {
   let Name = "ConstantUImm5_0_Report_UImm6";
   let DiagnosticType = "UImm5_0_Report_UImm6";
 }
 def ConstantUImm5Plus32AsmOperandClass
     : ConstantUImmAsmOperandClass<
           5, [ConstantUImm5ReportUImm6AsmOperandClass], 32>;
 def ConstantUImm5Plus32NormalizeAsmOperandClass
     : ConstantUImmAsmOperandClass<5, [ConstantUImm5Plus32AsmOperandClass], 32> {
   let Name = "ConstantUImm5_32_Norm";
   // We must also subtract 32 when we render the operand.
   let RenderMethod = "addConstantUImmOperands<5, 32, -32>";
 }
 def ConstantUImm5Plus1ReportUImm6AsmOperandClass
     : ConstantUImmAsmOperandClass<
           5, [ConstantUImm5Plus32NormalizeAsmOperandClass], 1>{
   let Name = "ConstantUImm5_Plus1_Report_UImm6";
 }
 def ConstantUImm5Plus1AsmOperandClass
     : ConstantUImmAsmOperandClass<
           5, [ConstantUImm5Plus1ReportUImm6AsmOperandClass], 1>;
 def ConstantUImm5AsmOperandClass
     : ConstantUImmAsmOperandClass<5, [ConstantUImm5Plus1AsmOperandClass]>;
 def ConstantSImm5AsmOperandClass
     : ConstantSImmAsmOperandClass<5, [ConstantUImm5AsmOperandClass]>;
 def ConstantUImm4AsmOperandClass
     : ConstantUImmAsmOperandClass<4, [ConstantSImm5AsmOperandClass]>;
 def ConstantSImm4AsmOperandClass
     : ConstantSImmAsmOperandClass<4, [ConstantUImm4AsmOperandClass]>;
 def ConstantUImm3AsmOperandClass
     : ConstantUImmAsmOperandClass<3, [ConstantSImm4AsmOperandClass]>;
 def ConstantUImm2Plus1AsmOperandClass
     : ConstantUImmAsmOperandClass<2, [ConstantUImm3AsmOperandClass], 1>;
 def ConstantUImm2AsmOperandClass
     : ConstantUImmAsmOperandClass<2, [ConstantUImm3AsmOperandClass]>;
 def ConstantUImm1AsmOperandClass
     : ConstantUImmAsmOperandClass<1, [ConstantUImm2AsmOperandClass]>;
 def ConstantImmzAsmOperandClass : AsmOperandClass {
   let Name = "ConstantImmz";
   let RenderMethod = "addConstantUImmOperands<1>";
   let PredicateMethod = "isConstantImmz";
   let SuperClasses = [ConstantUImm1AsmOperandClass];
   let DiagnosticType = "Immz";
 }
 
 def Simm19Lsl2AsmOperand
     : SimmLslAsmOperandClass<19, [], 2>;
 
 def MipsJumpTargetAsmOperand : AsmOperandClass {
   let Name = "JumpTarget";
   let ParserMethod = "parseJumpTarget";
   let PredicateMethod = "isImm";
   let RenderMethod = "addImmOperands";
 }
 
 // Instruction operand types
 def jmptarget   : Operand<OtherVT> {
   let EncoderMethod = "getJumpTargetOpValue";
   let ParserMatchClass = MipsJumpTargetAsmOperand;
 }
 def brtarget    : Operand<OtherVT> {
   let EncoderMethod = "getBranchTargetOpValue";
   let OperandType = "OPERAND_PCREL";
   let DecoderMethod = "DecodeBranchTarget";
   let ParserMatchClass = MipsJumpTargetAsmOperand;
 }
 def brtarget1SImm16 : Operand<OtherVT> {
   let EncoderMethod = "getBranchTargetOpValue1SImm16";
   let OperandType = "OPERAND_PCREL";
   let DecoderMethod = "DecodeBranchTarget1SImm16";
   let ParserMatchClass = MipsJumpTargetAsmOperand;
 }
 def calltarget  : Operand<iPTR> {
   let EncoderMethod = "getJumpTargetOpValue";
   let ParserMatchClass = MipsJumpTargetAsmOperand;
 }
 
 def imm64: Operand<i64>;
 
 def simm19_lsl2 : Operand<i32> {
   let EncoderMethod = "getSimm19Lsl2Encoding";
   let DecoderMethod = "DecodeSimm19Lsl2";
   let ParserMatchClass = Simm19Lsl2AsmOperand;
 }
 
 def simm18_lsl3 : Operand<i32> {
   let EncoderMethod = "getSimm18Lsl3Encoding";
   let DecoderMethod = "DecodeSimm18Lsl3";
   let ParserMatchClass = MipsJumpTargetAsmOperand;
 }
 
 // Zero
 def uimmz       : Operand<i32> {
   let PrintMethod = "printUImm<0>";
   let ParserMatchClass = ConstantImmzAsmOperandClass;
 }
 
 // size operand of ins instruction
 def uimm_range_2_64 : Operand<i32> {
   let PrintMethod = "printUImm<6, 2>";
   let EncoderMethod = "getSizeInsEncoding";
   let DecoderMethod = "DecodeInsSize";
   let ParserMatchClass = ConstantUImm5_Range2_64AsmOperandClass;
 }
 
 // Unsigned Operands
 foreach I = {1, 2, 3, 4, 5, 6, 7, 8, 10, 20, 26} in
   def uimm # I : Operand<i32> {
     let PrintMethod = "printUImm<" # I # ">";
     let ParserMatchClass =
         !cast<AsmOperandClass>("ConstantUImm" # I # "AsmOperandClass");
   }
 
 def uimm2_plus1 : Operand<i32> {
   let PrintMethod = "printUImm<2, 1>";
   let EncoderMethod = "getUImmWithOffsetEncoding<2, 1>";
   let DecoderMethod = "DecodeUImmWithOffset<2, 1>";
   let ParserMatchClass = ConstantUImm2Plus1AsmOperandClass;
 }
 
 def uimm5_plus1 : Operand<i32> {
   let PrintMethod = "printUImm<5, 1>";
   let EncoderMethod = "getUImmWithOffsetEncoding<5, 1>";
   let DecoderMethod = "DecodeUImmWithOffset<5, 1>";
   let ParserMatchClass = ConstantUImm5Plus1AsmOperandClass;
 }
 
 def uimm5_plus1_report_uimm6 : Operand<i32> {
   let PrintMethod = "printUImm<6, 1>";
   let EncoderMethod = "getUImmWithOffsetEncoding<5, 1>";
   let DecoderMethod = "DecodeUImmWithOffset<5, 1>";
   let ParserMatchClass = ConstantUImm5Plus1ReportUImm6AsmOperandClass;
 }
 
 def uimm5_plus32 : Operand<i32> {
   let PrintMethod = "printUImm<5, 32>";
   let ParserMatchClass = ConstantUImm5Plus32AsmOperandClass;
 }
 
 def uimm5_plus33 : Operand<i32> {
   let PrintMethod = "printUImm<5, 33>";
   let EncoderMethod = "getUImmWithOffsetEncoding<5, 1>";
   let DecoderMethod = "DecodeUImmWithOffset<5, 1>";
   let ParserMatchClass = ConstantUImm5Plus33AsmOperandClass;
 }
 
 def uimm5_inssize_plus1 : Operand<i32> {
   let PrintMethod = "printUImm<6>";
   let ParserMatchClass = ConstantUImm5Plus1AsmOperandClass;
   let EncoderMethod = "getSizeInsEncoding";
   let DecoderMethod = "DecodeInsSize";
 }
 
 def uimm5_plus32_normalize : Operand<i32> {
   let PrintMethod = "printUImm<5>";
   let ParserMatchClass = ConstantUImm5Plus32NormalizeAsmOperandClass;
 }
 
 def uimm5_lsl2 : Operand<OtherVT> {
   let EncoderMethod = "getUImm5Lsl2Encoding";
   let DecoderMethod = "DecodeUImmWithOffsetAndScale<5, 0, 4>";
   let ParserMatchClass = ConstantUImm5Lsl2AsmOperandClass;
 }
 
 def uimm5_plus32_normalize_64 : Operand<i64> {
   let PrintMethod = "printUImm<5>";
   let ParserMatchClass = ConstantUImm5Plus32NormalizeAsmOperandClass;
 }
 
 def uimm6_lsl2 : Operand<OtherVT> {
   let EncoderMethod = "getUImm6Lsl2Encoding";
   let DecoderMethod = "DecodeUImmWithOffsetAndScale<6, 0, 4>";
   let ParserMatchClass = ConstantUImm6Lsl2AsmOperandClass;
 }
 
 foreach I = {16} in
   def uimm # I : Operand<i32> {
     let PrintMethod = "printUImm<" # I # ">";
     let ParserMatchClass =
         !cast<AsmOperandClass>("UImm" # I # "AsmOperandClass");
   }
 
 // Like uimm16_64 but coerces simm16 to uimm16.
 def uimm16_relaxed : Operand<i32> {
   let PrintMethod = "printUImm<16>";
   let ParserMatchClass = UImm16RelaxedAsmOperandClass;
 }
 
 foreach I = {5} in
   def uimm # I # _64 : Operand<i64> {
     let PrintMethod = "printUImm<" # I # ">";
     let ParserMatchClass =
         !cast<AsmOperandClass>("ConstantUImm" # I # "AsmOperandClass");
   }
 
 foreach I = {16} in
   def uimm # I # _64 : Operand<i64> {
     let PrintMethod = "printUImm<" # I # ">";
     let ParserMatchClass =
         !cast<AsmOperandClass>("UImm" # I # "AsmOperandClass");
   }
 
 // Like uimm16_64 but coerces simm16 to uimm16.
 def uimm16_64_relaxed : Operand<i64> {
   let PrintMethod = "printUImm<16>";
   let ParserMatchClass = UImm16RelaxedAsmOperandClass;
 }
 
 def uimm16_altrelaxed : Operand<i32> {
   let PrintMethod = "printUImm<16>";
   let ParserMatchClass = UImm16AltRelaxedAsmOperandClass;
 }
 // Like uimm5 but reports a less confusing error for 32-63 when
 // an instruction alias permits that.
 def uimm5_report_uimm6 : Operand<i32> {
   let PrintMethod = "printUImm<6>";
   let ParserMatchClass = ConstantUImm5ReportUImm6AsmOperandClass;
 }
 
 // Like uimm5_64 but reports a less confusing error for 32-63 when
 // an instruction alias permits that.
 def uimm5_64_report_uimm6 : Operand<i64> {
   let PrintMethod = "printUImm<5>";
   let ParserMatchClass = ConstantUImm5ReportUImm6AsmOperandClass;
 }
 
 foreach I = {1, 2, 3, 4} in
   def uimm # I # _ptr : Operand<iPTR> {
     let PrintMethod = "printUImm<" # I # ">";
     let ParserMatchClass =
         !cast<AsmOperandClass>("ConstantUImm" # I # "AsmOperandClass");
   }
 
 foreach I = {1, 2, 3, 4, 5, 6, 8} in
   def vsplat_uimm # I : Operand<vAny> {
     let PrintMethod = "printUImm<" # I # ">";
     let ParserMatchClass =
         !cast<AsmOperandClass>("ConstantUImm" # I # "AsmOperandClass");
   }
 
 // Signed operands
 foreach I = {4, 5, 6, 9, 10, 11} in
   def simm # I : Operand<i32> {
     let DecoderMethod = "DecodeSImmWithOffsetAndScale<" # I # ">";
     let ParserMatchClass =
         !cast<AsmOperandClass>("ConstantSImm" # I # "AsmOperandClass");
   }
 
 foreach I = {1, 2, 3} in
   def simm10_lsl # I : Operand<i32> {
     let DecoderMethod = "DecodeSImmWithOffsetAndScale<10, " # I # ">";
     let ParserMatchClass =
         !cast<AsmOperandClass>("ConstantSImm10Lsl" # I # "AsmOperandClass");
   }
 
 foreach I = {10} in
   def simm # I # _64 : Operand<i64> {
     let DecoderMethod = "DecodeSImmWithOffsetAndScale<" # I # ">";
     let ParserMatchClass =
         !cast<AsmOperandClass>("ConstantSImm" # I # "AsmOperandClass");
   }
 
 foreach I = {5, 10} in
   def vsplat_simm # I : Operand<vAny> {
     let ParserMatchClass =
         !cast<AsmOperandClass>("ConstantSImm" # I # "AsmOperandClass");
   }
 
 def simm7_lsl2 : Operand<OtherVT> {
   let EncoderMethod = "getSImm7Lsl2Encoding";
   let DecoderMethod = "DecodeSImmWithOffsetAndScale<" # I # ", 0, 4>";
   let ParserMatchClass = ConstantSImm7Lsl2AsmOperandClass;
 }
 
 foreach I = {16, 32} in
   def simm # I : Operand<i32> {
     let DecoderMethod = "DecodeSImmWithOffsetAndScale<" # I # ">";
     let ParserMatchClass = !cast<AsmOperandClass>("SImm" # I # "AsmOperandClass");
   }
 
 // Like simm16 but coerces uimm16 to simm16.
 def simm16_relaxed : Operand<i32> {
   let DecoderMethod = "DecodeSImmWithOffsetAndScale<16>";
   let ParserMatchClass = SImm16RelaxedAsmOperandClass;
 }
 
 def simm16_64 : Operand<i64> {
   let DecoderMethod = "DecodeSImmWithOffsetAndScale<16>";
   let ParserMatchClass = SImm16AsmOperandClass;
 }
 
 // like simm32 but coerces simm32 to uimm32.
 def uimm32_coerced : Operand<i32> {
   let ParserMatchClass = UImm32CoercedAsmOperandClass;
 }
 // Like simm32 but coerces uimm32 to simm32.
 def simm32_relaxed : Operand<i32> {
   let DecoderMethod = "DecodeSImmWithOffsetAndScale<32>";
   let ParserMatchClass = SImm32RelaxedAsmOperandClass;
 }
 
 // This is almost the same as a uimm7 but 0x7f is interpreted as -1.
 def li16_imm : Operand<i32> {
   let DecoderMethod = "DecodeLi16Imm";
   let ParserMatchClass = ConstantUImm7Sub1AsmOperandClass;
 }
 
 def MipsMemAsmOperand : AsmOperandClass {
   let Name = "Mem";
   let ParserMethod = "parseMemOperand";
 }
 
 class MipsMemSimmAsmOperand<int Width, int Shift = 0> : AsmOperandClass {
   let Name = "MemOffsetSimm" # Width # "_" # Shift;
   let SuperClasses = [MipsMemAsmOperand];
   let RenderMethod = "addMemOperands";
   let ParserMethod = "parseMemOperand";
   let PredicateMethod = "isMemWithSimmOffset<" # Width # ", " # Shift # ">";
   let DiagnosticType = !if(!eq(Shift, 0), "MemSImm" # Width,
                                           "MemSImm" # Width # "Lsl" # Shift);
 }
 
 def MipsMemSimmPtrAsmOperand : AsmOperandClass {
   let Name = "MemOffsetSimmPtr";
   let SuperClasses = [MipsMemAsmOperand];
   let RenderMethod = "addMemOperands";
   let ParserMethod = "parseMemOperand";
   let PredicateMethod = "isMemWithPtrSizeOffset";
   let DiagnosticType = "MemSImmPtr";
 }
 
 def MipsInvertedImmoperand : AsmOperandClass {
   let Name = "InvNum";
   let RenderMethod = "addImmOperands";
   let ParserMethod = "parseInvNum";
 }
 
 def InvertedImOperand : Operand<i32> {
   let ParserMatchClass = MipsInvertedImmoperand;
 }
 
 def InvertedImOperand64 : Operand<i64> {
   let ParserMatchClass = MipsInvertedImmoperand;
 }
 
 class mem_generic : Operand<iPTR> {
   let PrintMethod = "printMemOperand";
   let MIOperandInfo = (ops ptr_rc, simm16);
   let EncoderMethod = "getMemEncoding";
   let ParserMatchClass = MipsMemAsmOperand;
   let OperandType = "OPERAND_MEMORY";
 }
 
 // Address operand
 def mem : mem_generic;
 
 // MSA specific address operand
 def mem_msa : mem_generic {
   let MIOperandInfo = (ops ptr_rc, simm10);
   let EncoderMethod = "getMSAMemEncoding";
 }
 
 def simm12 : Operand<i32> {
   let DecoderMethod = "DecodeSimm12";
 }
 
 def mem_simm9_exp : mem_generic {
   let MIOperandInfo = (ops ptr_rc, simm9);
   let ParserMatchClass = MipsMemSimmPtrAsmOperand;
   let OperandNamespace = "MipsII";
   let OperandType = "OPERAND_MEM_SIMM9";
 }
 
 foreach I = {9, 10, 11, 12, 16} in
   def mem_simm # I : mem_generic {
     let MIOperandInfo = (ops ptr_rc, !cast<Operand>("simm" # I));
     let ParserMatchClass = MipsMemSimmAsmOperand<I>;
   }
 
 foreach I = {1, 2, 3} in
   def mem_simm10_lsl # I : mem_generic {
     let MIOperandInfo = (ops ptr_rc, !cast<Operand>("simm10_lsl" # I));
     let EncoderMethod = "getMemEncoding<" # I  # ">";
     let ParserMatchClass = MipsMemSimmAsmOperand<10, I>;
   }
 
 def mem_simmptr : mem_generic {
   let ParserMatchClass = MipsMemSimmPtrAsmOperand;
 }
 
 def mem_ea : Operand<iPTR> {
   let PrintMethod = "printMemOperandEA";
   let MIOperandInfo = (ops ptr_rc, simm16);
   let EncoderMethod = "getMemEncoding";
   let OperandType = "OPERAND_MEMORY";
 }
 
 def PtrRC : Operand<iPTR> {
   let MIOperandInfo = (ops ptr_rc);
   let DecoderMethod = "DecodePtrRegisterClass";
   let ParserMatchClass = GPR32AsmOperand;
 }
 
 // size operand of ins instruction
 def size_ins : Operand<i32> {
   let EncoderMethod = "getSizeInsEncoding";
   let DecoderMethod = "DecodeInsSize";
 }
 
 // Transformation Function - get the lower 16 bits.
 def LO16 : SDNodeXForm<imm, [{
   return getImm(N, N->getZExtValue() & 0xFFFF);
 }]>;
 
 // Transformation Function - get the higher 16 bits.
 def HI16 : SDNodeXForm<imm, [{
   return getImm(N, (N->getZExtValue() >> 16) & 0xFFFF);
 }]>;
 
 // Plus 1.
 def Plus1 : SDNodeXForm<imm, [{ return getImm(N, N->getSExtValue() + 1); }]>;
 
 // Node immediate is zero (e.g. insve.d)
 def immz : PatLeaf<(imm), [{ return N->getSExtValue() == 0; }]>;
 
 // Node immediate fits as 16-bit sign extended on target immediate.
 // e.g. addi, andi
 def immSExt8  : PatLeaf<(imm), [{ return isInt<8>(N->getSExtValue()); }]>;
 
 // Node immediate fits as 16-bit sign extended on target immediate.
 // e.g. addi, andi
 def immSExt16  : PatLeaf<(imm), [{ return isInt<16>(N->getSExtValue()); }]>;
 def imm32SExt16  : IntImmLeaf<i32, [{ return isInt<16>(Imm.getSExtValue()); }]>;
 
 // Node immediate fits as 7-bit zero extended on target immediate.
 def immZExt7 : PatLeaf<(imm), [{ return isUInt<7>(N->getZExtValue()); }]>;
 def timmZExt7 : PatLeaf<(timm), [{ return isUInt<7>(N->getZExtValue()); }]>;
 
 // Node immediate fits as 16-bit zero extended on target immediate.
 // The LO16 param means that only the lower 16 bits of the node
 // immediate are caught.
 // e.g. addiu, sltiu
 def immZExt16  : PatLeaf<(imm), [{
   if (N->getValueType(0) == MVT::i32)
     return (uint32_t)N->getZExtValue() == (unsigned short)N->getZExtValue();
   else
     return (uint64_t)N->getZExtValue() == (unsigned short)N->getZExtValue();
 }], LO16>;
 def imm32ZExt16  : IntImmLeaf<i32, [{
   return (uint32_t)Imm.getZExtValue() == (unsigned short)Imm.getZExtValue();
 }]>;
 
 // Immediate can be loaded with LUi (32-bit int with lower 16-bit cleared).
 def immSExt32Low16Zero : PatLeaf<(imm), [{
   int64_t Val = N->getSExtValue();
   return isInt<32>(Val) && !(Val & 0xffff);
 }]>;
 
 // Zero-extended 32-bit unsigned int with lower 16-bit cleared.
 def immZExt32Low16Zero : PatLeaf<(imm), [{
   uint64_t Val = N->getZExtValue();
   return isUInt<32>(Val) && !(Val & 0xffff);
 }]>;
 
 // Note immediate fits as a 32 bit signed extended on target immediate.
 def immSExt32  : PatLeaf<(imm), [{ return isInt<32>(N->getSExtValue()); }]>;
 
 // Note immediate fits as a 32 bit zero extended on target immediate.
 def immZExt32  : PatLeaf<(imm), [{ return isUInt<32>(N->getZExtValue()); }]>;
 
 // shamt field must fit in 5 bits.
 def immZExt5 : ImmLeaf<i32, [{return Imm == (Imm & 0x1f);}]>;
 def timmZExt5 : TImmLeaf<i32, [{return Imm == (Imm & 0x1f);}]>;
 
 def immZExt5Plus1 : PatLeaf<(imm), [{
   return isUInt<5>(N->getZExtValue() - 1);
 }]>;
 def immZExt5Plus32 : PatLeaf<(imm), [{
   return isUInt<5>(N->getZExtValue() - 32);
 }]>;
 def immZExt5Plus33 : PatLeaf<(imm), [{
   return isUInt<5>(N->getZExtValue() - 33);
 }]>;
 
 def immZExt5To31 : SDNodeXForm<imm, [{
   return getImm(N, 31 - N->getZExtValue());
 }]>;
 
 // True if (N + 1) fits in 16-bit field.
 def immSExt16Plus1 : PatLeaf<(imm), [{
   return isInt<17>(N->getSExtValue()) && isInt<16>(N->getSExtValue() + 1);
 }]>;
 
 def immZExtRange2To64 : PatLeaf<(imm), [{
   return isUInt<7>(N->getZExtValue()) && (N->getZExtValue() >= 2) &&
          (N->getZExtValue() <= 64);
 }]>;
 
 def ORiPred  : PatLeaf<(imm), [{
   return isUInt<16>(N->getZExtValue()) && !isInt<16>(N->getSExtValue());
 }], LO16>;
 
 def LUiPred : PatLeaf<(imm), [{
   int64_t Val = N->getSExtValue();
   return !isInt<16>(Val) && isInt<32>(Val) && !(Val & 0xffff);
 }]>;
 
 def LUiORiPred  : PatLeaf<(imm), [{
   int64_t SVal = N->getSExtValue();
   return isInt<32>(SVal) && (SVal & 0xffff);
 }]>;
 
 // Mips Address Mode! SDNode frameindex could possibly be a match
 // since load and store instructions from stack used it.
 def addr :
   ComplexPattern<iPTR, 2, "selectIntAddr", [frameindex]>;
 
 def addrRegImm :
   ComplexPattern<iPTR, 2, "selectAddrRegImm", [frameindex]>;
 
 def addrDefault :
   ComplexPattern<iPTR, 2, "selectAddrDefault", [frameindex]>;
 
 def addrimm10 : ComplexPattern<iPTR, 2, "selectIntAddrSImm10", [frameindex]>;
 def addrimm10lsl1 : ComplexPattern<iPTR, 2, "selectIntAddrSImm10Lsl1",
                                    [frameindex]>;
 def addrimm10lsl2 : ComplexPattern<iPTR, 2, "selectIntAddrSImm10Lsl2",
                                    [frameindex]>;
 def addrimm10lsl3 : ComplexPattern<iPTR, 2, "selectIntAddrSImm10Lsl3",
                                    [frameindex]>;
 
 //===----------------------------------------------------------------------===//
 // Instructions specific format
 //===----------------------------------------------------------------------===//
 
 // Arithmetic and logical instructions with 3 register operands.
 class ArithLogicR<string opstr, RegisterOperand RO, bit isComm = 0,
                   InstrItinClass Itin = NoItinerary,
                   SDPatternOperator OpNode = null_frag>:
   InstSE<(outs RO:$rd), (ins RO:$rs, RO:$rt),
          !strconcat(opstr, "\t$rd, $rs, $rt"),
          [(set RO:$rd, (OpNode RO:$rs, RO:$rt))], Itin, FrmR, opstr> {
   let isCommutable = isComm;
   let isReMaterializable = 1;
   let TwoOperandAliasConstraint = "$rd = $rs";
 }
 
 // Arithmetic and logical instructions with 2 register operands.
 class ArithLogicI<string opstr, Operand Od, RegisterOperand RO,
                   InstrItinClass Itin = NoItinerary,
                   SDPatternOperator imm_type = null_frag,
                   SDPatternOperator OpNode = null_frag> :
   InstSE<(outs RO:$rt), (ins RO:$rs, Od:$imm16),
          !strconcat(opstr, "\t$rt, $rs, $imm16"),
          [(set RO:$rt, (OpNode RO:$rs, imm_type:$imm16))],
          Itin, FrmI, opstr> {
   let isReMaterializable = 1;
   let TwoOperandAliasConstraint = "$rs = $rt";
 }
 
 // Arithmetic Multiply ADD/SUB
 class MArithR<string opstr, InstrItinClass itin, bit isComm = 0> :
   InstSE<(outs), (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
          !strconcat(opstr, "\t$rs, $rt"), [], itin, FrmR, opstr> {
   let Defs = [HI0, LO0];
   let Uses = [HI0, LO0];
   let isCommutable = isComm;
 }
 
 //  Logical
 class LogicNOR<string opstr, RegisterOperand RO>:
   InstSE<(outs RO:$rd), (ins RO:$rs, RO:$rt),
          !strconcat(opstr, "\t$rd, $rs, $rt"),
          [(set RO:$rd, (not (or RO:$rs, RO:$rt)))], II_NOR, FrmR, opstr> {
   let isCommutable = 1;
 }
 
 // Shifts
 class shift_rotate_imm<string opstr, Operand ImmOpnd,
                        RegisterOperand RO, InstrItinClass itin,
                        SDPatternOperator OpNode = null_frag,
                        SDPatternOperator PF = null_frag> :
   InstSE<(outs RO:$rd), (ins RO:$rt, ImmOpnd:$shamt),
          !strconcat(opstr, "\t$rd, $rt, $shamt"),
          [(set RO:$rd, (OpNode RO:$rt, PF:$shamt))], itin, FrmR, opstr> {
   let TwoOperandAliasConstraint = "$rt = $rd";
 }
 
 class shift_rotate_reg<string opstr, RegisterOperand RO, InstrItinClass itin,
                        SDPatternOperator OpNode = null_frag>:
   InstSE<(outs RO:$rd), (ins RO:$rt, GPR32Opnd:$rs),
          !strconcat(opstr, "\t$rd, $rt, $rs"),
          [(set RO:$rd, (OpNode RO:$rt, GPR32Opnd:$rs))], itin, FrmR,
          opstr>;
 
 // Load Upper Immediate
 class LoadUpper<string opstr, RegisterOperand RO, Operand Imm>:
   InstSE<(outs RO:$rt), (ins Imm:$imm16), !strconcat(opstr, "\t$rt, $imm16"),
          [], II_LUI, FrmI, opstr>, IsAsCheapAsAMove {
   let hasSideEffects = 0;
   let isReMaterializable = 1;
 }
 
 // Memory Load/Store
 class LoadMemory<string opstr, DAGOperand RO, DAGOperand MO,
                  SDPatternOperator OpNode = null_frag,
                  InstrItinClass Itin = NoItinerary,
                  ComplexPattern Addr = addr> :
   InstSE<(outs RO:$rt), (ins MO:$addr), !strconcat(opstr, "\t$rt, $addr"),
          [(set RO:$rt, (OpNode Addr:$addr))], Itin, FrmI, opstr> {
   let DecoderMethod = "DecodeMem";
   let canFoldAsLoad = 1;
   string BaseOpcode = opstr;
   let mayLoad = 1;
 }
 
 class Load<string opstr, DAGOperand RO, SDPatternOperator OpNode = null_frag,
            InstrItinClass Itin = NoItinerary, ComplexPattern Addr = addr> :
   LoadMemory<opstr, RO, mem, OpNode, Itin, Addr>;
 
 class StoreMemory<string opstr, DAGOperand RO, DAGOperand MO,
             SDPatternOperator OpNode = null_frag,
             InstrItinClass Itin = NoItinerary, ComplexPattern Addr = addr> :
   InstSE<(outs), (ins RO:$rt, MO:$addr), !strconcat(opstr, "\t$rt, $addr"),
          [(OpNode RO:$rt, Addr:$addr)], Itin, FrmI, opstr> {
   let DecoderMethod = "DecodeMem";
   string BaseOpcode = opstr;
   let mayStore = 1;
 }
 
 class Store<string opstr, DAGOperand RO, SDPatternOperator OpNode = null_frag,
             InstrItinClass Itin = NoItinerary, ComplexPattern Addr = addr,
             DAGOperand MO = mem> :
   StoreMemory<opstr, RO, MO, OpNode, Itin, Addr>;
 
 // Load/Store Left/Right
 let canFoldAsLoad = 1 in
 class LoadLeftRight<string opstr, SDNode OpNode, RegisterOperand RO,
                     InstrItinClass Itin> :
   InstSE<(outs RO:$rt), (ins mem:$addr, RO:$src),
          !strconcat(opstr, "\t$rt, $addr"),
          [(set RO:$rt, (OpNode addr:$addr, RO:$src))], Itin, FrmI> {
   let DecoderMethod = "DecodeMem";
   string Constraints = "$src = $rt";
   let BaseOpcode = opstr;
 }
 
 class StoreLeftRight<string opstr, SDNode OpNode, RegisterOperand RO,
                      InstrItinClass Itin> :
   InstSE<(outs), (ins RO:$rt, mem:$addr), !strconcat(opstr, "\t$rt, $addr"),
          [(OpNode RO:$rt, addr:$addr)], Itin, FrmI> {
   let DecoderMethod = "DecodeMem";
   let BaseOpcode = opstr;
 }
 
 // COP2 Load/Store
 class LW_FT2<string opstr, RegisterOperand RC, InstrItinClass Itin,
              SDPatternOperator OpNode= null_frag> :
   InstSE<(outs RC:$rt), (ins mem_simm16:$addr),
          !strconcat(opstr, "\t$rt, $addr"),
          [(set RC:$rt, (OpNode addrDefault:$addr))], Itin, FrmFI, opstr> {
   let DecoderMethod = "DecodeFMem2";
   let mayLoad = 1;
 }
 
 class SW_FT2<string opstr, RegisterOperand RC, InstrItinClass Itin,
              SDPatternOperator OpNode= null_frag> :
   InstSE<(outs), (ins RC:$rt, mem_simm16:$addr),
          !strconcat(opstr, "\t$rt, $addr"),
          [(OpNode RC:$rt, addrDefault:$addr)], Itin, FrmFI, opstr> {
   let DecoderMethod = "DecodeFMem2";
   let mayStore = 1;
 }
 
 // COP3 Load/Store
 class LW_FT3<string opstr, RegisterOperand RC, InstrItinClass Itin,
              SDPatternOperator OpNode= null_frag> :
   InstSE<(outs RC:$rt), (ins mem:$addr), !strconcat(opstr, "\t$rt, $addr"),
          [(set RC:$rt, (OpNode addrDefault:$addr))], Itin, FrmFI, opstr> {
   let DecoderMethod = "DecodeFMem3";
   let mayLoad = 1;
 }
 
 class SW_FT3<string opstr, RegisterOperand RC, InstrItinClass Itin,
              SDPatternOperator OpNode= null_frag> :
   InstSE<(outs), (ins RC:$rt, mem:$addr), !strconcat(opstr, "\t$rt, $addr"),
          [(OpNode RC:$rt, addrDefault:$addr)], Itin, FrmFI, opstr> {
   let DecoderMethod = "DecodeFMem3";
   let mayStore = 1;
 }
 
 // Conditional Branch
 class CBranch<string opstr, DAGOperand opnd, PatFrag cond_op,
               RegisterOperand RO> :
   InstSE<(outs), (ins RO:$rs, RO:$rt, opnd:$offset),
          !strconcat(opstr, "\t$rs, $rt, $offset"),
          [(brcond (i32 (cond_op RO:$rs, RO:$rt)), bb:$offset)], II_BCC,
          FrmI, opstr> {
   let isBranch = 1;
   let isTerminator = 1;
   let hasDelaySlot = 1;
   let Defs = [AT];
   bit isCTI = 1;
 }
 
 class CBranchLikely<string opstr, DAGOperand opnd, RegisterOperand RO> :
   InstSE<(outs), (ins RO:$rs, RO:$rt, opnd:$offset),
          !strconcat(opstr, "\t$rs, $rt, $offset"), [], II_BCC, FrmI, opstr> {
   let isBranch = 1;
   let isTerminator = 1;
   let hasDelaySlot = 1;
   let Defs = [AT];
   bit isCTI = 1;
 }
 
 class CBranchZero<string opstr, DAGOperand opnd, PatFrag cond_op,
                   RegisterOperand RO> :
   InstSE<(outs), (ins RO:$rs, opnd:$offset),
          !strconcat(opstr, "\t$rs, $offset"),
          [(brcond (i32 (cond_op RO:$rs, 0)), bb:$offset)], II_BCCZ,
          FrmI, opstr> {
   let isBranch = 1;
   let isTerminator = 1;
   let hasDelaySlot = 1;
   let Defs = [AT];
   bit isCTI = 1;
 }
 
 class CBranchZeroLikely<string opstr, DAGOperand opnd, RegisterOperand RO> :
   InstSE<(outs), (ins RO:$rs, opnd:$offset),
          !strconcat(opstr, "\t$rs, $offset"), [], II_BCCZ, FrmI, opstr> {
   let isBranch = 1;
   let isTerminator = 1;
   let hasDelaySlot = 1;
   let Defs = [AT];
   bit isCTI = 1;
 }
 
 // SetCC
 class SetCC_R<string opstr, PatFrag cond_op, RegisterOperand RO> :
   InstSE<(outs GPR32Opnd:$rd), (ins RO:$rs, RO:$rt),
          !strconcat(opstr, "\t$rd, $rs, $rt"),
          [(set GPR32Opnd:$rd, (cond_op RO:$rs, RO:$rt))],
          II_SLT_SLTU, FrmR, opstr>;
 
 class SetCC_I<string opstr, PatFrag cond_op, Operand Od, PatLeaf imm_type,
               RegisterOperand RO>:
   InstSE<(outs GPR32Opnd:$rt), (ins RO:$rs, Od:$imm16),
          !strconcat(opstr, "\t$rt, $rs, $imm16"),
          [(set GPR32Opnd:$rt, (cond_op RO:$rs, imm_type:$imm16))],
          II_SLTI_SLTIU, FrmI, opstr>;
 
 // Jump
 class JumpFJ<DAGOperand opnd, string opstr, SDPatternOperator operator,
              SDPatternOperator targetoperator, string bopstr> :
   InstSE<(outs), (ins opnd:$target), !strconcat(opstr, "\t$target"),
          [(operator targetoperator:$target)], II_J, FrmJ, bopstr> {
   let isTerminator=1;
   let isBarrier=1;
   let hasDelaySlot = 1;
   let DecoderMethod = "DecodeJumpTarget";
   let Defs = [AT];
   bit isCTI = 1;
 }
 
 // Unconditional branch
 class UncondBranch<Instruction BEQInst, DAGOperand opnd> :
   PseudoSE<(outs), (ins brtarget:$offset), [(br bb:$offset)], II_B>,
   PseudoInstExpansion<(BEQInst ZERO, ZERO, opnd:$offset)> {
   let isBranch = 1;
   let isTerminator = 1;
   let isBarrier = 1;
   let hasDelaySlot = 1;
   let AdditionalPredicates = [RelocPIC];
   let Defs = [AT];
   bit isCTI = 1;
 }
 
 // Base class for indirect branch and return instruction classes.
 let isTerminator=1, isBarrier=1, hasDelaySlot = 1, isCTI = 1 in
 class JumpFR<string opstr, RegisterOperand RO,
              SDPatternOperator operator = null_frag>:
   InstSE<(outs), (ins RO:$rs), "jr\t$rs", [(operator RO:$rs)], II_JR,
          FrmR, opstr>;
 
 // Indirect branch
 class IndirectBranch<string opstr, RegisterOperand RO> : JumpFR<opstr, RO> {
   let isBranch = 1;
   let isIndirectBranch = 1;
 }
 
 // Jump and Link (Call)
 let isCall=1, hasDelaySlot=1, isCTI=1, Defs = [RA] in {
   class JumpLink<string opstr, DAGOperand opnd> :
     InstSE<(outs), (ins opnd:$target), !strconcat(opstr, "\t$target"),
            [(MipsJmpLink tglobaladdr:$target)], II_JAL, FrmJ, opstr> {
     let DecoderMethod = "DecodeJumpTarget";
   }
 
   class JumpLinkRegPseudo<RegisterOperand RO, Instruction JALRInst,
                           Register RetReg, RegisterOperand ResRO = RO>:
     PseudoSE<(outs), (ins RO:$rs), [(MipsJmpLink RO:$rs)], II_JALR>,
     PseudoInstExpansion<(JALRInst RetReg, ResRO:$rs)> {
     let hasPostISelHook = 1;
   }
 
   class JumpLinkReg<string opstr, RegisterOperand RO>:
     InstSE<(outs RO:$rd), (ins RO:$rs), !strconcat(opstr, "\t$rd, $rs"),
            [], II_JALR, FrmR, opstr> {
     let hasPostISelHook = 1;
   }
 
   class BGEZAL_FT<string opstr, DAGOperand opnd,
                   RegisterOperand RO> :
     InstSE<(outs), (ins RO:$rs, opnd:$offset),
            !strconcat(opstr, "\t$rs, $offset"), [], II_BCCZAL, FrmI, opstr> {
     let hasDelaySlot = 1;
   }
 
 }
 
 let isCall = 1, isTerminator = 1, isReturn = 1, isBarrier = 1, hasDelaySlot = 1,
     hasExtraSrcRegAllocReq = 1, isCTI = 1, Defs = [AT] in {
   class TailCall<Instruction JumpInst, DAGOperand Opnd> :
     PseudoSE<(outs), (ins calltarget:$target), [], II_J>,
     PseudoInstExpansion<(JumpInst Opnd:$target)>;
 
   class TailCallReg<Instruction JumpInst, RegisterOperand RO> :
     PseudoSE<(outs), (ins RO:$rs), [(MipsTailCall RO:$rs)], II_JR>,
     PseudoInstExpansion<(JumpInst RO:$rs)> {
     let hasPostISelHook = 1;
   }
 }
 
 class BAL_BR_Pseudo<Instruction RealInst, DAGOperand opnd> :
   PseudoSE<(outs), (ins opnd:$offset), [], II_BCCZAL>,
   PseudoInstExpansion<(RealInst ZERO, opnd:$offset)> {
   let isBranch = 1;
   let isTerminator = 1;
   let isBarrier = 1;
   let hasDelaySlot = 1;
   let Defs = [RA];
   bit isCTI = 1;
 }
 
 let isCTI = 1 in {
 // Syscall
 class SYS_FT<string opstr, Operand ImmOp, InstrItinClass itin = NoItinerary> :
   InstSE<(outs), (ins ImmOp:$code_),
          !strconcat(opstr, "\t$code_"), [], itin, FrmI, opstr>;
 // Break
 class BRK_FT<string opstr> :
   InstSE<(outs), (ins uimm10:$code_1, uimm10:$code_2),
          !strconcat(opstr, "\t$code_1, $code_2"), [], II_BREAK,
          FrmOther, opstr>;
 
 // (D)Eret
 class ER_FT<string opstr, InstrItinClass itin = NoItinerary> :
   InstSE<(outs), (ins),
          opstr, [], itin, FrmOther, opstr>;
 
 // Wait
 class WAIT_FT<string opstr> :
   InstSE<(outs), (ins), opstr, [], II_WAIT, FrmOther, opstr>;
 }
 
 // Interrupts
 class DEI_FT<string opstr, RegisterOperand RO,
              InstrItinClass itin = NoItinerary> :
   InstSE<(outs RO:$rt), (ins),
          !strconcat(opstr, "\t$rt"), [], itin, FrmOther, opstr>;
 
 // Sync
 let hasSideEffects = 1 in
 class SYNC_FT<string opstr> :
   InstSE<(outs), (ins uimm5:$stype), "sync $stype",
          [(MipsSync immZExt5:$stype)], II_SYNC, FrmOther, opstr>;
 
 class SYNCI_FT<string opstr, DAGOperand MO> :
   InstSE<(outs), (ins MO:$addr), !strconcat(opstr, "\t$addr"), [],
          II_SYNCI, FrmOther, opstr> {
   let hasSideEffects = 1;
   let DecoderMethod = "DecodeSyncI";
 }
 
 let hasSideEffects = 1, isCTI = 1 in {
 class TEQ_FT<string opstr, RegisterOperand RO, Operand ImmOp,
              InstrItinClass itin = NoItinerary> :
   InstSE<(outs), (ins RO:$rs, RO:$rt, ImmOp:$code_),
          !strconcat(opstr, "\t$rs, $rt, $code_"), [], itin, FrmI, opstr>;
 
 class TEQI_FT<string opstr, RegisterOperand RO,
               InstrItinClass itin = NoItinerary> :
   InstSE<(outs), (ins RO:$rs, simm16:$imm16),
          !strconcat(opstr, "\t$rs, $imm16"), [], itin, FrmOther, opstr>;
 }
 
 // Mul, Div
 class Mult<string opstr, InstrItinClass itin, RegisterOperand RO,
            list<Register> DefRegs> :
   InstSE<(outs), (ins RO:$rs, RO:$rt), !strconcat(opstr, "\t$rs, $rt"), [],
          itin, FrmR, opstr> {
   let isCommutable = 1;
   let Defs = DefRegs;
   let hasSideEffects = 0;
 }
 
 // Pseudo multiply/divide instruction with explicit accumulator register
 // operands.
 class MultDivPseudo<Instruction RealInst, RegisterClass R0, RegisterOperand R1,
                     SDPatternOperator OpNode, InstrItinClass Itin,
                     bit IsComm = 1, bit HasSideEffects = 0,
                     bit UsesCustomInserter = 0> :
   PseudoSE<(outs R0:$ac), (ins R1:$rs, R1:$rt),
            [(set R0:$ac, (OpNode R1:$rs, R1:$rt))], Itin>,
   PseudoInstExpansion<(RealInst R1:$rs, R1:$rt)> {
   let isCommutable = IsComm;
   let hasSideEffects = HasSideEffects;
   let usesCustomInserter = UsesCustomInserter;
 }
 
 // Pseudo multiply add/sub instruction with explicit accumulator register
 // operands.
 class MAddSubPseudo<Instruction RealInst, SDPatternOperator OpNode,
                     InstrItinClass itin>
   : PseudoSE<(outs ACC64:$ac),
              (ins GPR32Opnd:$rs, GPR32Opnd:$rt, ACC64:$acin),
              [(set ACC64:$ac,
               (OpNode GPR32Opnd:$rs, GPR32Opnd:$rt, ACC64:$acin))],
              itin>,
     PseudoInstExpansion<(RealInst GPR32Opnd:$rs, GPR32Opnd:$rt)> {
   string Constraints = "$acin = $ac";
 }
 
 class Div<string opstr, InstrItinClass itin, RegisterOperand RO,
           list<Register> DefRegs> :
   InstSE<(outs), (ins RO:$rs, RO:$rt), !strconcat(opstr, "\t$$zero, $rs, $rt"),
          [], itin, FrmR, opstr> {
   let Defs = DefRegs;
 }
 
 // Move from Hi/Lo
 class PseudoMFLOHI<RegisterClass DstRC, RegisterClass SrcRC, SDNode OpNode>
   : PseudoSE<(outs DstRC:$rd), (ins SrcRC:$hilo),
              [(set DstRC:$rd, (OpNode SrcRC:$hilo))], II_MFHI_MFLO>;
 
 class MoveFromLOHI<string opstr, RegisterOperand RO, Register UseReg>:
   InstSE<(outs RO:$rd), (ins), !strconcat(opstr, "\t$rd"), [], II_MFHI_MFLO,
          FrmR, opstr> {
   let Uses = [UseReg];
   let hasSideEffects = 0;
   let isMoveReg = 1;
 }
 
 class PseudoMTLOHI<RegisterClass DstRC, RegisterClass SrcRC>
   : PseudoSE<(outs DstRC:$lohi), (ins SrcRC:$lo, SrcRC:$hi),
              [(set DstRC:$lohi, (MipsMTLOHI SrcRC:$lo, SrcRC:$hi))],
              II_MTHI_MTLO>;
 
 class MoveToLOHI<string opstr, RegisterOperand RO, list<Register> DefRegs>:
   InstSE<(outs), (ins RO:$rs), !strconcat(opstr, "\t$rs"), [], II_MTHI_MTLO,
   FrmR, opstr> {
   let Defs = DefRegs;
   let hasSideEffects = 0;
   let isMoveReg = 1;
 }
 
 class EffectiveAddress<string opstr, RegisterOperand RO> :
   InstSE<(outs RO:$rt), (ins mem_ea:$addr), !strconcat(opstr, "\t$rt, $addr"),
          [(set RO:$rt, addr:$addr)], II_ADDIU, FrmI,
          !strconcat(opstr, "_lea")> {
   let isCodeGenOnly = 1;
   let hasNoSchedulingInfo = 1;
   let DecoderMethod = "DecodeMem";
 }
 
 // Count Leading Ones/Zeros in Word
 class CountLeading0<string opstr, RegisterOperand RO,
                   InstrItinClass itin = NoItinerary>:
   InstSE<(outs RO:$rd), (ins RO:$rs), !strconcat(opstr, "\t$rd, $rs"),
          [(set RO:$rd, (ctlz RO:$rs))], itin, FrmR, opstr>;
 
 class CountLeading1<string opstr, RegisterOperand RO,
                   InstrItinClass itin = NoItinerary>:
   InstSE<(outs RO:$rd), (ins RO:$rs), !strconcat(opstr, "\t$rd, $rs"),
          [(set RO:$rd, (ctlz (not RO:$rs)))], itin, FrmR, opstr>;
 
 // Sign Extend in Register.
 class SignExtInReg<string opstr, ValueType vt, RegisterOperand RO,
                    InstrItinClass itin> :
   InstSE<(outs RO:$rd), (ins RO:$rt), !strconcat(opstr, "\t$rd, $rt"),
          [(set RO:$rd, (sext_inreg RO:$rt, vt))], itin, FrmR, opstr>;
 
 // Subword Swap
 class SubwordSwap<string opstr, RegisterOperand RO,
                   InstrItinClass itin = NoItinerary>:
   InstSE<(outs RO:$rd), (ins RO:$rt), !strconcat(opstr, "\t$rd, $rt"), [], itin,
          FrmR, opstr> {
   let hasSideEffects = 0;
 }
 
 // Read Hardware
 class ReadHardware<RegisterOperand CPURegOperand, RegisterOperand RO> :
   InstSE<(outs CPURegOperand:$rt), (ins RO:$rd, uimm8:$sel),
          "rdhwr\t$rt, $rd, $sel", [], II_RDHWR, FrmR, "rdhwr">;
 
 // Ext and Ins
 class ExtBase<string opstr, RegisterOperand RO, Operand PosOpnd,
               Operand SizeOpnd, PatFrag PosImm, PatFrag SizeImm,
               SDPatternOperator Op = null_frag> :
   InstSE<(outs RO:$rt), (ins RO:$rs, PosOpnd:$pos, SizeOpnd:$size),
          !strconcat(opstr, "\t$rt, $rs, $pos, $size"),
          [(set RO:$rt, (Op RO:$rs, PosImm:$pos, SizeImm:$size))], II_EXT,
          FrmR, opstr>;
 
 // 'ins' and its' 64 bit variants are matched by C++ code.
 class InsBase<string opstr, RegisterOperand RO, Operand PosOpnd,
               Operand SizeOpnd, PatFrag PosImm, PatFrag SizeImm>:
   InstSE<(outs RO:$rt), (ins RO:$rs, PosOpnd:$pos, SizeOpnd:$size, RO:$src),
          !strconcat(opstr, "\t$rt, $rs, $pos, $size"),
          [(set RO:$rt, (null_frag RO:$rs, PosImm:$pos, SizeImm:$size,
                                   RO:$src))],
          II_INS, FrmR, opstr> {
   let Constraints = "$src = $rt";
 }
 
 // Atomic instructions with 2 source operands (ATOMIC_SWAP & ATOMIC_LOAD_*).
 class Atomic2Ops<PatFrag Op, RegisterClass DRC> :
   PseudoSE<(outs DRC:$dst), (ins PtrRC:$ptr, DRC:$incr),
            [(set DRC:$dst, (Op iPTR:$ptr, DRC:$incr))]> {
   let hasNoSchedulingInfo = 1;
 }
 
 class Atomic2OpsPostRA<RegisterClass RC> :
   PseudoSE<(outs RC:$dst), (ins PtrRC:$ptr, RC:$incr), []> {
   let mayLoad = 1;
   let mayStore = 1;
 }
 
 class Atomic2OpsSubwordPostRA<RegisterClass RC> :
   PseudoSE<(outs RC:$dst), (ins PtrRC:$ptr, RC:$incr, RC:$mask, RC:$mask2,
                                 RC:$shiftamnt), []>;
 
 // Atomic Compare & Swap.
 // Atomic compare and swap is lowered into two stages. The first stage happens
 // during ISelLowering, which produces the PostRA version of this instruction.
 class AtomicCmpSwap<PatFrag Op, RegisterClass DRC> :
   PseudoSE<(outs DRC:$dst), (ins PtrRC:$ptr, DRC:$cmp, DRC:$swap),
            [(set DRC:$dst, (Op iPTR:$ptr, DRC:$cmp, DRC:$swap))]> {
   let hasNoSchedulingInfo = 1;
 }
 
 class AtomicCmpSwapPostRA<RegisterClass RC> :
   PseudoSE<(outs RC:$dst), (ins PtrRC:$ptr, RC:$cmp, RC:$swap), []> {
   let mayLoad = 1;
   let mayStore = 1;
 }
 
 class AtomicCmpSwapSubwordPostRA<RegisterClass RC> :
   PseudoSE<(outs RC:$dst), (ins PtrRC:$ptr, RC:$mask, RC:$ShiftCmpVal,
                                 RC:$mask2, RC:$ShiftNewVal, RC:$ShiftAmt), []> {
   let mayLoad = 1;
   let mayStore = 1;
 }
 
 class LLBase<string opstr, RegisterOperand RO, DAGOperand MO = mem> :
   InstSE<(outs RO:$rt), (ins MO:$addr), !strconcat(opstr, "\t$rt, $addr"),
          [], II_LL, FrmI, opstr> {
   let DecoderMethod = "DecodeMem";
   let mayLoad = 1;
 }
 
 class SCBase<string opstr, RegisterOperand RO> :
   InstSE<(outs RO:$dst), (ins RO:$rt, mem:$addr),
          !strconcat(opstr, "\t$rt, $addr"), [], II_SC, FrmI> {
   let DecoderMethod = "DecodeMem";
   let mayStore = 1;
   let Constraints = "$rt = $dst";
 }
 
 class MFC3OP<string asmstr, RegisterOperand RO, RegisterOperand RD,
              InstrItinClass itin> :
   InstSE<(outs RO:$rt), (ins RD:$rd, uimm3:$sel),
          !strconcat(asmstr, "\t$rt, $rd, $sel"), [], itin, FrmFR> {
   let BaseOpcode = asmstr;
 }
 
 class MTC3OP<string asmstr, RegisterOperand RO, RegisterOperand RD,
              InstrItinClass itin> :
   InstSE<(outs RO:$rd), (ins RD:$rt, uimm3:$sel),
          !strconcat(asmstr, "\t$rt, $rd, $sel"), [], itin, FrmFR> {
   let BaseOpcode = asmstr;
 }
 
 class TrapBase<Instruction RealInst>
   : PseudoSE<(outs), (ins), [(trap)], II_TRAP>,
     PseudoInstExpansion<(RealInst 0, 0)> {
   let mayStore = 0;
   let mayLoad = 0;
   let hasSideEffects = 1;
   let isTrap = 1;
   let isCodeGenOnly = 1;
 }
 
 //===----------------------------------------------------------------------===//
 // Pseudo instructions
 //===----------------------------------------------------------------------===//
 
 // Return RA.
 let isReturn=1, isTerminator=1, isBarrier=1, hasCtrlDep=1, isCTI=1 in {
   let hasDelaySlot=1 in
   def RetRA : PseudoSE<(outs), (ins), [(MipsRet)]>;
 
   let hasSideEffects=1 in
   def ERet : PseudoSE<(outs), (ins), [(MipsERet)]>;
 }
 
 let Defs = [SP], Uses = [SP], hasSideEffects = 1, hasNoSchedulingInfo = 1 in {
 def ADJCALLSTACKDOWN : MipsPseudo<(outs), (ins i32imm:$amt1, i32imm:$amt2),
                                   [(callseq_start timm:$amt1, timm:$amt2)]>;
 def ADJCALLSTACKUP   : MipsPseudo<(outs), (ins i32imm:$amt1, i32imm:$amt2),
                                   [(callseq_end timm:$amt1, timm:$amt2)]>;
 }
 
 let usesCustomInserter = 1 in {
   def ATOMIC_LOAD_ADD_I8   : Atomic2Ops<atomic_load_add_8, GPR32>;
   def ATOMIC_LOAD_ADD_I16  : Atomic2Ops<atomic_load_add_16, GPR32>;
   def ATOMIC_LOAD_ADD_I32  : Atomic2Ops<atomic_load_add_32, GPR32>;
   def ATOMIC_LOAD_SUB_I8   : Atomic2Ops<atomic_load_sub_8, GPR32>;
   def ATOMIC_LOAD_SUB_I16  : Atomic2Ops<atomic_load_sub_16, GPR32>;
   def ATOMIC_LOAD_SUB_I32  : Atomic2Ops<atomic_load_sub_32, GPR32>;
   def ATOMIC_LOAD_AND_I8   : Atomic2Ops<atomic_load_and_8, GPR32>;
   def ATOMIC_LOAD_AND_I16  : Atomic2Ops<atomic_load_and_16, GPR32>;
   def ATOMIC_LOAD_AND_I32  : Atomic2Ops<atomic_load_and_32, GPR32>;
   def ATOMIC_LOAD_OR_I8    : Atomic2Ops<atomic_load_or_8, GPR32>;
   def ATOMIC_LOAD_OR_I16   : Atomic2Ops<atomic_load_or_16, GPR32>;
   def ATOMIC_LOAD_OR_I32   : Atomic2Ops<atomic_load_or_32, GPR32>;
   def ATOMIC_LOAD_XOR_I8   : Atomic2Ops<atomic_load_xor_8, GPR32>;
   def ATOMIC_LOAD_XOR_I16  : Atomic2Ops<atomic_load_xor_16, GPR32>;
   def ATOMIC_LOAD_XOR_I32  : Atomic2Ops<atomic_load_xor_32, GPR32>;
   def ATOMIC_LOAD_NAND_I8  : Atomic2Ops<atomic_load_nand_8, GPR32>;
   def ATOMIC_LOAD_NAND_I16 : Atomic2Ops<atomic_load_nand_16, GPR32>;
   def ATOMIC_LOAD_NAND_I32 : Atomic2Ops<atomic_load_nand_32, GPR32>;
 
   def ATOMIC_SWAP_I8       : Atomic2Ops<atomic_swap_8, GPR32>;
   def ATOMIC_SWAP_I16      : Atomic2Ops<atomic_swap_16, GPR32>;
   def ATOMIC_SWAP_I32      : Atomic2Ops<atomic_swap_32, GPR32>;
 
   def ATOMIC_CMP_SWAP_I8   : AtomicCmpSwap<atomic_cmp_swap_8, GPR32>;
   def ATOMIC_CMP_SWAP_I16  : AtomicCmpSwap<atomic_cmp_swap_16, GPR32>;
   def ATOMIC_CMP_SWAP_I32  : AtomicCmpSwap<atomic_cmp_swap_32, GPR32>;
 
   def ATOMIC_LOAD_MIN_I8   : Atomic2Ops<atomic_load_min_8, GPR32>;
   def ATOMIC_LOAD_MIN_I16  : Atomic2Ops<atomic_load_min_16, GPR32>;
   def ATOMIC_LOAD_MIN_I32  : Atomic2Ops<atomic_load_min_32, GPR32>;
   def ATOMIC_LOAD_MAX_I8   : Atomic2Ops<atomic_load_max_8, GPR32>;
   def ATOMIC_LOAD_MAX_I16  : Atomic2Ops<atomic_load_max_16, GPR32>;
   def ATOMIC_LOAD_MAX_I32  : Atomic2Ops<atomic_load_max_32, GPR32>;
   def ATOMIC_LOAD_UMIN_I8  : Atomic2Ops<atomic_load_umin_8, GPR32>;
   def ATOMIC_LOAD_UMIN_I16 : Atomic2Ops<atomic_load_umin_16, GPR32>;
   def ATOMIC_LOAD_UMIN_I32 : Atomic2Ops<atomic_load_umin_32, GPR32>;
   def ATOMIC_LOAD_UMAX_I8  : Atomic2Ops<atomic_load_umax_8, GPR32>;
   def ATOMIC_LOAD_UMAX_I16 : Atomic2Ops<atomic_load_umax_16, GPR32>;
   def ATOMIC_LOAD_UMAX_I32 : Atomic2Ops<atomic_load_umax_32, GPR32>;
 }
 
 def ATOMIC_LOAD_ADD_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_ADD_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_ADD_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
 def ATOMIC_LOAD_SUB_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_SUB_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_SUB_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
 def ATOMIC_LOAD_AND_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_AND_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_AND_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
 def ATOMIC_LOAD_OR_I8_POSTRA    : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_OR_I16_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_OR_I32_POSTRA   : Atomic2OpsPostRA<GPR32>;
 def ATOMIC_LOAD_XOR_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_XOR_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_XOR_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
 def ATOMIC_LOAD_NAND_I8_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_NAND_I16_POSTRA : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_NAND_I32_POSTRA : Atomic2OpsPostRA<GPR32>;
 
 def ATOMIC_SWAP_I8_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_SWAP_I16_POSTRA : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_SWAP_I32_POSTRA : Atomic2OpsPostRA<GPR32>;
 
 def ATOMIC_CMP_SWAP_I8_POSTRA : AtomicCmpSwapSubwordPostRA<GPR32>;
 def ATOMIC_CMP_SWAP_I16_POSTRA : AtomicCmpSwapSubwordPostRA<GPR32>;
 def ATOMIC_CMP_SWAP_I32_POSTRA : AtomicCmpSwapPostRA<GPR32>;
 
 def ATOMIC_LOAD_MIN_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_MIN_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_MIN_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
 def ATOMIC_LOAD_MAX_I8_POSTRA   : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_MAX_I16_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_MAX_I32_POSTRA  : Atomic2OpsPostRA<GPR32>;
 def ATOMIC_LOAD_UMIN_I8_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_UMIN_I16_POSTRA : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_UMIN_I32_POSTRA : Atomic2OpsPostRA<GPR32>;
 def ATOMIC_LOAD_UMAX_I8_POSTRA  : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_UMAX_I16_POSTRA : Atomic2OpsSubwordPostRA<GPR32>;
 def ATOMIC_LOAD_UMAX_I32_POSTRA : Atomic2OpsPostRA<GPR32>;
 
 /// Pseudo instructions for loading and storing accumulator registers.
 let isPseudo = 1, isCodeGenOnly = 1, hasNoSchedulingInfo = 1 in {
   def LOAD_ACC64  : Load<"", ACC64>;
   def STORE_ACC64 : Store<"", ACC64>;
 }
 
 // We need these two pseudo instructions to avoid offset calculation for long
 // branches.  See the comment in file MipsLongBranch.cpp for detailed
 // explanation.
 
 // Expands to: lui $dst, %highest/%higher/%hi/%lo($tgt - $baltgt)
 def LONG_BRANCH_LUi : PseudoSE<(outs GPR32Opnd:$dst),
   (ins brtarget:$tgt, brtarget:$baltgt), []> {
   bit hasNoSchedulingInfo = 1;
 }
 // Expands to: lui $dst, highest/%higher/%hi/%lo($tgt)
 def LONG_BRANCH_LUi2Op : PseudoSE<(outs GPR32Opnd:$dst),
   (ins brtarget:$tgt), []> {
   bit hasNoSchedulingInfo = 1;
 }
 
 // Expands to: addiu $dst, $src, %highest/%higher/%hi/%lo($tgt - $baltgt)
 def LONG_BRANCH_ADDiu : PseudoSE<(outs GPR32Opnd:$dst),
   (ins GPR32Opnd:$src, brtarget:$tgt, brtarget:$baltgt), []> {
   bit hasNoSchedulingInfo = 1;
 }
 // Expands to: addiu $dst, $src, %highest/%higher/%hi/%lo($tgt)
 def LONG_BRANCH_ADDiu2Op : PseudoSE<(outs GPR32Opnd:$dst),
   (ins GPR32Opnd:$src, brtarget:$tgt), []> {
   bit hasNoSchedulingInfo = 1;
 }
 
 //===----------------------------------------------------------------------===//
 // Instruction definition
 //===----------------------------------------------------------------------===//
 //===----------------------------------------------------------------------===//
 // MipsI Instructions
 //===----------------------------------------------------------------------===//
 
 /// Arithmetic Instructions (ALU Immediate)
 let AdditionalPredicates = [NotInMicroMips] in {
   def ADDiu : MMRel, StdMMR6Rel, ArithLogicI<"addiu", simm16_relaxed, GPR32Opnd,
                                              II_ADDIU, imm32SExt16, add>,
               ADDI_FM<0x9>, IsAsCheapAsAMove, ISA_MIPS1;
 
   def ANDi : MMRel, StdMMR6Rel,
              ArithLogicI<"andi", uimm16, GPR32Opnd, II_ANDI, imm32ZExt16, and>,
              ADDI_FM<0xc>, ISA_MIPS1;
   def ORi  : MMRel, StdMMR6Rel,
              ArithLogicI<"ori", uimm16, GPR32Opnd, II_ORI, imm32ZExt16, or>,
              ADDI_FM<0xd>, ISA_MIPS1;
   def XORi : MMRel, StdMMR6Rel,
              ArithLogicI<"xori", uimm16, GPR32Opnd, II_XORI, imm32ZExt16, xor>,
              ADDI_FM<0xe>, ISA_MIPS1;
   def ADDi  : MMRel, ArithLogicI<"addi", simm16_relaxed, GPR32Opnd, II_ADDI>,
               ADDI_FM<0x8>, ISA_MIPS1_NOT_32R6_64R6;
   def SLTi  : MMRel, SetCC_I<"slti", setlt, simm16, immSExt16, GPR32Opnd>,
               SLTI_FM<0xa>, ISA_MIPS1;
   def SLTiu : MMRel, SetCC_I<"sltiu", setult, simm16, immSExt16, GPR32Opnd>,
               SLTI_FM<0xb>, ISA_MIPS1;
 
   def LUi   : MMRel, LoadUpper<"lui", GPR32Opnd, uimm16_relaxed>, LUI_FM,
               ISA_MIPS1;
 
   /// Arithmetic Instructions (3-Operand, R-Type)
   def ADDu : MMRel, StdMMR6Rel, ArithLogicR<"addu", GPR32Opnd, 1, II_ADDU, add>,
              ADD_FM<0, 0x21>, ISA_MIPS1;
   def SUBu : MMRel, StdMMR6Rel, ArithLogicR<"subu", GPR32Opnd, 0, II_SUBU, sub>,
              ADD_FM<0, 0x23>, ISA_MIPS1;
 
   let Defs = [HI0, LO0] in
     def MUL   : MMRel, ArithLogicR<"mul", GPR32Opnd, 1, II_MUL, mul>,
                 ADD_FM<0x1c, 2>, ISA_MIPS32_NOT_32R6_64R6;
 
   def ADD   : MMRel, StdMMR6Rel, ArithLogicR<"add", GPR32Opnd, 1, II_ADD>,
               ADD_FM<0, 0x20>, ISA_MIPS1;
   def SUB   : MMRel, StdMMR6Rel, ArithLogicR<"sub", GPR32Opnd, 0, II_SUB>,
               ADD_FM<0, 0x22>, ISA_MIPS1;
 
   def SLT   : MMRel, SetCC_R<"slt", setlt, GPR32Opnd>, ADD_FM<0, 0x2a>,
               ISA_MIPS1;
   def SLTu  : MMRel, SetCC_R<"sltu", setult, GPR32Opnd>, ADD_FM<0, 0x2b>,
               ISA_MIPS1;
   def AND   : MMRel, StdMMR6Rel, ArithLogicR<"and", GPR32Opnd, 1, II_AND, and>,
               ADD_FM<0, 0x24>, ISA_MIPS1;
   def OR    : MMRel, StdMMR6Rel, ArithLogicR<"or", GPR32Opnd, 1, II_OR, or>,
               ADD_FM<0, 0x25>, ISA_MIPS1;
   def XOR   : MMRel, StdMMR6Rel, ArithLogicR<"xor", GPR32Opnd, 1, II_XOR, xor>,
               ADD_FM<0, 0x26>, ISA_MIPS1;
   def NOR   : MMRel, StdMMR6Rel, LogicNOR<"nor", GPR32Opnd>, ADD_FM<0, 0x27>,
               ISA_MIPS1;
 }
 
 let AdditionalPredicates = [NotInMicroMips] in {
   /// Shift Instructions
   def SLL  : MMRel, shift_rotate_imm<"sll", uimm5, GPR32Opnd, II_SLL, shl,
                                      immZExt5>, SRA_FM<0, 0>, ISA_MIPS1;
   def SRL  : MMRel, shift_rotate_imm<"srl", uimm5, GPR32Opnd, II_SRL, srl,
                                      immZExt5>, SRA_FM<2, 0>, ISA_MIPS1;
   def SRA  : MMRel, shift_rotate_imm<"sra", uimm5, GPR32Opnd, II_SRA, sra,
                                      immZExt5>, SRA_FM<3, 0>, ISA_MIPS1;
   def SLLV : MMRel, shift_rotate_reg<"sllv", GPR32Opnd, II_SLLV, shl>,
              SRLV_FM<4, 0>, ISA_MIPS1;
   def SRLV : MMRel, shift_rotate_reg<"srlv", GPR32Opnd, II_SRLV, srl>,
              SRLV_FM<6, 0>, ISA_MIPS1;
   def SRAV : MMRel, shift_rotate_reg<"srav", GPR32Opnd, II_SRAV, sra>,
              SRLV_FM<7, 0>, ISA_MIPS1;
 
   // Rotate Instructions
   def ROTR  : MMRel, shift_rotate_imm<"rotr", uimm5, GPR32Opnd, II_ROTR, rotr,
                                       immZExt5>,
               SRA_FM<2, 1>, ISA_MIPS32R2;
   def ROTRV : MMRel, shift_rotate_reg<"rotrv", GPR32Opnd, II_ROTRV, rotr>,
               SRLV_FM<6, 1>, ISA_MIPS32R2;
 }
 
 /// Load and Store Instructions
 ///  aligned
 let AdditionalPredicates = [NotInMicroMips] in {
   def LB  : LoadMemory<"lb", GPR32Opnd, mem_simmptr, sextloadi8, II_LB>, MMRel,
             LW_FM<0x20>, ISA_MIPS1;
   def LBu : LoadMemory<"lbu", GPR32Opnd, mem_simmptr, zextloadi8, II_LBU,
                        addrDefault>, MMRel, LW_FM<0x24>, ISA_MIPS1;
   def LH  : LoadMemory<"lh", GPR32Opnd, mem_simmptr, sextloadi16, II_LH,
                        addrDefault>, MMRel, LW_FM<0x21>, ISA_MIPS1;
   def LHu : LoadMemory<"lhu", GPR32Opnd, mem_simmptr, zextloadi16, II_LHU>,
             MMRel, LW_FM<0x25>, ISA_MIPS1;
   def LW  : StdMMR6Rel, Load<"lw", GPR32Opnd, load, II_LW, addrDefault>, MMRel,
             LW_FM<0x23>, ISA_MIPS1;
   def SB  : StdMMR6Rel, Store<"sb", GPR32Opnd, truncstorei8, II_SB>, MMRel,
             LW_FM<0x28>, ISA_MIPS1;
   def SH  : Store<"sh", GPR32Opnd, truncstorei16, II_SH>, MMRel, LW_FM<0x29>,
             ISA_MIPS1;
   def SW  : StdMMR6Rel, Store<"sw", GPR32Opnd, store, II_SW>,
             MMRel, LW_FM<0x2b>, ISA_MIPS1;
 }
 
 /// load/store left/right
 let AdditionalPredicates = [NotInMicroMips] in {
 def LWL : MMRel, LoadLeftRight<"lwl", MipsLWL, GPR32Opnd, II_LWL>, LW_FM<0x22>,
           ISA_MIPS1_NOT_32R6_64R6;
 def LWR : MMRel, LoadLeftRight<"lwr", MipsLWR, GPR32Opnd, II_LWR>, LW_FM<0x26>,
           ISA_MIPS1_NOT_32R6_64R6;
 def SWL : MMRel, StoreLeftRight<"swl", MipsSWL, GPR32Opnd, II_SWL>, LW_FM<0x2a>,
           ISA_MIPS1_NOT_32R6_64R6;
 def SWR : MMRel, StoreLeftRight<"swr", MipsSWR, GPR32Opnd, II_SWR>, LW_FM<0x2e>,
           ISA_MIPS1_NOT_32R6_64R6;
 
 // COP2 Memory Instructions
 def LWC2 : StdMMR6Rel, LW_FT2<"lwc2", COP2Opnd, II_LWC2, load>, LW_FM<0x32>,
            ISA_MIPS1_NOT_32R6_64R6;
 def SWC2 : StdMMR6Rel, SW_FT2<"swc2", COP2Opnd, II_SWC2, store>,
            LW_FM<0x3a>, ISA_MIPS1_NOT_32R6_64R6;
 def LDC2 : StdMMR6Rel, LW_FT2<"ldc2", COP2Opnd, II_LDC2, load>, LW_FM<0x36>,
            ISA_MIPS2_NOT_32R6_64R6;
 def SDC2 : StdMMR6Rel, SW_FT2<"sdc2", COP2Opnd, II_SDC2, store>,
            LW_FM<0x3e>, ISA_MIPS2_NOT_32R6_64R6;
 
 // COP3 Memory Instructions
 let DecoderNamespace = "COP3_" in {
   def LWC3 : LW_FT3<"lwc3", COP3Opnd, II_LWC3, load>, LW_FM<0x33>,
              ISA_MIPS1_NOT_32R6_64R6, NOT_ASE_CNMIPS;
   def SWC3 : SW_FT3<"swc3", COP3Opnd, II_SWC3, store>, LW_FM<0x3b>,
              ISA_MIPS1_NOT_32R6_64R6, NOT_ASE_CNMIPS;
   def LDC3 : LW_FT3<"ldc3", COP3Opnd, II_LDC3, load>, LW_FM<0x37>,
              ISA_MIPS2, NOT_ASE_CNMIPS;
   def SDC3 : SW_FT3<"sdc3", COP3Opnd, II_SDC3, store>, LW_FM<0x3f>,
              ISA_MIPS2, NOT_ASE_CNMIPS;
 }
 
   def SYNC : MMRel, StdMMR6Rel, SYNC_FT<"sync">, SYNC_FM, ISA_MIPS2;
   def SYNCI : MMRel, StdMMR6Rel, SYNCI_FT<"synci", mem_simm16>, SYNCI_FM,
               ISA_MIPS32R2;
 }
 
 let AdditionalPredicates = [NotInMicroMips] in {
   def TEQ : MMRel, TEQ_FT<"teq", GPR32Opnd, uimm10, II_TEQ>, TEQ_FM<0x34>,
             ISA_MIPS2;
   def TGE : MMRel, TEQ_FT<"tge", GPR32Opnd, uimm10, II_TGE>, TEQ_FM<0x30>,
             ISA_MIPS2;
   def TGEU : MMRel, TEQ_FT<"tgeu", GPR32Opnd, uimm10, II_TGEU>, TEQ_FM<0x31>,
              ISA_MIPS2;
   def TLT : MMRel, TEQ_FT<"tlt", GPR32Opnd, uimm10, II_TLT>, TEQ_FM<0x32>,
             ISA_MIPS2;
   def TLTU : MMRel, TEQ_FT<"tltu", GPR32Opnd, uimm10, II_TLTU>, TEQ_FM<0x33>,
             ISA_MIPS2;
   def TNE : MMRel, TEQ_FT<"tne", GPR32Opnd, uimm10, II_TNE>, TEQ_FM<0x36>,
             ISA_MIPS2;
 
   def TEQI : MMRel, TEQI_FT<"teqi", GPR32Opnd, II_TEQI>, TEQI_FM<0xc>,
              ISA_MIPS2_NOT_32R6_64R6;
   def TGEI : MMRel, TEQI_FT<"tgei", GPR32Opnd, II_TGEI>, TEQI_FM<0x8>,
              ISA_MIPS2_NOT_32R6_64R6;
   def TGEIU : MMRel, TEQI_FT<"tgeiu", GPR32Opnd, II_TGEIU>, TEQI_FM<0x9>,
               ISA_MIPS2_NOT_32R6_64R6;
   def TLTI : MMRel, TEQI_FT<"tlti", GPR32Opnd, II_TLTI>, TEQI_FM<0xa>,
              ISA_MIPS2_NOT_32R6_64R6;
   def TTLTIU : MMRel, TEQI_FT<"tltiu", GPR32Opnd, II_TTLTIU>, TEQI_FM<0xb>,
                ISA_MIPS2_NOT_32R6_64R6;
   def TNEI : MMRel, TEQI_FT<"tnei", GPR32Opnd, II_TNEI>, TEQI_FM<0xe>,
              ISA_MIPS2_NOT_32R6_64R6;
 }
 
 let AdditionalPredicates = [NotInMicroMips] in {
   def BREAK : MMRel, StdMMR6Rel, BRK_FT<"break">, BRK_FM<0xd>, ISA_MIPS1;
   def SYSCALL : MMRel, SYS_FT<"syscall", uimm20, II_SYSCALL>, SYS_FM<0xc>,
                 ISA_MIPS1;
   def TRAP : TrapBase<BREAK>, ISA_MIPS1;
   def SDBBP : MMRel, SYS_FT<"sdbbp", uimm20, II_SDBBP>, SDBBP_FM,
               ISA_MIPS32_NOT_32R6_64R6;
 
   def ERET : MMRel, ER_FT<"eret", II_ERET>, ER_FM<0x18, 0x0>, INSN_MIPS3_32;
   def ERETNC : MMRel, ER_FT<"eretnc", II_ERETNC>, ER_FM<0x18, 0x1>,
                ISA_MIPS32R5;
   def DERET : MMRel, ER_FT<"deret", II_DERET>, ER_FM<0x1f, 0x0>, ISA_MIPS32;
 
   def EI : MMRel, StdMMR6Rel, DEI_FT<"ei", GPR32Opnd, II_EI>, EI_FM<1>,
            ISA_MIPS32R2;
   def DI : MMRel, StdMMR6Rel, DEI_FT<"di", GPR32Opnd, II_DI>, EI_FM<0>,
            ISA_MIPS32R2;
 
   def WAIT : MMRel, StdMMR6Rel, WAIT_FT<"wait">, WAIT_FM, INSN_MIPS3_32;
 }
 
 let AdditionalPredicates = [NotInMicroMips] in {
 /// Load-linked, Store-conditional
 def LL : LLBase<"ll", GPR32Opnd>, LW_FM<0x30>, PTR_32, ISA_MIPS2_NOT_32R6_64R6;
 def SC : SCBase<"sc", GPR32Opnd>, LW_FM<0x38>, PTR_32, ISA_MIPS2_NOT_32R6_64R6;
 }
 /// Jump and Branch Instructions
 let AdditionalPredicates = [NotInMicroMips, RelocNotPIC] in
 def J       : MMRel, JumpFJ<jmptarget, "j", br, bb, "j">, FJ<2>,
               IsBranch, ISA_MIPS1;
 
 let AdditionalPredicates = [NotInMicroMips] in {
 def JR      : MMRel, IndirectBranch<"jr", GPR32Opnd>, MTLO_FM<8>,
               ISA_MIPS1_NOT_32R6_64R6;
 def BEQ     : MMRel, CBranch<"beq", brtarget, seteq, GPR32Opnd>, BEQ_FM<4>,
               ISA_MIPS1;
 def BEQL    : MMRel, CBranchLikely<"beql", brtarget, GPR32Opnd>,
               BEQ_FM<20>, ISA_MIPS2_NOT_32R6_64R6;
 def BNE     : MMRel, CBranch<"bne", brtarget, setne, GPR32Opnd>, BEQ_FM<5>,
               ISA_MIPS1;
 def BNEL    : MMRel, CBranchLikely<"bnel", brtarget, GPR32Opnd>,
               BEQ_FM<21>, ISA_MIPS2_NOT_32R6_64R6;
 def BGEZ    : MMRel, CBranchZero<"bgez", brtarget, setge, GPR32Opnd>,
               BGEZ_FM<1, 1>, ISA_MIPS1;
 def BGEZL   : MMRel, CBranchZeroLikely<"bgezl", brtarget, GPR32Opnd>,
               BGEZ_FM<1, 3>, ISA_MIPS2_NOT_32R6_64R6;
 def BGTZ    : MMRel, CBranchZero<"bgtz", brtarget, setgt, GPR32Opnd>,
               BGEZ_FM<7, 0>, ISA_MIPS1;
 def BGTZL   : MMRel, CBranchZeroLikely<"bgtzl", brtarget, GPR32Opnd>,
               BGEZ_FM<23, 0>, ISA_MIPS2_NOT_32R6_64R6;
 def BLEZ    : MMRel, CBranchZero<"blez", brtarget, setle, GPR32Opnd>,
               BGEZ_FM<6, 0>, ISA_MIPS1;
 def BLEZL   : MMRel, CBranchZeroLikely<"blezl", brtarget, GPR32Opnd>,
               BGEZ_FM<22, 0>, ISA_MIPS2_NOT_32R6_64R6;
 def BLTZ    : MMRel, CBranchZero<"bltz", brtarget, setlt, GPR32Opnd>,
               BGEZ_FM<1, 0>, ISA_MIPS1;
 def BLTZL   : MMRel, CBranchZeroLikely<"bltzl", brtarget, GPR32Opnd>,
               BGEZ_FM<1, 2>, ISA_MIPS2_NOT_32R6_64R6;
 def B       : UncondBranch<BEQ, brtarget>, ISA_MIPS1;
 
 def JAL  : MMRel, JumpLink<"jal", calltarget>, FJ<3>, ISA_MIPS1;
 
 }
 
 let AdditionalPredicates = [NotInMicroMips, NoIndirectJumpGuards] in {
   def JALR : JumpLinkReg<"jalr", GPR32Opnd>, JALR_FM, ISA_MIPS1;
   def JALRPseudo : JumpLinkRegPseudo<GPR32Opnd, JALR, RA>, ISA_MIPS1;
 }
 
 let AdditionalPredicates = [NotInMicroMips] in {
   def JALX : MMRel, JumpLink<"jalx", calltarget>, FJ<0x1D>,
              ISA_MIPS32_NOT_32R6_64R6;
   def BGEZAL : MMRel, BGEZAL_FT<"bgezal", brtarget, GPR32Opnd>, BGEZAL_FM<0x11>,
                ISA_MIPS1_NOT_32R6_64R6;
   def BGEZALL : MMRel, BGEZAL_FT<"bgezall", brtarget, GPR32Opnd>,
                 BGEZAL_FM<0x13>, ISA_MIPS2_NOT_32R6_64R6;
   def BLTZAL : MMRel, BGEZAL_FT<"bltzal", brtarget, GPR32Opnd>, BGEZAL_FM<0x10>,
                ISA_MIPS1_NOT_32R6_64R6;
   def BLTZALL : MMRel, BGEZAL_FT<"bltzall", brtarget, GPR32Opnd>,
                 BGEZAL_FM<0x12>, ISA_MIPS2_NOT_32R6_64R6;
   def BAL_BR : BAL_BR_Pseudo<BGEZAL, brtarget>, ISA_MIPS1;
 }
 let AdditionalPredicates = [NotInMips16Mode, NotInMicroMips] in {
   def TAILCALL : TailCall<J, jmptarget>, ISA_MIPS1;
 }
 let AdditionalPredicates = [NotInMips16Mode, NotInMicroMips,
                             NoIndirectJumpGuards] in
   def TAILCALLREG : TailCallReg<JR, GPR32Opnd>, ISA_MIPS1_NOT_32R6_64R6;
 
 // Indirect branches are matched as PseudoIndirectBranch/PseudoIndirectBranch64
 // then are expanded to JR, JR64, JALR, or JALR64 depending on the ISA.
 class PseudoIndirectBranchBase<Instruction JumpInst, RegisterOperand RO> :
     MipsPseudo<(outs), (ins RO:$rs), [(brind RO:$rs)],
                II_IndirectBranchPseudo>,
     PseudoInstExpansion<(JumpInst RO:$rs)> {
   let isTerminator=1;
   let isBarrier=1;
   let hasDelaySlot = 1;
   let isBranch = 1;
   let isIndirectBranch = 1;
   bit isCTI = 1;
 }
 
 let AdditionalPredicates = [NotInMips16Mode, NotInMicroMips,
                             NoIndirectJumpGuards] in
   def PseudoIndirectBranch : PseudoIndirectBranchBase<JR, GPR32Opnd>,
                              ISA_MIPS1_NOT_32R6_64R6;
 
 // Return instructions are matched as a RetRA instruction, then are expanded
 // into PseudoReturn/PseudoReturn64 after register allocation. Finally,
 // MipsAsmPrinter expands this into JR, JR64, JALR, or JALR64 depending on the
 // ISA.
 class PseudoReturnBase<RegisterOperand RO> : MipsPseudo<(outs), (ins RO:$rs),
                                                         [], II_ReturnPseudo> {
   let isTerminator = 1;
   let isBarrier = 1;
   let hasDelaySlot = 1;
   let isReturn = 1;
   let isCodeGenOnly = 1;
   let hasCtrlDep = 1;
   let hasExtraSrcRegAllocReq = 1;
   bit isCTI = 1;
 }
 
 def PseudoReturn : PseudoReturnBase<GPR32Opnd>;
 
 // Exception handling related node and instructions.
 // The conversion sequence is:
 // ISD::EH_RETURN -> MipsISD::EH_RETURN ->
 // MIPSeh_return -> (stack change + indirect branch)
 //
 // MIPSeh_return takes the place of regular return instruction
 // but takes two arguments (V1, V0) which are used for storing
 // the offset and return address respectively.
 def SDT_MipsEHRET : SDTypeProfile<0, 2, [SDTCisInt<0>, SDTCisPtrTy<1>]>;
 
 def MIPSehret : SDNode<"MipsISD::EH_RETURN", SDT_MipsEHRET,
                       [SDNPHasChain, SDNPOptInGlue, SDNPVariadic]>;
 
 let Uses = [V0, V1], isTerminator = 1, isReturn = 1,
            isBarrier = 1, isCTI = 1, hasNoSchedulingInfo = 1 in {
   def MIPSeh_return32 : MipsPseudo<(outs), (ins GPR32:$spoff, GPR32:$dst),
                                    [(MIPSehret GPR32:$spoff, GPR32:$dst)]>;
   def MIPSeh_return64 : MipsPseudo<(outs), (ins GPR64:$spoff, GPR64:$dst),
                                    [(MIPSehret GPR64:$spoff, GPR64:$dst)]>;
 }
 
 /// Multiply and Divide Instructions.
 let AdditionalPredicates = [NotInMicroMips] in {
   def MULT  : MMRel, Mult<"mult", II_MULT, GPR32Opnd, [HI0, LO0]>,
               MULT_FM<0, 0x18>, ISA_MIPS1_NOT_32R6_64R6;
   def MULTu : MMRel, Mult<"multu", II_MULTU, GPR32Opnd, [HI0, LO0]>,
               MULT_FM<0, 0x19>, ISA_MIPS1_NOT_32R6_64R6;
   def SDIV  : MMRel, Div<"div", II_DIV, GPR32Opnd, [HI0, LO0]>,
               MULT_FM<0, 0x1a>, ISA_MIPS1_NOT_32R6_64R6;
   def UDIV  : MMRel, Div<"divu", II_DIVU, GPR32Opnd, [HI0, LO0]>,
               MULT_FM<0, 0x1b>, ISA_MIPS1_NOT_32R6_64R6;
   def MTHI : MMRel, MoveToLOHI<"mthi", GPR32Opnd, [HI0]>, MTLO_FM<0x11>,
              ISA_MIPS1_NOT_32R6_64R6;
   def MTLO : MMRel, MoveToLOHI<"mtlo", GPR32Opnd, [LO0]>, MTLO_FM<0x13>,
              ISA_MIPS1_NOT_32R6_64R6;
   def MFHI : MMRel, MoveFromLOHI<"mfhi", GPR32Opnd, AC0>, MFLO_FM<0x10>,
              ISA_MIPS1_NOT_32R6_64R6;
   def MFLO : MMRel, MoveFromLOHI<"mflo", GPR32Opnd, AC0>, MFLO_FM<0x12>,
              ISA_MIPS1_NOT_32R6_64R6;
 
   /// Sign Ext In Register Instructions.
   def SEB : MMRel, StdMMR6Rel, SignExtInReg<"seb", i8, GPR32Opnd, II_SEB>,
             SEB_FM<0x10, 0x20>, ISA_MIPS32R2;
   def SEH : MMRel, StdMMR6Rel, SignExtInReg<"seh", i16, GPR32Opnd, II_SEH>,
             SEB_FM<0x18, 0x20>, ISA_MIPS32R2;
 
   /// Count Leading
   def CLZ : MMRel, CountLeading0<"clz", GPR32Opnd, II_CLZ>, CLO_FM<0x20>,
             ISA_MIPS32_NOT_32R6_64R6;
   def CLO : MMRel, CountLeading1<"clo", GPR32Opnd, II_CLO>, CLO_FM<0x21>,
             ISA_MIPS32_NOT_32R6_64R6;
 
   /// Word Swap Bytes Within Halfwords
   def WSBH : MMRel, SubwordSwap<"wsbh", GPR32Opnd, II_WSBH>, SEB_FM<2, 0x20>,
              ISA_MIPS32R2;
 
   /// No operation.
   def NOP : PseudoSE<(outs), (ins), []>,
                      PseudoInstExpansion<(SLL ZERO, ZERO, 0)>, ISA_MIPS1;
 
   // FrameIndexes are legalized when they are operands from load/store
   // instructions. The same not happens for stack address copies, so an
   // add op with mem ComplexPattern is used and the stack address copy
   // can be matched. It's similar to Sparc LEA_ADDRi
   let AdditionalPredicates = [NotInMicroMips] in
     def LEA_ADDiu : MMRel, EffectiveAddress<"addiu", GPR32Opnd>, LW_FM<9>,
                     ISA_MIPS1;
 
   // MADD*/MSUB*
   def MADD  : MMRel, MArithR<"madd", II_MADD, 1>, MULT_FM<0x1c, 0>,
               ISA_MIPS32_NOT_32R6_64R6;
   def MADDU : MMRel, MArithR<"maddu", II_MADDU, 1>, MULT_FM<0x1c, 1>,
               ISA_MIPS32_NOT_32R6_64R6;
   def MSUB  : MMRel, MArithR<"msub", II_MSUB>, MULT_FM<0x1c, 4>,
               ISA_MIPS32_NOT_32R6_64R6;
   def MSUBU : MMRel, MArithR<"msubu", II_MSUBU>, MULT_FM<0x1c, 5>,
               ISA_MIPS32_NOT_32R6_64R6;
 }
 
 let AdditionalPredicates = [NotDSP] in {
 def PseudoMULT  : MultDivPseudo<MULT, ACC64, GPR32Opnd, MipsMult, II_MULT>,
                   ISA_MIPS1_NOT_32R6_64R6;
 def PseudoMULTu : MultDivPseudo<MULTu, ACC64, GPR32Opnd, MipsMultu, II_MULTU>,
                   ISA_MIPS1_NOT_32R6_64R6;
 def PseudoMFHI : PseudoMFLOHI<GPR32, ACC64, MipsMFHI>, ISA_MIPS1_NOT_32R6_64R6;
 def PseudoMFLO : PseudoMFLOHI<GPR32, ACC64, MipsMFLO>, ISA_MIPS1_NOT_32R6_64R6;
 def PseudoMTLOHI : PseudoMTLOHI<ACC64, GPR32>, ISA_MIPS1_NOT_32R6_64R6;
 def PseudoMADD  : MAddSubPseudo<MADD, MipsMAdd, II_MADD>,
                   ISA_MIPS32_NOT_32R6_64R6;
 def PseudoMADDU : MAddSubPseudo<MADDU, MipsMAddu, II_MADDU>,
                   ISA_MIPS32_NOT_32R6_64R6;
 def PseudoMSUB  : MAddSubPseudo<MSUB, MipsMSub, II_MSUB>,
                   ISA_MIPS32_NOT_32R6_64R6;
 def PseudoMSUBU : MAddSubPseudo<MSUBU, MipsMSubu, II_MSUBU>,
                   ISA_MIPS32_NOT_32R6_64R6;
 }
 
 let AdditionalPredicates = [NotInMicroMips] in {
   def PseudoSDIV : MultDivPseudo<SDIV, ACC64, GPR32Opnd, MipsDivRem, II_DIV,
                                  0, 1, 1>, ISA_MIPS1_NOT_32R6_64R6;
   def PseudoUDIV : MultDivPseudo<UDIV, ACC64, GPR32Opnd, MipsDivRemU, II_DIVU,
                                  0, 1, 1>, ISA_MIPS1_NOT_32R6_64R6;
   def RDHWR : MMRel, ReadHardware<GPR32Opnd, HWRegsOpnd>, RDHWR_FM, ISA_MIPS1;
   // TODO: Add '0 < pos+size <= 32' constraint check to ext instruction
   def EXT : MMRel, StdMMR6Rel, ExtBase<"ext", GPR32Opnd, uimm5, uimm5_plus1,
                                        immZExt5, immZExt5Plus1, MipsExt>,
             EXT_FM<0>, ISA_MIPS32R2;
   def INS : MMRel, StdMMR6Rel, InsBase<"ins", GPR32Opnd, uimm5,
                                        uimm5_inssize_plus1, immZExt5,
                                        immZExt5Plus1>,
             EXT_FM<4>, ISA_MIPS32R2;
 }
 /// Move Control Registers From/To CPU Registers
 let AdditionalPredicates = [NotInMicroMips] in {
   def MTC0 : MTC3OP<"mtc0", COP0Opnd, GPR32Opnd, II_MTC0>,
              MFC3OP_FM<0x10, 4, 0>, ISA_MIPS1;
   def MFC0 : MFC3OP<"mfc0", GPR32Opnd, COP0Opnd, II_MFC0>,
              MFC3OP_FM<0x10, 0, 0>, ISA_MIPS1;
   def MFC2 : MFC3OP<"mfc2", GPR32Opnd, COP2Opnd, II_MFC2>,
              MFC3OP_FM<0x12, 0, 0>, ISA_MIPS1;
   def MTC2 : MTC3OP<"mtc2", COP2Opnd, GPR32Opnd, II_MTC2>,
              MFC3OP_FM<0x12, 4, 0>, ISA_MIPS1;
 }
 
 class Barrier<string asmstr, InstrItinClass itin = NoItinerary> :
   InstSE<(outs), (ins), asmstr, [], itin, FrmOther, asmstr>;
 let AdditionalPredicates = [NotInMicroMips] in {
   def SSNOP : MMRel, StdMMR6Rel, Barrier<"ssnop", II_SSNOP>, BARRIER_FM<1>,
               ISA_MIPS1;
   def EHB : MMRel, Barrier<"ehb", II_EHB>, BARRIER_FM<3>, ISA_MIPS1;
 
   let isCTI = 1 in
   def PAUSE : MMRel, StdMMR6Rel, Barrier<"pause", II_PAUSE>, BARRIER_FM<5>,
               ISA_MIPS32R2;
 }
 
 // JR_HB and JALR_HB are defined here using the new style naming
 // scheme because some of this code is shared with Mips32r6InstrInfo.td
 // and because of that it doesn't follow the naming convention of the
 // rest of the file. To avoid a mixture of old vs new style, the new
 // style was chosen.
 class JR_HB_DESC_BASE<string instr_asm, RegisterOperand GPROpnd> {
   dag OutOperandList = (outs);
   dag InOperandList = (ins GPROpnd:$rs);
   string AsmString = !strconcat(instr_asm, "\t$rs");
   list<dag> Pattern = [];
 }
 
 class JALR_HB_DESC_BASE<string instr_asm, RegisterOperand GPROpnd> {
   dag OutOperandList = (outs GPROpnd:$rd);
   dag InOperandList = (ins GPROpnd:$rs);
   string AsmString = !strconcat(instr_asm, "\t$rd, $rs");
   list<dag> Pattern = [];
 }
 
 class JR_HB_DESC<RegisterOperand RO> :
   InstSE<(outs), (ins), "", [], II_JR_HB, FrmJ>, JR_HB_DESC_BASE<"jr.hb", RO> {
   let isBranch=1;
   let isIndirectBranch=1;
   let hasDelaySlot=1;
   let isTerminator=1;
   let isBarrier=1;
   bit isCTI = 1;
 }
 
 class JALR_HB_DESC<RegisterOperand RO> :
   InstSE<(outs), (ins), "", [], II_JALR_HB, FrmJ>, JALR_HB_DESC_BASE<"jalr.hb",
                                                                      RO> {
   let isIndirectBranch=1;
   let hasDelaySlot=1;
   bit isCTI = 1;
 }
 
 class JR_HB_ENC : JR_HB_FM<8>;
 class JALR_HB_ENC : JALR_HB_FM<9>;
 
 def JR_HB : JR_HB_DESC<GPR32Opnd>, JR_HB_ENC, ISA_MIPS32R2_NOT_32R6_64R6;
 def JALR_HB : JALR_HB_DESC<GPR32Opnd>, JALR_HB_ENC, ISA_MIPS32;
 
 let AdditionalPredicates = [NotInMicroMips, UseIndirectJumpsHazard] in
   def JALRHBPseudo : JumpLinkRegPseudo<GPR32Opnd, JALR_HB, RA>;
 
 
 let AdditionalPredicates = [NotInMips16Mode, NotInMicroMips,
                             UseIndirectJumpsHazard] in {
   def TAILCALLREGHB : TailCallReg<JR_HB, GPR32Opnd>, ISA_MIPS32_NOT_32R6_64R6;
   def PseudoIndirectHazardBranch : PseudoIndirectBranchBase<JR_HB, GPR32Opnd>,
                                    ISA_MIPS32R2_NOT_32R6_64R6;
 }
 
 class TLB<string asmstr, InstrItinClass itin = NoItinerary> :
   InstSE<(outs), (ins), asmstr, [], itin, FrmOther, asmstr>;
 let AdditionalPredicates = [NotInMicroMips] in {
   def TLBP : MMRel, TLB<"tlbp", II_TLBP>, COP0_TLB_FM<0x08>, ISA_MIPS1;
   def TLBR : MMRel, TLB<"tlbr", II_TLBR>, COP0_TLB_FM<0x01>, ISA_MIPS1;
   def TLBWI : MMRel, TLB<"tlbwi", II_TLBWI>, COP0_TLB_FM<0x02>, ISA_MIPS1;
   def TLBWR : MMRel, TLB<"tlbwr", II_TLBWR>, COP0_TLB_FM<0x06>, ISA_MIPS1;
 }
 class CacheOp<string instr_asm, Operand MemOpnd,
               InstrItinClass itin = NoItinerary> :
     InstSE<(outs), (ins  MemOpnd:$addr, uimm5:$hint),
            !strconcat(instr_asm, "\t$hint, $addr"), [], itin, FrmOther,
            instr_asm> {
   let DecoderMethod = "DecodeCacheOp";
 }
 
 let AdditionalPredicates = [NotInMicroMips] in {
   def CACHE : MMRel, CacheOp<"cache", mem, II_CACHE>, CACHEOP_FM<0b101111>,
               INSN_MIPS3_32_NOT_32R6_64R6;
   def PREF :  MMRel, CacheOp<"pref", mem, II_PREF>, CACHEOP_FM<0b110011>,
               INSN_MIPS3_32_NOT_32R6_64R6;
 }
 // FIXME: We are missing the prefx instruction.
 def ROL : MipsAsmPseudoInst<(outs),
                             (ins GPR32Opnd:$rs, GPR32Opnd:$rt, GPR32Opnd:$rd),
                             "rol\t$rs, $rt, $rd">;
 def ROLImm : MipsAsmPseudoInst<(outs),
                                (ins GPR32Opnd:$rs, GPR32Opnd:$rt, simm16:$imm),
                                "rol\t$rs, $rt, $imm">;
 def : MipsInstAlias<"rol $rd, $rs",
                     (ROL GPR32Opnd:$rd, GPR32Opnd:$rd, GPR32Opnd:$rs), 0>;
 def : MipsInstAlias<"rol $rd, $imm",
                     (ROLImm GPR32Opnd:$rd, GPR32Opnd:$rd, simm16:$imm), 0>;
 
 def ROR : MipsAsmPseudoInst<(outs),
                             (ins GPR32Opnd:$rs, GPR32Opnd:$rt, GPR32Opnd:$rd),
                             "ror\t$rs, $rt, $rd">;
 def RORImm : MipsAsmPseudoInst<(outs),
                                (ins GPR32Opnd:$rs, GPR32Opnd:$rt, simm16:$imm),
                                "ror\t$rs, $rt, $imm">;
 def : MipsInstAlias<"ror $rd, $rs",
                     (ROR GPR32Opnd:$rd, GPR32Opnd:$rd, GPR32Opnd:$rs), 0>;
 def : MipsInstAlias<"ror $rd, $imm",
                     (RORImm GPR32Opnd:$rd, GPR32Opnd:$rd, simm16:$imm), 0>;
 
 def DROL : MipsAsmPseudoInst<(outs),
                              (ins GPR32Opnd:$rs, GPR32Opnd:$rt, GPR32Opnd:$rd),
                              "drol\t$rs, $rt, $rd">, ISA_MIPS64;
 def DROLImm : MipsAsmPseudoInst<(outs),
                                 (ins GPR32Opnd:$rs, GPR32Opnd:$rt, simm16:$imm),
                                 "drol\t$rs, $rt, $imm">, ISA_MIPS64;
 def : MipsInstAlias<"drol $rd, $rs",
                     (DROL GPR32Opnd:$rd, GPR32Opnd:$rd, GPR32Opnd:$rs), 0>,
       ISA_MIPS64;
 def : MipsInstAlias<"drol $rd, $imm",
                     (DROLImm GPR32Opnd:$rd, GPR32Opnd:$rd, simm16:$imm), 0>,
       ISA_MIPS64;
 
 def DROR : MipsAsmPseudoInst<(outs),
                              (ins GPR32Opnd:$rs, GPR32Opnd:$rt, GPR32Opnd:$rd),
                              "dror\t$rs, $rt, $rd">, ISA_MIPS64;
 def DRORImm : MipsAsmPseudoInst<(outs),
                                 (ins GPR32Opnd:$rs, GPR32Opnd:$rt, simm16:$imm),
                                 "dror\t$rs, $rt, $imm">, ISA_MIPS64;
 def : MipsInstAlias<"dror $rd, $rs",
                     (DROR GPR32Opnd:$rd, GPR32Opnd:$rd, GPR32Opnd:$rs), 0>,
       ISA_MIPS64;
 def : MipsInstAlias<"dror $rd, $imm",
                     (DRORImm GPR32Opnd:$rd, GPR32Opnd:$rd, simm16:$imm), 0>,
       ISA_MIPS64;
 
 def ABSMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd), (ins GPR32Opnd:$rs),
                                  "abs\t$rd, $rs">;
 
 def SEQMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                  (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
                                  "seq $rd, $rs, $rt">, NOT_ASE_CNMIPS;
 
 def : MipsInstAlias<"seq $rd, $rs",
                     (SEQMacro GPR32Opnd:$rd, GPR32Opnd:$rd, GPR32Opnd:$rs), 0>,
                     NOT_ASE_CNMIPS;
 
 def SEQIMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                   (ins GPR32Opnd:$rs, simm32_relaxed:$imm),
                                   "seq $rd, $rs, $imm">, NOT_ASE_CNMIPS;
 
 def : MipsInstAlias<"seq $rd, $imm",
                     (SEQIMacro GPR32Opnd:$rd, GPR32Opnd:$rd, simm32:$imm), 0>,
                     NOT_ASE_CNMIPS;
 
 def SNEMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                  (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
                                  "sne $rd, $rs, $rt">, NOT_ASE_CNMIPS;
 
 def : MipsInstAlias<"sne $rd, $rs",
                     (SNEMacro GPR32Opnd:$rd, GPR32Opnd:$rd, GPR32Opnd:$rs), 0>,
                     NOT_ASE_CNMIPS;
 
 def SNEIMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                   (ins GPR32Opnd:$rs, simm32_relaxed:$imm),
                                   "sne $rd, $rs, $imm">, NOT_ASE_CNMIPS;
 
 def : MipsInstAlias<"sne $rd, $imm",
                     (SNEIMacro GPR32Opnd:$rd, GPR32Opnd:$rd, simm32:$imm), 0>,
                     NOT_ASE_CNMIPS;
 
 def MULImmMacro : MipsAsmPseudoInst<(outs), (ins GPR32Opnd:$rd, GPR32Opnd:$rs,
                                                  simm32_relaxed:$imm),
                                     "mul\t$rd, $rs, $imm">,
                   ISA_MIPS1_NOT_32R6_64R6;
 def MULOMacro : MipsAsmPseudoInst<(outs), (ins GPR32Opnd:$rd, GPR32Opnd:$rs,
                                                GPR32Opnd:$rt),
                                   "mulo\t$rd, $rs, $rt">,
                 ISA_MIPS1_NOT_32R6_64R6;
 def MULOUMacro : MipsAsmPseudoInst<(outs), (ins GPR32Opnd:$rd, GPR32Opnd:$rs,
                                                 GPR32Opnd:$rt),
                                    "mulou\t$rd, $rs, $rt">,
                  ISA_MIPS1_NOT_32R6_64R6;
 
 // Virtualization ASE
 class HYPCALL_FT<string opstr> :
   InstSE<(outs), (ins uimm10:$code_),
          !strconcat(opstr, "\t$code_"), [], II_HYPCALL, FrmOther, opstr> {
   let BaseOpcode = opstr;
 }
 
 let AdditionalPredicates = [NotInMicroMips] in {
   def MFGC0    : MMRel, MFC3OP<"mfgc0", GPR32Opnd, COP0Opnd, II_MFGC0>,
                  MFC3OP_FM<0x10, 3, 0>, ISA_MIPS32R5, ASE_VIRT;
   def MTGC0    : MMRel, MTC3OP<"mtgc0", COP0Opnd, GPR32Opnd, II_MTGC0>,
                  MFC3OP_FM<0x10, 3, 2>, ISA_MIPS32R5, ASE_VIRT;
   def MFHGC0   : MMRel, MFC3OP<"mfhgc0", GPR32Opnd, COP0Opnd, II_MFHGC0>,
                  MFC3OP_FM<0x10, 3, 4>, ISA_MIPS32R5, ASE_VIRT;
   def MTHGC0   : MMRel, MTC3OP<"mthgc0", COP0Opnd, GPR32Opnd, II_MTHGC0>,
                  MFC3OP_FM<0x10, 3, 6>, ISA_MIPS32R5, ASE_VIRT;
   def TLBGINV  : MMRel, TLB<"tlbginv", II_TLBGINV>, COP0_TLB_FM<0b001011>,
                  ISA_MIPS32R5, ASE_VIRT;
   def TLBGINVF : MMRel, TLB<"tlbginvf", II_TLBGINVF>, COP0_TLB_FM<0b001100>,
                  ISA_MIPS32R5, ASE_VIRT;
   def TLBGP    : MMRel, TLB<"tlbgp", II_TLBGP>, COP0_TLB_FM<0b010000>,
                  ISA_MIPS32R5, ASE_VIRT;
   def TLBGR    : MMRel, TLB<"tlbgr", II_TLBGR>, COP0_TLB_FM<0b001001>,
                  ISA_MIPS32R5, ASE_VIRT;
   def TLBGWI   : MMRel, TLB<"tlbgwi", II_TLBGWI>, COP0_TLB_FM<0b001010>,
                  ISA_MIPS32R5, ASE_VIRT;
   def TLBGWR   : MMRel, TLB<"tlbgwr", II_TLBGWR>, COP0_TLB_FM<0b001110>,
                  ISA_MIPS32R5, ASE_VIRT;
   def HYPCALL  : MMRel, HYPCALL_FT<"hypcall">,
                  HYPCALL_FM<0b101000>, ISA_MIPS32R5, ASE_VIRT;
 }
 
 //===----------------------------------------------------------------------===//
 // Instruction aliases
 //===----------------------------------------------------------------------===//
 
 multiclass OneOrTwoOperandMacroImmediateAlias<string Memnomic,
                                               Instruction Opcode,
                                               RegisterOperand RO = GPR32Opnd,
                                               Operand Imm = simm32_relaxed> {
   def : MipsInstAlias<!strconcat(Memnomic, " $rs, $rt, $imm"),
                                 (Opcode RO:$rs,
                                         RO:$rt,
                                         Imm:$imm), 0>;
   def : MipsInstAlias<!strconcat(Memnomic, " $rs, $imm"),
                                 (Opcode RO:$rs,
                                         RO:$rs,
                                         Imm:$imm), 0>;
 }
 
 let AdditionalPredicates = [NotInMicroMips] in {
   def : MipsInstAlias<"move $dst, $src",
                       (OR GPR32Opnd:$dst, GPR32Opnd:$src, ZERO), 1>,
         GPR_32, ISA_MIPS1;
   def : MipsInstAlias<"move $dst, $src",
                       (ADDu GPR32Opnd:$dst, GPR32Opnd:$src, ZERO), 1>,
         GPR_32, ISA_MIPS1;
 
   def : MipsInstAlias<"bal $offset", (BGEZAL ZERO, brtarget:$offset), 1>,
         ISA_MIPS1_NOT_32R6_64R6;
 
   def : MipsInstAlias<"j $rs", (JR GPR32Opnd:$rs), 0>, ISA_MIPS1;
 
   def : MipsInstAlias<"jalr $rs", (JALR RA, GPR32Opnd:$rs), 0>;
 
   def : MipsInstAlias<"jalr.hb $rs", (JALR_HB RA, GPR32Opnd:$rs), 1>,
         ISA_MIPS32;
 
   def : MipsInstAlias<"neg $rt, $rs",
                       (SUB GPR32Opnd:$rt, ZERO, GPR32Opnd:$rs), 1>, ISA_MIPS1;
   def : MipsInstAlias<"neg $rt",
                       (SUB GPR32Opnd:$rt, ZERO, GPR32Opnd:$rt), 1>, ISA_MIPS1;
   def : MipsInstAlias<"negu $rt, $rs",
                       (SUBu GPR32Opnd:$rt, ZERO, GPR32Opnd:$rs), 1>, ISA_MIPS1;
   def : MipsInstAlias<"negu $rt",
                       (SUBu GPR32Opnd:$rt, ZERO, GPR32Opnd:$rt), 1>, ISA_MIPS1;
 
   def SGE : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                               (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
                               "sge\t$rd, $rs, $rt">, ISA_MIPS1;
   def : MipsInstAlias<"sge $rs, $rt",
                       (SGE GPR32Opnd:$rs, GPR32Opnd:$rs, GPR32Opnd:$rt), 0>,
         ISA_MIPS1;
   def SGEImm : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                  (ins GPR32Opnd:$rs, simm32:$imm),
                                  "sge\t$rd, $rs, $imm">, GPR_32;
   def : MipsInstAlias<"sge $rs, $imm", (SGEImm GPR32Opnd:$rs,
                                                GPR32Opnd:$rs,
                                                simm32:$imm), 0>,
         GPR_32;
 
   def SGEU : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
                                "sgeu\t$rd, $rs, $rt">, ISA_MIPS1;
   def : MipsInstAlias<"sgeu $rs, $rt",
                       (SGEU GPR32Opnd:$rs, GPR32Opnd:$rs, GPR32Opnd:$rt), 0>,
         ISA_MIPS1;
   def SGEUImm : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                   (ins GPR32Opnd:$rs, uimm32_coerced:$imm),
                                   "sgeu\t$rd, $rs, $imm">, GPR_32;
   def : MipsInstAlias<"sgeu $rs, $imm", (SGEUImm GPR32Opnd:$rs,
                                                  GPR32Opnd:$rs,
                                                  uimm32_coerced:$imm), 0>,
         GPR_32;
 
   def : MipsInstAlias<
           "sgt $rd, $rs, $rt",
           (SLT GPR32Opnd:$rd, GPR32Opnd:$rt, GPR32Opnd:$rs), 0>, ISA_MIPS1;
   def : MipsInstAlias<
           "sgt $rs, $rt",
           (SLT GPR32Opnd:$rs, GPR32Opnd:$rt, GPR32Opnd:$rs), 0>, ISA_MIPS1;
 
   def SGTImm : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                  (ins GPR32Opnd:$rs, simm32:$imm),
                                  "sgt\t$rd, $rs, $imm">, GPR_32;
   def : MipsInstAlias<"sgt $rs, $imm", (SGTImm GPR32Opnd:$rs,
                                                GPR32Opnd:$rs,
                                                simm32:$imm), 0>,
         GPR_32;
   def : MipsInstAlias<
           "sgtu $rd, $rs, $rt",
           (SLTu GPR32Opnd:$rd, GPR32Opnd:$rt, GPR32Opnd:$rs), 0>, ISA_MIPS1;
   def : MipsInstAlias<
           "sgtu $$rs, $rt",
           (SLTu GPR32Opnd:$rs, GPR32Opnd:$rt, GPR32Opnd:$rs), 0>, ISA_MIPS1;
 
   def SGTUImm : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                   (ins GPR32Opnd:$rs, uimm32_coerced:$imm),
                                   "sgtu\t$rd, $rs, $imm">, GPR_32;
   def : MipsInstAlias<"sgtu $rs, $imm", (SGTUImm GPR32Opnd:$rs,
                                                  GPR32Opnd:$rs,
                                                  uimm32_coerced:$imm), 0>,
         GPR_32;
 
   def SLE : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                               (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
                               "sle\t$rd, $rs, $rt">, ISA_MIPS1;
   def : MipsInstAlias<"sle $rs, $rt",
                       (SLE GPR32Opnd:$rs, GPR32Opnd:$rs, GPR32Opnd:$rt), 0>,
         ISA_MIPS1;
   def SLEImm : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                  (ins GPR32Opnd:$rs, simm32:$imm),
                                  "sle\t$rd, $rs, $imm">, GPR_32;
   def : MipsInstAlias<"sle $rs, $imm", (SLEImm GPR32Opnd:$rs,
                                                GPR32Opnd:$rs,
                                                simm32:$imm), 0>,
         GPR_32;
 
   def SLEU : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
                                "sleu\t$rd, $rs, $rt">, ISA_MIPS1;
   def : MipsInstAlias<"sleu $rs, $rt",
                       (SLEU GPR32Opnd:$rs, GPR32Opnd:$rs, GPR32Opnd:$rt), 0>,
         ISA_MIPS1;
   def SLEUImm : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                   (ins GPR32Opnd:$rs, uimm32_coerced:$imm),
                                   "sleu\t$rd, $rs, $imm">, GPR_32;
   def : MipsInstAlias<"sleu $rs, $imm", (SLEUImm GPR32Opnd:$rs,
                                                  GPR32Opnd:$rs,
                                                  uimm32_coerced:$imm), 0>,
         GPR_32;
 
   def : MipsInstAlias<
           "not $rt, $rs",
           (NOR GPR32Opnd:$rt, GPR32Opnd:$rs, ZERO), 0>, ISA_MIPS1;
   def : MipsInstAlias<
           "not $rt",
           (NOR GPR32Opnd:$rt, GPR32Opnd:$rt, ZERO), 0>, ISA_MIPS1;
 
   def : MipsInstAlias<"nop", (SLL ZERO, ZERO, 0), 1>, ISA_MIPS1;
 
   defm : OneOrTwoOperandMacroImmediateAlias<"add", ADDi>,
          ISA_MIPS1_NOT_32R6_64R6;
 
   defm : OneOrTwoOperandMacroImmediateAlias<"addu", ADDiu>, ISA_MIPS1;
 
   defm : OneOrTwoOperandMacroImmediateAlias<"and", ANDi>, ISA_MIPS1, GPR_32;
 
   defm : OneOrTwoOperandMacroImmediateAlias<"or", ORi>, ISA_MIPS1, GPR_32;
 
   defm : OneOrTwoOperandMacroImmediateAlias<"xor", XORi>, ISA_MIPS1, GPR_32;
 
   defm : OneOrTwoOperandMacroImmediateAlias<"slt", SLTi>, ISA_MIPS1, GPR_32;
 
   defm : OneOrTwoOperandMacroImmediateAlias<"sltu", SLTiu>, ISA_MIPS1, GPR_32;
 
   def : MipsInstAlias<"mfgc0 $rt, $rd",
                       (MFGC0 GPR32Opnd:$rt, COP0Opnd:$rd, 0), 0>,
                       ISA_MIPS32R5, ASE_VIRT;
   def : MipsInstAlias<"mtgc0 $rt, $rd",
                       (MTGC0 COP0Opnd:$rd, GPR32Opnd:$rt, 0), 0>,
                       ISA_MIPS32R5, ASE_VIRT;
   def : MipsInstAlias<"mfhgc0 $rt, $rd",
                       (MFHGC0 GPR32Opnd:$rt, COP0Opnd:$rd, 0), 0>,
                       ISA_MIPS32R5, ASE_VIRT;
   def : MipsInstAlias<"mthgc0 $rt, $rd",
                       (MTHGC0 COP0Opnd:$rd, GPR32Opnd:$rt, 0), 0>,
                       ISA_MIPS32R5, ASE_VIRT;
   def : MipsInstAlias<"mfc0 $rt, $rd", (MFC0 GPR32Opnd:$rt, COP0Opnd:$rd, 0), 0>,
         ISA_MIPS1;
   def : MipsInstAlias<"mtc0 $rt, $rd", (MTC0 COP0Opnd:$rd, GPR32Opnd:$rt, 0), 0>,
         ISA_MIPS1;
   def : MipsInstAlias<"mfc2 $rt, $rd", (MFC2 GPR32Opnd:$rt, COP2Opnd:$rd, 0), 0>,
         ISA_MIPS1;
   def : MipsInstAlias<"mtc2 $rt, $rd", (MTC2 COP2Opnd:$rd, GPR32Opnd:$rt, 0), 0>,
         ISA_MIPS1;
 
   def : MipsInstAlias<"b $offset", (BEQ ZERO, ZERO, brtarget:$offset), 0>,
         ISA_MIPS1;
 
   def : MipsInstAlias<"bnez $rs,$offset",
                       (BNE GPR32Opnd:$rs, ZERO, brtarget:$offset), 0>,
         ISA_MIPS1;
   def : MipsInstAlias<"bnezl $rs, $offset",
                       (BNEL GPR32Opnd:$rs, ZERO, brtarget:$offset), 1>,
         ISA_MIPS2;
   def : MipsInstAlias<"beqz $rs,$offset",
                       (BEQ GPR32Opnd:$rs, ZERO, brtarget:$offset), 0>,
         ISA_MIPS1;
   def : MipsInstAlias<"beqzl $rs, $offset",
                       (BEQL GPR32Opnd:$rs, ZERO, brtarget:$offset), 1>,
         ISA_MIPS2;
 
   def : MipsInstAlias<"syscall", (SYSCALL 0), 1>, ISA_MIPS1;
 
   def : MipsInstAlias<"break", (BREAK 0, 0), 1>, ISA_MIPS1;
   def : MipsInstAlias<"break $imm", (BREAK uimm10:$imm, 0), 1>, ISA_MIPS1;
   def : MipsInstAlias<"ei", (EI ZERO), 1>, ISA_MIPS32R2;
   def : MipsInstAlias<"di", (DI ZERO), 1>, ISA_MIPS32R2;
 
   def : MipsInstAlias<"teq $rs, $rt",
                       (TEQ GPR32Opnd:$rs, GPR32Opnd:$rt, 0), 1>, ISA_MIPS2;
   def : MipsInstAlias<"tge $rs, $rt",
                       (TGE GPR32Opnd:$rs, GPR32Opnd:$rt, 0), 1>, ISA_MIPS2;
   def : MipsInstAlias<"tgeu $rs, $rt",
                       (TGEU GPR32Opnd:$rs, GPR32Opnd:$rt, 0), 1>, ISA_MIPS2;
   def : MipsInstAlias<"tlt $rs, $rt",
                       (TLT GPR32Opnd:$rs, GPR32Opnd:$rt, 0), 1>, ISA_MIPS2;
   def : MipsInstAlias<"tltu $rs, $rt",
                       (TLTU GPR32Opnd:$rs, GPR32Opnd:$rt, 0), 1>, ISA_MIPS2;
   def : MipsInstAlias<"tne $rs, $rt",
                       (TNE GPR32Opnd:$rs, GPR32Opnd:$rt, 0), 1>, ISA_MIPS2;
   def : MipsInstAlias<"rdhwr $rt, $rs",
                       (RDHWR GPR32Opnd:$rt, HWRegsOpnd:$rs, 0), 1>, ISA_MIPS1;
 
 }
 def : MipsInstAlias<"sub, $rd, $rs, $imm",
                     (ADDi GPR32Opnd:$rd, GPR32Opnd:$rs,
                           InvertedImOperand:$imm), 0>, ISA_MIPS1_NOT_32R6_64R6;
 def : MipsInstAlias<"sub $rs, $imm",
                     (ADDi GPR32Opnd:$rs, GPR32Opnd:$rs, InvertedImOperand:$imm),
                     0>, ISA_MIPS1_NOT_32R6_64R6;
 def : MipsInstAlias<"subu, $rd, $rs, $imm",
                     (ADDiu GPR32Opnd:$rd, GPR32Opnd:$rs,
                            InvertedImOperand:$imm), 0>;
 def : MipsInstAlias<"subu $rs, $imm", (ADDiu GPR32Opnd:$rs, GPR32Opnd:$rs,
                                              InvertedImOperand:$imm), 0>;
 let AdditionalPredicates = [NotInMicroMips] in {
   def : MipsInstAlias<"sll $rd, $rt, $rs",
                       (SLLV GPR32Opnd:$rd, GPR32Opnd:$rt, GPR32Opnd:$rs), 0>;
   def : MipsInstAlias<"sra $rd, $rt, $rs",
                       (SRAV GPR32Opnd:$rd, GPR32Opnd:$rt, GPR32Opnd:$rs), 0>;
   def : MipsInstAlias<"srl $rd, $rt, $rs",
                       (SRLV GPR32Opnd:$rd, GPR32Opnd:$rt, GPR32Opnd:$rs), 0>;
   def : MipsInstAlias<"sll $rd, $rt",
                       (SLLV GPR32Opnd:$rd, GPR32Opnd:$rd, GPR32Opnd:$rt), 0>;
   def : MipsInstAlias<"sra $rd, $rt",
                       (SRAV GPR32Opnd:$rd, GPR32Opnd:$rd, GPR32Opnd:$rt), 0>;
   def : MipsInstAlias<"srl $rd, $rt",
                       (SRLV GPR32Opnd:$rd, GPR32Opnd:$rd, GPR32Opnd:$rt), 0>;
   def : MipsInstAlias<"seh $rd", (SEH GPR32Opnd:$rd, GPR32Opnd:$rd), 0>,
                      ISA_MIPS32R2;
   def : MipsInstAlias<"seb $rd", (SEB GPR32Opnd:$rd, GPR32Opnd:$rd), 0>,
                      ISA_MIPS32R2;
 }
 def : MipsInstAlias<"sdbbp", (SDBBP 0)>, ISA_MIPS32_NOT_32R6_64R6;
 let AdditionalPredicates = [NotInMicroMips] in
   def : MipsInstAlias<"sync", (SYNC 0), 1>, ISA_MIPS2;
 
 def : MipsInstAlias<"mulo $rs, $rt",
                     (MULOMacro GPR32Opnd:$rs, GPR32Opnd:$rs, GPR32Opnd:$rt), 0>,
                     ISA_MIPS1_NOT_32R6_64R6;
 def : MipsInstAlias<"mulou $rs, $rt",
                     (MULOUMacro GPR32Opnd:$rs, GPR32Opnd:$rs, GPR32Opnd:$rt), 0>,
                     ISA_MIPS1_NOT_32R6_64R6;
 
 let AdditionalPredicates = [NotInMicroMips] in
   def : MipsInstAlias<"hypcall", (HYPCALL 0), 1>, ISA_MIPS32R5, ASE_VIRT;
 
 //===----------------------------------------------------------------------===//
 // Assembler Pseudo Instructions
 //===----------------------------------------------------------------------===//
 
 // We use uimm32_coerced to accept a 33 bit signed number that is rendered into
 // a 32 bit number.
 class LoadImmediate32<string instr_asm, Operand Od, RegisterOperand RO> :
   MipsAsmPseudoInst<(outs RO:$rt), (ins Od:$imm32),
                      !strconcat(instr_asm, "\t$rt, $imm32")> ;
 def LoadImm32 : LoadImmediate32<"li", uimm32_coerced, GPR32Opnd>;
 
 class LoadAddressFromReg32<string instr_asm, Operand MemOpnd,
                            RegisterOperand RO> :
   MipsAsmPseudoInst<(outs RO:$rt), (ins MemOpnd:$addr),
                      !strconcat(instr_asm, "\t$rt, $addr")> ;
 def LoadAddrReg32 : LoadAddressFromReg32<"la", mem, GPR32Opnd>;
 
 class LoadAddressFromImm32<string instr_asm, Operand Od, RegisterOperand RO> :
   MipsAsmPseudoInst<(outs RO:$rt), (ins Od:$imm32),
                      !strconcat(instr_asm, "\t$rt, $imm32")> ;
 def LoadAddrImm32 : LoadAddressFromImm32<"la", i32imm, GPR32Opnd>;
 
 def JalTwoReg : MipsAsmPseudoInst<(outs GPR32Opnd:$rd), (ins GPR32Opnd:$rs),
                       "jal\t$rd, $rs"> ;
 def JalOneReg : MipsAsmPseudoInst<(outs), (ins GPR32Opnd:$rs),
                       "jal\t$rs"> ;
 
 class NORIMM_DESC_BASE<RegisterOperand RO, DAGOperand Imm> :
    MipsAsmPseudoInst<(outs RO:$rs), (ins RO:$rt, Imm:$imm),
                       "nor\t$rs, $rt, $imm">;
 def NORImm : NORIMM_DESC_BASE<GPR32Opnd, simm32_relaxed>, GPR_32;
 def : MipsInstAlias<"nor\t$rs, $imm", (NORImm GPR32Opnd:$rs, GPR32Opnd:$rs,
                                               simm32_relaxed:$imm)>, GPR_32;
 
 let hasDelaySlot = 1, isCTI = 1 in {
 def BneImm : MipsAsmPseudoInst<(outs GPR32Opnd:$rt),
                                (ins imm64:$imm64, brtarget:$offset),
                                "bne\t$rt, $imm64, $offset">;
 def BeqImm : MipsAsmPseudoInst<(outs GPR32Opnd:$rt),
                                (ins imm64:$imm64, brtarget:$offset),
                                "beq\t$rt, $imm64, $offset">;
 
 class CondBranchPseudo<string instr_asm> :
   MipsAsmPseudoInst<(outs), (ins GPR32Opnd:$rs, GPR32Opnd:$rt,
                                  brtarget:$offset),
                     !strconcat(instr_asm, "\t$rs, $rt, $offset")>;
 }
 
 def BLT : CondBranchPseudo<"blt">;
 def BLE : CondBranchPseudo<"ble">;
 def BGE : CondBranchPseudo<"bge">;
 def BGT : CondBranchPseudo<"bgt">;
 def BLTU : CondBranchPseudo<"bltu">;
 def BLEU : CondBranchPseudo<"bleu">;
 def BGEU : CondBranchPseudo<"bgeu">;
 def BGTU : CondBranchPseudo<"bgtu">;
 def BLTL : CondBranchPseudo<"bltl">, ISA_MIPS2_NOT_32R6_64R6;
 def BLEL : CondBranchPseudo<"blel">, ISA_MIPS2_NOT_32R6_64R6;
 def BGEL : CondBranchPseudo<"bgel">, ISA_MIPS2_NOT_32R6_64R6;
 def BGTL : CondBranchPseudo<"bgtl">, ISA_MIPS2_NOT_32R6_64R6;
 def BLTUL: CondBranchPseudo<"bltul">, ISA_MIPS2_NOT_32R6_64R6;
 def BLEUL: CondBranchPseudo<"bleul">, ISA_MIPS2_NOT_32R6_64R6;
 def BGEUL: CondBranchPseudo<"bgeul">, ISA_MIPS2_NOT_32R6_64R6;
 def BGTUL: CondBranchPseudo<"bgtul">, ISA_MIPS2_NOT_32R6_64R6;
 
 let isCTI = 1 in
 class CondBranchImmPseudo<string instr_asm> :
   MipsAsmPseudoInst<(outs), (ins GPR32Opnd:$rs, imm64:$imm, brtarget:$offset),
                     !strconcat(instr_asm, "\t$rs, $imm, $offset")>;
 
 def BEQLImmMacro : CondBranchImmPseudo<"beql">, ISA_MIPS2_NOT_32R6_64R6;
 def BNELImmMacro : CondBranchImmPseudo<"bnel">, ISA_MIPS2_NOT_32R6_64R6;
 
 def BLTImmMacro  : CondBranchImmPseudo<"blt">;
 def BLEImmMacro  : CondBranchImmPseudo<"ble">;
 def BGEImmMacro  : CondBranchImmPseudo<"bge">;
 def BGTImmMacro  : CondBranchImmPseudo<"bgt">;
 def BLTUImmMacro : CondBranchImmPseudo<"bltu">;
 def BLEUImmMacro : CondBranchImmPseudo<"bleu">;
 def BGEUImmMacro : CondBranchImmPseudo<"bgeu">;
 def BGTUImmMacro : CondBranchImmPseudo<"bgtu">;
 def BLTLImmMacro : CondBranchImmPseudo<"bltl">, ISA_MIPS2_NOT_32R6_64R6;
 def BLELImmMacro : CondBranchImmPseudo<"blel">, ISA_MIPS2_NOT_32R6_64R6;
 def BGELImmMacro : CondBranchImmPseudo<"bgel">, ISA_MIPS2_NOT_32R6_64R6;
 def BGTLImmMacro : CondBranchImmPseudo<"bgtl">, ISA_MIPS2_NOT_32R6_64R6;
 def BLTULImmMacro : CondBranchImmPseudo<"bltul">, ISA_MIPS2_NOT_32R6_64R6;
 def BLEULImmMacro : CondBranchImmPseudo<"bleul">, ISA_MIPS2_NOT_32R6_64R6;
 def BGEULImmMacro : CondBranchImmPseudo<"bgeul">, ISA_MIPS2_NOT_32R6_64R6;
 def BGTULImmMacro : CondBranchImmPseudo<"bgtul">, ISA_MIPS2_NOT_32R6_64R6;
 
 // FIXME: Predicates are removed because instructions are matched regardless of
 // predicates, because PredicateControl was not in the hierarchy. This was
 // done to emit more precise error message from expansion function.
 // Once the tablegen-erated errors are made better, this needs to be fixed and
 // predicates needs to be restored.
 
 def SDivMacro : MipsAsmPseudoInst<(outs GPR32NonZeroOpnd:$rd),
                                   (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
                                   "div\t$rd, $rs, $rt">,
                 ISA_MIPS1_NOT_32R6_64R6;
 def SDivIMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                    (ins GPR32Opnd:$rs, simm32:$imm),
                                    "div\t$rd, $rs, $imm">,
                  ISA_MIPS1_NOT_32R6_64R6;
 def UDivMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                   (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
                                   "divu\t$rd, $rs, $rt">,
                 ISA_MIPS1_NOT_32R6_64R6;
 def UDivIMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                    (ins GPR32Opnd:$rs, simm32:$imm),
                                    "divu\t$rd, $rs, $imm">,
                  ISA_MIPS1_NOT_32R6_64R6;
 
 
 def : MipsInstAlias<"div $rs, $rt", (SDIV GPR32ZeroOpnd:$rs,
                                           GPR32Opnd:$rt), 0>,
      ISA_MIPS1_NOT_32R6_64R6;
 def : MipsInstAlias<"div $rs, $rt", (SDivMacro GPR32NonZeroOpnd:$rs,
                                                GPR32NonZeroOpnd:$rs,
                                                GPR32Opnd:$rt), 0>,
      ISA_MIPS1_NOT_32R6_64R6;
 def : MipsInstAlias<"div $rd, $imm", (SDivIMacro GPR32Opnd:$rd, GPR32Opnd:$rd,
                                                  simm32:$imm), 0>,
       ISA_MIPS1_NOT_32R6_64R6;
 
 def : MipsInstAlias<"divu $rt, $rs", (UDIV GPR32ZeroOpnd:$rt,
                                            GPR32Opnd:$rs), 0>,
       ISA_MIPS1_NOT_32R6_64R6;
 def : MipsInstAlias<"divu $rt, $rs", (UDivMacro GPR32NonZeroOpnd:$rt,
                                                 GPR32NonZeroOpnd:$rt,
                                                 GPR32Opnd:$rs), 0>,
       ISA_MIPS1_NOT_32R6_64R6;
 
 def : MipsInstAlias<"divu $rd, $imm", (UDivIMacro GPR32Opnd:$rd, GPR32Opnd:$rd,
                                                   simm32:$imm), 0>,
       ISA_MIPS1_NOT_32R6_64R6;
 
 def SRemMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                   (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
                                   "rem\t$rd, $rs, $rt">,
                 ISA_MIPS1_NOT_32R6_64R6;
 def SRemIMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                    (ins GPR32Opnd:$rs, simm32_relaxed:$imm),
                                    "rem\t$rd, $rs, $imm">,
                  ISA_MIPS1_NOT_32R6_64R6;
 def URemMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                   (ins GPR32Opnd:$rs, GPR32Opnd:$rt),
                                   "remu\t$rd, $rs, $rt">,
                 ISA_MIPS1_NOT_32R6_64R6;
 def URemIMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rd),
                                    (ins GPR32Opnd:$rs, simm32_relaxed:$imm),
                                    "remu\t$rd, $rs, $imm">,
                  ISA_MIPS1_NOT_32R6_64R6;
 
 def : MipsInstAlias<"rem $rt, $rs", (SRemMacro GPR32Opnd:$rt, GPR32Opnd:$rt,
                                                GPR32Opnd:$rs), 0>,
       ISA_MIPS1_NOT_32R6_64R6;
 def : MipsInstAlias<"rem $rd, $imm", (SRemIMacro GPR32Opnd:$rd, GPR32Opnd:$rd,
                                       simm32_relaxed:$imm), 0>,
       ISA_MIPS1_NOT_32R6_64R6;
 def : MipsInstAlias<"remu $rt, $rs", (URemMacro GPR32Opnd:$rt, GPR32Opnd:$rt,
                                                 GPR32Opnd:$rs), 0>,
       ISA_MIPS1_NOT_32R6_64R6;
 def : MipsInstAlias<"remu $rd, $imm", (URemIMacro GPR32Opnd:$rd, GPR32Opnd:$rd,
                                        simm32_relaxed:$imm), 0>,
       ISA_MIPS1_NOT_32R6_64R6;
 
 def Ulh : MipsAsmPseudoInst<(outs GPR32Opnd:$rt), (ins mem:$addr),
                             "ulh\t$rt, $addr">; //, ISA_MIPS1_NOT_32R6_64R6;
 
 def Ulhu : MipsAsmPseudoInst<(outs GPR32Opnd:$rt), (ins mem:$addr),
                              "ulhu\t$rt, $addr">; //, ISA_MIPS1_NOT_32R6_64R6;
 
 def Ulw : MipsAsmPseudoInst<(outs GPR32Opnd:$rt), (ins mem:$addr),
                             "ulw\t$rt, $addr">; //, ISA_MIPS1_NOT_32R6_64R6;
 
 def Ush : MipsAsmPseudoInst<(outs GPR32Opnd:$rt), (ins mem:$addr),
                             "ush\t$rt, $addr">; //, ISA_MIPS1_NOT_32R6_64R6;
 
 def Usw : MipsAsmPseudoInst<(outs GPR32Opnd:$rt), (ins mem:$addr),
                             "usw\t$rt, $addr">; //, ISA_MIPS1_NOT_32R6_64R6;
 
 def LDMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rt),
                                 (ins mem_simm16:$addr), "ld $rt, $addr">,
                                 ISA_MIPS1_NOT_MIPS3;
 def SDMacro : MipsAsmPseudoInst<(outs GPR32Opnd:$rt),
                                 (ins mem_simm16:$addr), "sd $rt, $addr">,
                                 ISA_MIPS1_NOT_MIPS3;
 //===----------------------------------------------------------------------===//
 //  Arbitrary patterns that map to one or more instructions
 //===----------------------------------------------------------------------===//
 
 // Load/store pattern templates.
 class LoadRegImmPat<Instruction LoadInst, ValueType ValTy, PatFrag Node> :
   MipsPat<(ValTy (Node addrRegImm:$a)), (LoadInst addrRegImm:$a)>;
 
 class StoreRegImmPat<Instruction StoreInst, ValueType ValTy> :
   MipsPat<(store ValTy:$v, addrRegImm:$a), (StoreInst ValTy:$v, addrRegImm:$a)>;
 
 // Materialize constants.
 multiclass MaterializeImms<ValueType VT, Register ZEROReg,
                            Instruction ADDiuOp, Instruction LUiOp,
                            Instruction ORiOp> {
 
 // Constant synthesis previously relied on the ordering of the patterns below.
 // By making the predicates they use non-overlapping, the patterns were
 // reordered so that the effect of the newly introduced predicates can be
 // observed.
 
 // Arbitrary immediates
 def : MipsPat<(VT LUiORiPred:$imm),
               (ORiOp (LUiOp (HI16 imm:$imm)), (LO16 imm:$imm))>;
 
 // Bits 32-16 set, sign/zero extended.
 def : MipsPat<(VT LUiPred:$imm), (LUiOp (HI16 imm:$imm))>;
 
 // Small immediates
 def : MipsPat<(VT ORiPred:$imm), (ORiOp ZEROReg, imm:$imm)>;
 def : MipsPat<(VT immSExt16:$imm), (ADDiuOp ZEROReg, imm:$imm)>;
 }
 
 let AdditionalPredicates = [NotInMicroMips] in
   defm : MaterializeImms<i32, ZERO, ADDiu, LUi, ORi>, ISA_MIPS1;
 
 // Carry MipsPatterns
 let AdditionalPredicates = [NotInMicroMips] in {
   def : MipsPat<(subc GPR32:$lhs, GPR32:$rhs),
                 (SUBu GPR32:$lhs, GPR32:$rhs)>, ISA_MIPS1;
 }
 def : MipsPat<(addc GPR32:$lhs, GPR32:$rhs),
               (ADDu GPR32:$lhs, GPR32:$rhs)>, ISA_MIPS1, ASE_NOT_DSP;
 def : MipsPat<(addc  GPR32:$src, immSExt16:$imm),
               (ADDiu GPR32:$src, imm:$imm)>, ISA_MIPS1, ASE_NOT_DSP;
 
 // Support multiplication for pre-Mips32 targets that don't have
 // the MUL instruction.
 def : MipsPat<(mul GPR32:$lhs, GPR32:$rhs),
               (PseudoMFLO (PseudoMULT GPR32:$lhs, GPR32:$rhs))>,
       ISA_MIPS1_NOT_32R6_64R6;
 
 // SYNC
 def : MipsPat<(MipsSync (i32 immz)),
               (SYNC 0)>, ISA_MIPS2;
 
 // Call
 def : MipsPat<(MipsJmpLink (i32 texternalsym:$dst)),
               (JAL texternalsym:$dst)>, ISA_MIPS1;
 //def : MipsPat<(MipsJmpLink GPR32:$dst),
 //              (JALR GPR32:$dst)>;
 
 // Tail call
 let AdditionalPredicates = [NotInMicroMips] in {
   def : MipsPat<(MipsTailCall (iPTR tglobaladdr:$dst)),
                 (TAILCALL tglobaladdr:$dst)>, ISA_MIPS1;
   def : MipsPat<(MipsTailCall (iPTR texternalsym:$dst)),
                 (TAILCALL texternalsym:$dst)>, ISA_MIPS1;
 }
 // hi/lo relocs
 multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,
                           Register ZeroReg, RegisterOperand GPROpnd> {
   def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;
   def : MipsPat<(MipsHi tblockaddress:$in), (Lui tblockaddress:$in)>;
   def : MipsPat<(MipsHi tjumptable:$in), (Lui tjumptable:$in)>;
   def : MipsPat<(MipsHi tconstpool:$in), (Lui tconstpool:$in)>;
   def : MipsPat<(MipsHi texternalsym:$in), (Lui texternalsym:$in)>;
 
   def : MipsPat<(MipsLo tglobaladdr:$in),
                 (Addiu ZeroReg, tglobaladdr:$in)>;
   def : MipsPat<(MipsLo tblockaddress:$in),
                 (Addiu ZeroReg, tblockaddress:$in)>;
   def : MipsPat<(MipsLo tjumptable:$in),
                 (Addiu ZeroReg, tjumptable:$in)>;
   def : MipsPat<(MipsLo tconstpool:$in),
                 (Addiu ZeroReg, tconstpool:$in)>;
   def : MipsPat<(MipsLo tglobaltlsaddr:$in),
                 (Addiu ZeroReg, tglobaltlsaddr:$in)>;
   def : MipsPat<(MipsLo texternalsym:$in),
                 (Addiu ZeroReg, texternalsym:$in)>;
 
   def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),
                 (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;
   def : MipsPat<(add GPROpnd:$hi, (MipsLo tblockaddress:$lo)),
                 (Addiu GPROpnd:$hi, tblockaddress:$lo)>;
   def : MipsPat<(add GPROpnd:$hi, (MipsLo tjumptable:$lo)),
                 (Addiu GPROpnd:$hi, tjumptable:$lo)>;
   def : MipsPat<(add GPROpnd:$hi, (MipsLo tconstpool:$lo)),
                 (Addiu GPROpnd:$hi, tconstpool:$lo)>;
   def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaltlsaddr:$lo)),
                 (Addiu GPROpnd:$hi, tglobaltlsaddr:$lo)>;
   def : MipsPat<(add GPROpnd:$hi, (MipsLo texternalsym:$lo)),
                 (Addiu GPROpnd:$hi, texternalsym:$lo)>;
 }
 
 // wrapper_pic
 class WrapperPat<SDNode node, Instruction ADDiuOp, RegisterClass RC>:
       MipsPat<(MipsWrapper RC:$gp, node:$in), (ADDiuOp RC:$gp, node:$in)>;
 
 let AdditionalPredicates = [NotInMicroMips] in {
   defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>, ISA_MIPS1;
 
   def : MipsPat<(MipsGotHi tglobaladdr:$in), (LUi tglobaladdr:$in)>, ISA_MIPS1;
   def : MipsPat<(MipsGotHi texternalsym:$in), (LUi texternalsym:$in)>,
         ISA_MIPS1;
 
   def : MipsPat<(MipsTlsHi tglobaltlsaddr:$in), (LUi tglobaltlsaddr:$in)>,
         ISA_MIPS1;
 
   // gp_rel relocs
   def : MipsPat<(add GPR32:$gp, (MipsGPRel tglobaladdr:$in)),
                 (ADDiu GPR32:$gp, tglobaladdr:$in)>, ISA_MIPS1, ABI_NOT_N64;
   def : MipsPat<(add GPR32:$gp, (MipsGPRel tconstpool:$in)),
                 (ADDiu GPR32:$gp, tconstpool:$in)>, ISA_MIPS1, ABI_NOT_N64;
 
   def : WrapperPat<tglobaladdr, ADDiu, GPR32>, ISA_MIPS1;
   def : WrapperPat<tconstpool, ADDiu, GPR32>, ISA_MIPS1;
   def : WrapperPat<texternalsym, ADDiu, GPR32>, ISA_MIPS1;
   def : WrapperPat<tblockaddress, ADDiu, GPR32>, ISA_MIPS1;
   def : WrapperPat<tjumptable, ADDiu, GPR32>, ISA_MIPS1;
   def : WrapperPat<tglobaltlsaddr, ADDiu, GPR32>, ISA_MIPS1;
 
   // Mips does not have "not", so we expand our way
   def : MipsPat<(not GPR32:$in),
                 (NOR GPR32Opnd:$in, ZERO)>, ISA_MIPS1;
 }
 
 // extended loads
 let AdditionalPredicates = [NotInMicroMips] in {
   def : MipsPat<(i32 (extloadi1  addr:$src)), (LBu addr:$src)>, ISA_MIPS1;
   def : MipsPat<(i32 (extloadi8  addr:$src)), (LBu addr:$src)>, ISA_MIPS1;
   def : MipsPat<(i32 (extloadi16 addr:$src)), (LHu addr:$src)>, ISA_MIPS1;
 
   // peepholes
   def : MipsPat<(store (i32 0), addr:$dst), (SW ZERO, addr:$dst)>, ISA_MIPS1;
 }
 
 // brcond patterns
 multiclass BrcondPats<RegisterClass RC, Instruction BEQOp, Instruction BEQOp1,
                       Instruction BNEOp, Instruction SLTOp, Instruction SLTuOp,
                       Instruction SLTiOp, Instruction SLTiuOp,
                       Register ZEROReg> {
 def : MipsPat<(brcond (i32 (setne RC:$lhs, 0)), bb:$dst),
               (BNEOp RC:$lhs, ZEROReg, bb:$dst)>;
 def : MipsPat<(brcond (i32 (seteq RC:$lhs, 0)), bb:$dst),
               (BEQOp RC:$lhs, ZEROReg, bb:$dst)>;
 
 def : MipsPat<(brcond (i32 (setge RC:$lhs, RC:$rhs)), bb:$dst),
               (BEQOp1 (SLTOp RC:$lhs, RC:$rhs), ZERO, bb:$dst)>;
 def : MipsPat<(brcond (i32 (setuge RC:$lhs, RC:$rhs)), bb:$dst),
               (BEQOp1 (SLTuOp RC:$lhs, RC:$rhs), ZERO, bb:$dst)>;
 def : MipsPat<(brcond (i32 (setge RC:$lhs, immSExt16:$rhs)), bb:$dst),
               (BEQOp1 (SLTiOp RC:$lhs, immSExt16:$rhs), ZERO, bb:$dst)>;
 def : MipsPat<(brcond (i32 (setuge RC:$lhs, immSExt16:$rhs)), bb:$dst),
               (BEQOp1 (SLTiuOp RC:$lhs, immSExt16:$rhs), ZERO, bb:$dst)>;
 def : MipsPat<(brcond (i32 (setgt RC:$lhs, immSExt16Plus1:$rhs)), bb:$dst),
               (BEQOp1 (SLTiOp RC:$lhs, (Plus1 imm:$rhs)), ZERO, bb:$dst)>;
 def : MipsPat<(brcond (i32 (setugt RC:$lhs, immSExt16Plus1:$rhs)), bb:$dst),
               (BEQOp1 (SLTiuOp RC:$lhs, (Plus1 imm:$rhs)), ZERO, bb:$dst)>;
 
 def : MipsPat<(brcond (i32 (setle RC:$lhs, RC:$rhs)), bb:$dst),
               (BEQOp1 (SLTOp RC:$rhs, RC:$lhs), ZERO, bb:$dst)>;
 def : MipsPat<(brcond (i32 (setule RC:$lhs, RC:$rhs)), bb:$dst),
               (BEQOp1 (SLTuOp RC:$rhs, RC:$lhs), ZERO, bb:$dst)>;
 
 def : MipsPat<(brcond RC:$cond, bb:$dst),
               (BNEOp RC:$cond, ZEROReg, bb:$dst)>;
 }
 let AdditionalPredicates = [NotInMicroMips] in {
   defm : BrcondPats<GPR32, BEQ, BEQ, BNE, SLT, SLTu, SLTi, SLTiu, ZERO>,
          ISA_MIPS1;
   def : MipsPat<(brcond (i32 (setlt i32:$lhs, 1)), bb:$dst),
                 (BLEZ i32:$lhs, bb:$dst)>, ISA_MIPS1;
   def : MipsPat<(brcond (i32 (setgt i32:$lhs, -1)), bb:$dst),
                 (BGEZ i32:$lhs, bb:$dst)>, ISA_MIPS1;
 }
 
 // setcc patterns
 multiclass SeteqPats<RegisterClass RC, Instruction SLTiuOp, Instruction XOROp,
                      Instruction SLTuOp, Register ZEROReg> {
   def : MipsPat<(seteq RC:$lhs, 0),
                 (SLTiuOp RC:$lhs, 1)>;
   def : MipsPat<(setne RC:$lhs, 0),
                 (SLTuOp ZEROReg, RC:$lhs)>;
   def : MipsPat<(seteq RC:$lhs, RC:$rhs),
                 (SLTiuOp (XOROp RC:$lhs, RC:$rhs), 1)>;
   def : MipsPat<(setne RC:$lhs, RC:$rhs),
                 (SLTuOp ZEROReg, (XOROp RC:$lhs, RC:$rhs))>;
 }
 
 multiclass SetlePats<RegisterClass RC, Instruction XORiOp, Instruction SLTOp,
                      Instruction SLTuOp> {
   def : MipsPat<(setle RC:$lhs, RC:$rhs),
                 (XORiOp (SLTOp RC:$rhs, RC:$lhs), 1)>;
   def : MipsPat<(setule RC:$lhs, RC:$rhs),
                 (XORiOp (SLTuOp RC:$rhs, RC:$lhs), 1)>;
 }
 
 multiclass SetgtPats<RegisterClass RC, Instruction SLTOp, Instruction SLTuOp> {
   def : MipsPat<(setgt RC:$lhs, RC:$rhs),
                 (SLTOp RC:$rhs, RC:$lhs)>;
   def : MipsPat<(setugt RC:$lhs, RC:$rhs),
                 (SLTuOp RC:$rhs, RC:$lhs)>;
 }
 
 multiclass SetgePats<RegisterClass RC, Instruction XORiOp, Instruction SLTOp,
                      Instruction SLTuOp> {
   def : MipsPat<(setge RC:$lhs, RC:$rhs),
                 (XORiOp (SLTOp RC:$lhs, RC:$rhs), 1)>;
   def : MipsPat<(setuge RC:$lhs, RC:$rhs),
                 (XORiOp (SLTuOp RC:$lhs, RC:$rhs), 1)>;
 }
 
 multiclass SetgeImmPats<RegisterClass RC, Instruction XORiOp,
                         Instruction SLTiOp, Instruction SLTiuOp> {
   def : MipsPat<(setge RC:$lhs, immSExt16:$rhs),
                 (XORiOp (SLTiOp RC:$lhs, immSExt16:$rhs), 1)>;
   def : MipsPat<(setuge RC:$lhs, immSExt16:$rhs),
                 (XORiOp (SLTiuOp RC:$lhs, immSExt16:$rhs), 1)>;
 }
 
 let AdditionalPredicates = [NotInMicroMips] in {
   defm : SeteqPats<GPR32, SLTiu, XOR, SLTu, ZERO>, ISA_MIPS1;
   defm : SetlePats<GPR32, XORi, SLT, SLTu>, ISA_MIPS1;
   defm : SetgtPats<GPR32, SLT, SLTu>, ISA_MIPS1;
   defm : SetgePats<GPR32, XORi, SLT, SLTu>, ISA_MIPS1;
   defm : SetgeImmPats<GPR32, XORi, SLTi, SLTiu>, ISA_MIPS1;
 
   // bswap pattern
   def : MipsPat<(bswap GPR32:$rt), (ROTR (WSBH GPR32:$rt), 16)>, ISA_MIPS32R2;
 }
 
 // Load halfword/word patterns.
 let AdditionalPredicates = [NotInMicroMips] in {
   let AddedComplexity = 40 in {
     def : LoadRegImmPat<LBu, i32, zextloadi8>, ISA_MIPS1;
     def : LoadRegImmPat<LHu, i32, zextloadi16>, ISA_MIPS1;
     def : LoadRegImmPat<LB, i32, sextloadi8>, ISA_MIPS1;
     def : LoadRegImmPat<LH, i32, sextloadi16>, ISA_MIPS1;
     def : LoadRegImmPat<LW, i32, load>, ISA_MIPS1;
   }
 
   // Atomic load patterns.
   def : MipsPat<(atomic_load_8 addr:$a), (LB addr:$a)>, ISA_MIPS1;
   def : MipsPat<(atomic_load_16 addr:$a), (LH addr:$a)>, ISA_MIPS1;
   def : MipsPat<(atomic_load_32 addr:$a), (LW addr:$a)>, ISA_MIPS1;
 
   // Atomic store patterns.
   def : MipsPat<(atomic_store_8 addr:$a, GPR32:$v), (SB GPR32:$v, addr:$a)>,
         ISA_MIPS1;
   def : MipsPat<(atomic_store_16 addr:$a, GPR32:$v), (SH GPR32:$v, addr:$a)>,
         ISA_MIPS1;
   def : MipsPat<(atomic_store_32 addr:$a, GPR32:$v), (SW GPR32:$v, addr:$a)>,
         ISA_MIPS1;
 }
 
 //===----------------------------------------------------------------------===//
 // Floating Point Support
 //===----------------------------------------------------------------------===//
 
 include "MipsInstrFPU.td"
 include "Mips64InstrInfo.td"
 include "MipsCondMov.td"
 
 include "Mips32r6InstrInfo.td"
 include "Mips64r6InstrInfo.td"
 
 //
 // Mips16
 
 include "Mips16InstrFormats.td"
 include "Mips16InstrInfo.td"
 
 // DSP
 include "MipsDSPInstrFormats.td"
 include "MipsDSPInstrInfo.td"
 
 // MSA
 include "MipsMSAInstrFormats.td"
 include "MipsMSAInstrInfo.td"
 
 // EVA
 include "MipsEVAInstrFormats.td"
 include "MipsEVAInstrInfo.td"
 
 // MT
 include "MipsMTInstrFormats.td"
 include "MipsMTInstrInfo.td"
 
 // Micromips
 include "MicroMipsInstrFormats.td"
 include "MicroMipsInstrInfo.td"
 include "MicroMipsInstrFPU.td"
 
 // Micromips r6
 include "MicroMips32r6InstrFormats.td"
 include "MicroMips32r6InstrInfo.td"
 
 // Micromips DSP
 include "MicroMipsDSPInstrFormats.td"
 include "MicroMipsDSPInstrInfo.td"
diff --git a/llvm/lib/Target/Mips/MipsScheduleGeneric.td b/llvm/lib/Target/Mips/MipsScheduleGeneric.td
index f076f2f9cf10..b4da840d060a 100644
--- a/llvm/lib/Target/Mips/MipsScheduleGeneric.td
+++ b/llvm/lib/Target/Mips/MipsScheduleGeneric.td
@@ -1,1623 +1,1622 @@
 //=- MipsScheduleGeneric.td - Generic Scheduling Definitions -*- tablegen -*-=//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
 //
 // This file describes the interAptiv processor in a manner of speaking. It
 // describes a hypothetical version of the in-order MIPS32R2 interAptiv with all
 // branches of the MIPS ISAs, ASEs and ISA variants. The itinerary lists are
 // broken down into per ISA lists, so that this file can be used to rapidly
 // develop new schedule models.
 //
 //===----------------------------------------------------------------------===//
 def MipsGenericModel : SchedMachineModel {
   int IssueWidth = 1;
   int MicroOpBufferSize = 0;
 
   // These figures assume an L1 hit.
   int LoadLatency = 2;
   int MispredictPenalty = 4;
 
   int HighLatency = 37;
   list<Predicate> UnsupportedFeatures = [];
 
   let CompleteModel = 1;
   let PostRAScheduler = 1;
 
   // FIXME: Remove when all errors have been fixed.
   let FullInstRWOverlapCheck = 1;
 }
 
 let SchedModel = MipsGenericModel in {
 
 // ALU Pipeline
 // ============
 
 def GenericALU : ProcResource<1> { let BufferSize = 1; }
 def GenericIssueALU : ProcResource<1> { let Super = GenericALU; }
 
 def GenericWriteALU : SchedWriteRes<[GenericIssueALU]>;
 
 // add, addi, addiu, addu, and, andi, clo, clz, ext, ins, lui, nor, or, ori,
 // rotr, rotrv, seb, seh, sll, sllv, slt, slti, sltiu, sltu, sra, srav, srl,
 // srlv, ssnop, sub, subu, wsbh, xor, xori
 def : InstRW<[GenericWriteALU], (instrs ADD, ADDi, ADDiu, ADDu, AND, ANDi,
                                  CLO, CLZ, EXT, INS, LEA_ADDiu, LUi, NOP,
                                  NOR, OR, ORi, ROTR, ROTRV, SEB, SEH, SLL,
                                  SLLV, SLT, SLTi, SLTiu, SLTu, SRA, SRAV, SRL,
                                  SRLV, SSNOP, SUB, SUBu, WSBH, XOR, XORi)>;
 
 def : InstRW<[GenericWriteALU], (instrs COPY)>;
 
 // MIPSR6
 // ======
 
 // addiupc, align, aluipc, aui, auipc, bitswap, clo, clz, lsa, seleqz, selnez
 def : InstRW<[GenericWriteALU], (instrs ADDIUPC, ALIGN, ALUIPC, AUI,
                                  AUIPC, BITSWAP, CLO_R6, CLZ_R6, LSA_R6,
                                  SELEQZ, SELNEZ)>;
 
 // MIPS16e
 // =======
 
 def : InstRW<[GenericWriteALU], (instrs AddiuRxImmX16, AddiuRxRxImm16,
                                  AddiuRxRxImmX16, AddiuRxRyOffMemX16,
                                  AddiuRxPcImmX16, AddiuSpImm16, AddiuSpImmX16,
                                  AdduRxRyRz16, AndRxRxRy16, CmpRxRy16,
                                  CmpiRxImm16, CmpiRxImmX16, LiRxImm16,
                                  LiRxImmX16, LiRxImmAlignX16, Move32R16,
                                  MoveR3216, Mfhi16, Mflo16, NegRxRy16,
                                  NotRxRy16, OrRxRxRy16, SebRx16, SehRx16,
                                  SllX16, SllvRxRy16, SltiRxImm16,
                                  SltiRxImmX16, SltiCCRxImmX16,
                                  SltiuRxImm16, SltiuRxImmX16, SltiuCCRxImmX16,
                                  SltRxRy16, SltCCRxRy16, SltuRxRy16,
                                  SltuRxRyRz16, SltuCCRxRy16, SravRxRy16,
                                  SraX16, SrlvRxRy16, SrlX16, SubuRxRyRz16,
                                  XorRxRxRy16)>;
 
 def : InstRW<[GenericWriteALU], (instrs Constant32, LwConstant32,
                                  GotPrologue16, CONSTPOOL_ENTRY)>;
 
 // microMIPS
 // =========
 
 def : InstRW<[GenericWriteALU], (instrs ADDIUPC_MM, ADDIUR1SP_MM, ADDIUR2_MM,
                                  ADDIUS5_MM, ADDIUSP_MM, ADDU16_MM, ADD_MM,
                                  ADDi_MM, ADDiu_MM, ADDu_MM, AND16_MM,
                                  ANDI16_MM, AND_MM, ANDi_MM, CLO_MM, CLZ_MM,
                                  EXT_MM, INS_MM, LEA_ADDiu_MM, LI16_MM,
                                  LUi_MM, MOVE16_MM, MOVEP_MM, NOR_MM,
                                  NOT16_MM, OR16_MM, OR_MM, ORi_MM, ROTRV_MM,
                                  ROTR_MM, SEB_MM, SEH_MM, SLL16_MM, SLLV_MM,
                                  SLL_MM, SLT_MM, SLTi_MM, SLTiu_MM, SLTu_MM,
                                  SRAV_MM, SRA_MM, SRL16_MM, SRLV_MM, SRL_MM,
                                  SSNOP_MM, SUBU16_MM, SUB_MM, SUBu_MM,
                                  WSBH_MM, XOR16_MM, XOR_MM, XORi_MM)>;
 
 // microMIPS32r6
 // =============
 
 def : InstRW<[GenericWriteALU], (instrs ADDIUPC_MMR6, ADDIU_MMR6, ADDU16_MMR6,
                                  ADDU_MMR6, ADD_MMR6, ALIGN_MMR6, ALUIPC_MMR6,
                                  AND16_MMR6, ANDI16_MMR6, ANDI_MMR6, AND_MMR6,
                                  AUIPC_MMR6, AUI_MMR6, BITSWAP_MMR6, CLO_MMR6,
                                  CLZ_MMR6, EXT_MMR6, INS_MMR6, LI16_MMR6,
                                  LSA_MMR6, LUI_MMR6, MOVE16_MMR6, NOR_MMR6,
                                  NOT16_MMR6, OR16_MMR6, ORI_MMR6, OR_MMR6,
                                  SELEQZ_MMR6, SELNEZ_MMR6, SLL16_MMR6,
                                  SLL_MMR6, SRL16_MMR6, SSNOP_MMR6, SUBU16_MMR6,
                                  SUBU_MMR6, SUB_MMR6, WSBH_MMR6, XOR16_MMR6,
 										             XORI_MMR6, XOR_MMR6)>;
 
 // MIPS64
 // ======
 
 def : InstRW<[GenericWriteALU], (instrs AND64, ANDi64, DEXT64_32, DSLL64_32,
                                  ORi64, SEB64, SEH64, SLL64_32, SLL64_64,
                                  SLT64, SLTi64, SLTiu64, SLTu64, XOR64,
                                  XORi64)>;
 
 def : InstRW<[GenericWriteALU], (instrs DADD, DADDi, DADDiu, DADDu, DCLO,
                                  DCLZ, DEXT, DEXTM, DEXTU, DINS, DINSM, DINSU,
                                  DROTR, DROTR32, DROTRV, DSBH, DSHD, DSLL,
                                  DSLL32, DSLLV, DSRA, DSRA32, DSRAV, DSRL,
                                  DSRL32, DSRLV, DSUB, DSUBu, LEA_ADDiu64,
                                  LUi64, NOR64, OR64)>;
 
 // MIPS64R6
 // ========
 
 def : InstRW<[GenericWriteALU], (instrs DALIGN, DAHI, DATI, DAUI, DCLO_R6,
                                  DCLZ_R6, DBITSWAP, DLSA, DLSA_R6, SELEQZ64,
                                  SELNEZ64)>;
 
 
 def GenericMDU : ProcResource<1> { let BufferSize = 1; }
 def GenericIssueMDU : ProcResource<1> { let Super = GenericALU; }
 def GenericIssueDIV : ProcResource<1> { let Super = GenericMDU; }
 def GenericWriteHILO : SchedWriteRes<[GenericIssueMDU]>;
 def GenericWriteALULong : SchedWriteRes<[GenericIssueALU]> { let Latency = 5; }
 def GenericWriteMove : SchedWriteRes<[GenericIssueALU]> { let Latency = 2; }
 def GenericWriteMul : SchedWriteRes<[GenericIssueMDU]> { let Latency = 4; }
 
 def : InstRW<[GenericWriteHILO], (instrs MADD, MADDU, MSUB, MSUBU)>;
 
 def : InstRW<[GenericWriteHILO], (instrs PseudoMADD_MM, PseudoMADDU_MM,
                                   PseudoMSUB_MM, PseudoMSUBU_MM,
                                   PseudoMULT_MM, PseudoMULTu_MM)>;
 
 def : InstRW<[GenericWriteHILO], (instrs PseudoMADD, PseudoMADDU, PseudoMSUB,
                                   PseudoMSUBU, PseudoMULT, PseudoMULTu)>;
 
 def GenericWriteMDUtoGPR : SchedWriteRes<[GenericIssueMDU]> {
   let Latency = 5;
 }
 
 def GenericWriteDIV : SchedWriteRes<[GenericIssueDIV]> {
   // Estimated worst case
   let Latency = 33;
   let ResourceCycles = [33];
 }
 def GenericWriteDIVU : SchedWriteRes<[GenericIssueDIV]> {
   // Estimated worst case
   let Latency = 31;
   let ResourceCycles = [31];
 }
 
 // mul
 def : InstRW<[GenericWriteMDUtoGPR], (instrs MUL)>;
 
 // mult, multu
 def : InstRW<[GenericWriteMul], (instrs MULT, MULTu)>;
 
 // div, sdiv
 def : InstRW<[GenericWriteDIV], (instrs PseudoSDIV, SDIV)>;
 
 def : InstRW<[GenericWriteDIVU], (instrs PseudoUDIV, UDIV)>;
 
 // mfhi, mflo, movn, mthi, mtlo, rdwhr
 def : InstRW<[GenericWriteALULong], (instrs MFHI, MFLO, PseudoMFHI,
                                      PseudoMFLO)>;
 
 def : InstRW<[GenericWriteALULong], (instrs PseudoMFHI_MM, PseudoMFLO_MM)>;
 
 def : InstRW<[GenericWriteMove], (instrs MTHI, MTLO, RDHWR, PseudoMTLOHI)>;
 def : InstRW<[GenericWriteMove], (instrs PseudoMTLOHI_MM)>;
 
 def : InstRW<[GenericWriteALU], (instrs MOVN_I_I, MOVZ_I_I)>;
 
 // MIPSR6
 // ======
 
 // muh, muhu, mulu, mul
 def : InstRW<[GenericWriteMul], (instrs MUH, MUHU, MULU, MUL_R6)>;
 
 // divu, udiv
 def : InstRW<[GenericWriteDIV], (instrs MOD, MODU, DIV, DIVU)>;
 
 
 // MIPS16e
 // =======
 
-def : InstRW<[GenericWriteHILO], (instrs MultRxRy16, MultuRxRy16,
-                                  MultRxRyRz16, MultuRxRyRz16)>;
+def : InstRW<[GenericWriteHILO], (instrs MultRxRy16, MultuRxRy16)>;
 
 def : InstRW<[GenericWriteDIV], (instrs DivRxRy16)>;
 
 def : InstRW<[GenericWriteDIVU], (instrs DivuRxRy16)>;
 
 // microMIPS
 // =========
 
 def : InstRW<[GenericWriteMul], (instrs MULT_MM, MULTu_MM, MADD_MM, MADDU_MM,
                                  MSUB_MM, MSUBU_MM)>;
 
 def : InstRW<[GenericWriteALULong], (instrs MUL_MM)>;
 
 def : InstRW<[GenericWriteDIV], (instrs SDIV_MM, SDIV_MM_Pseudo)>;
 
 def : InstRW<[GenericWriteDIVU], (instrs UDIV_MM, UDIV_MM_Pseudo)>;
 
 def : InstRW<[GenericWriteMove], (instrs MFHI16_MM, MFLO16_MM, MOVF_I_MM,
                                   MOVT_I_MM, MFHI_MM, MFLO_MM, MTHI_MM,
                                   MTLO_MM)>;
 
 def : InstRW<[GenericWriteMove], (instrs RDHWR_MM)>;
 
 // microMIPS32r6
 // =============
 
 def : InstRW<[GenericWriteMul], (instrs MUHU_MMR6, MUH_MMR6, MULU_MMR6,
                                  MUL_MMR6)>;
 
 def : InstRW<[GenericWriteDIV], (instrs MODU_MMR6, MOD_MMR6, DIVU_MMR6,
                                  DIV_MMR6)>;
 
 def : InstRW<[GenericWriteMove], (instrs RDHWR_MMR6)>;
 
 // MIPS64
 // ======
 
 def : InstRW<[GenericWriteHILO], (instrs DMULU, DMULT, DMULTu, PseudoDMULT,
                                   PseudoDMULTu)>;
 
 def : InstRW<[GenericWriteDIV], (instrs DSDIV, PseudoDSDIV)>;
 
 def : InstRW<[GenericWriteDIVU], (instrs DUDIV, PseudoDUDIV)>;
 
 def : InstRW<[GenericWriteALULong], (instrs MFHI64, MFLO64, PseudoMFHI64,
                                      PseudoMFLO64, PseudoMTLOHI64)>;
 
 def : InstRW<[GenericWriteMove], (instrs MTHI64, MTLO64, RDHWR64)>;
 
 // mov[zn]
 def : InstRW<[GenericWriteALU], (instrs MOVN_I_I64, MOVN_I64_I, MOVN_I64_I64,
                                  MOVZ_I_I64, MOVZ_I64_I, MOVZ_I64_I64)>;
 
 
 // MIPS64R6
 // ========
 
 def : InstRW<[GenericWriteMDUtoGPR], (instrs DMUH, DMUHU, DMUL_R6)>;
 
 def : InstRW<[GenericWriteDIV], (instrs DDIV, DMOD)>;
 
 def : InstRW<[GenericWriteDIVU], (instrs DDIVU, DMODU)>;
 
 // CTISTD Pipeline
 // ---------------
 
 def GenericIssueCTISTD : ProcResource<1> { let Super = GenericALU; }
 
 def GenericLDST : ProcResource<1> { let BufferSize = 1; }
 def GenericIssueLDST : ProcResource<1> { let Super = GenericLDST; }
 
 def GenericWriteJump : SchedWriteRes<[GenericIssueCTISTD]>;
 def GenericWriteJumpAndLink : SchedWriteRes<[GenericIssueCTISTD]> {
   let Latency = 2;
 }
 
 // b, beq, beql, bg[et]z, bl[et]z, bne, bnel, j, syscall, jal, bltzal, jalx,
 // jalr, jr.hb, jr, jalr.hb, jarlc, jialc
 def : InstRW<[GenericWriteJump], (instrs B, BAL, BAL_BR, BEQ, BNE, BGTZ, BGEZ,
                                   BLEZ, BLTZ, BLTZAL, J, JALX, JR, JR_HB, ERET,
                                   ERet, ERETNC, DERET)>;
 
 def : InstRW<[GenericWriteJump], (instrs BEQL, BNEL, BGEZL, BGTZL, BLEZL,
                                   BLTZL)>;
 
 def : InstRW<[GenericWriteJump], (instrs TAILCALL, TAILCALLREG,
                                   TAILCALLREGHB, PseudoIndirectBranch,
                                   PseudoIndirectHazardBranch, PseudoReturn,
                                   RetRA)>;
 
 def : InstRW<[GenericWriteJumpAndLink], (instrs BGEZAL, JAL, JALR, JALR_HB,
                                          JALRHBPseudo, JALRPseudo)>;
 
 def : InstRW<[GenericWriteJumpAndLink], (instrs BGEZALL, BLTZALL)>;
 
 def GenericWriteTrap : SchedWriteRes<[GenericIssueCTISTD]>;
 
 def : InstRW<[GenericWriteTrap], (instrs BREAK, SYSCALL, TEQ, TEQI,
                                   TGE, TGEI, TGEIU, TGEU, TNE,
                                   TNEI, TLT, TLTI, TLTU, TTLTIU,
                                   TRAP, SDBBP)>;
 
 // MIPSR6
 // ======
 
 def : InstRW<[GenericWriteJumpAndLink], (instrs BALC, BEQZALC, BGEZALC,
                                          BGTZALC, BLEZALC, BLTZALC,
                                          BNEZALC,
                                          JIALC)>;
 
 def : InstRW<[GenericWriteJump], (instrs BC, BC2EQZ, BC2NEZ, BEQC, BEQZC, BGEC,
                                   BGEUC, BGEZC, BGTZC, BLEZC, BLTC, BLTUC,
                                   BLTZC, BNEC, BNEZC, BNVC, BOVC, JIC, JR_HB_R6,
                                   SIGRIE, PseudoIndirectBranchR6,
                                   PseudoIndrectHazardBranchR6)>;
 
 def : InstRW<[GenericWriteJump], (instrs TAILCALLR6REG, TAILCALLHBR6REG)>;
 
 def : InstRW<[GenericWriteTrap], (instrs SDBBP_R6)>;
 
 // MIPS16e
 // =======
 
 def : InstRW<[GenericWriteJump], (instrs Bimm16, BimmX16, BeqzRxImm16,
                                   BeqzRxImmX16, BnezRxImm16, BnezRxImmX16,
                                   Bteqz16, BteqzX16, BteqzT8CmpX16,
                                   BteqzT8CmpiX16, BteqzT8SltX16,
                                   BteqzT8SltuX16, BteqzT8SltiX16,
                                   BteqzT8SltiuX16, Btnez16, BtnezX16,
                                   BtnezT8CmpX16, BtnezT8CmpiX16,
                                   BtnezT8SltX16, BtnezT8SltuX16,
                                   BtnezT8SltiX16, BtnezT8SltiuX16, JrRa16,
                                   JrcRa16, JrcRx16, RetRA16)>;
 
 def : InstRW<[GenericWriteJumpAndLink], (instrs Jal16, JalB16, JumpLinkReg16)>;
 
 def : InstRW<[GenericWriteTrap], (instrs Break16)>;
 
 def : InstRW<[GenericWriteALULong], (instrs SelBeqZ, SelTBteqZCmp,
                                      SelTBteqZCmpi, SelTBteqZSlt,
                                      SelTBteqZSlti, SelTBteqZSltu,
                                      SelTBteqZSltiu, SelBneZ, SelTBtneZCmp,
                                      SelTBtneZCmpi, SelTBtneZSlt,
                                      SelTBtneZSlti, SelTBtneZSltu,
                                      SelTBtneZSltiu)>;
 
 // microMIPS
 // =========
 
 def : InstRW<[GenericWriteJump], (instrs B16_MM, BAL_BR_MM, BC1F_MM, BC1T_MM,
                                   BEQZ16_MM, BEQZC_MM, BEQ_MM, BGEZ_MM,
                                   BGTZ_MM, BLEZ_MM, BLTZ_MM, BNEZ16_MM,
                                   BNEZC_MM, BNE_MM, B_MM, DERET_MM, ERET_MM,
                                   JR16_MM, JR_MM, J_MM, B_MM_Pseudo)>;
 
 def : InstRW<[GenericWriteJumpAndLink], (instrs BGEZALS_MM, BGEZAL_MM,
                                          BLTZALS_MM, BLTZAL_MM, JALR16_MM,
                                          JALRS16_MM, JALRS_MM, JALR_MM,
                                          JALS_MM, JALX_MM, JAL_MM)>;
 
 def : InstRW<[GenericWriteJump], (instrs TAILCALLREG_MM, TAILCALL_MM,
                                   PseudoIndirectBranch_MM)>;
 
 def : InstRW<[GenericWriteTrap], (instrs BREAK16_MM, BREAK_MM, SDBBP16_MM,
                                   SDBBP_MM, SYSCALL_MM, TEQI_MM, TEQ_MM,
                                   TGEIU_MM, TGEI_MM, TGEU_MM, TGE_MM, TLTIU_MM,
                                   TLTI_MM, TLTU_MM, TLT_MM, TNEI_MM, TNE_MM,
                                   TRAP_MM)>;
 
 // microMIPS32r6
 // =============
 
 def : InstRW<[GenericWriteJump], (instrs BC16_MMR6, BC1EQZC_MMR6, BC1NEZC_MMR6,
                                   BC2EQZC_MMR6, BC2NEZC_MMR6, BC_MMR6,
                                   BEQC_MMR6, BEQZC16_MMR6, BEQZC_MMR6,
                                   BGEC_MMR6, BGEUC_MMR6, BGEZC_MMR6,
                                   BGTZC_MMR6, BLEZC_MMR6, BLTC_MMR6,
                                   BLTUC_MMR6, BLTZC_MMR6, BNEC_MMR6,
                                   BNEZC16_MMR6, BNEZC_MMR6, BNVC_MMR6,
                                   BOVC_MMR6, DERET_MMR6, ERETNC_MMR6, JAL_MMR6,
                                   ERET_MMR6, JIC_MMR6, JRADDIUSP, JRC16_MM,
                                   JRC16_MMR6, JRCADDIUSP_MMR6, SIGRIE_MMR6,
                                   B_MMR6_Pseudo, PseudoIndirectBranch_MMR6)>;
 
 def : InstRW<[GenericWriteJumpAndLink], (instrs BALC_MMR6, BEQZALC_MMR6,
                                          BGEZALC_MMR6, BGTZALC_MMR6,
                                          BLEZALC_MMR6, BLTZALC_MMR6,
                                          BNEZALC_MMR6, JALRC16_MMR6,
                                          JALRC_HB_MMR6, JALRC_MMR6,
                                          JIALC_MMR6)>;
 
 def : InstRW<[GenericWriteJump], (instrs TAILCALLREG_MMR6, TAILCALL_MMR6)>;
 
 def : InstRW<[GenericWriteTrap], (instrs BREAK16_MMR6, BREAK_MMR6, SDBBP_MMR6,
                                   SDBBP16_MMR6)>;
 
 // MIPS64
 // ======
 
 def : InstRW<[GenericWriteJump], (instrs BEQ64, BGEZ64, BGTZ64, BLEZ64,
                                   BLTZ64, BNE64, JR64)>;
 
 def : InstRW<[GenericWriteJumpAndLink], (instrs JALR64, JALR64Pseudo,
                                          JALRHB64Pseudo, JALR_HB64)>;
 
 def : InstRW<[GenericWriteJump], (instrs JR_HB64, TAILCALLREG64,
                                   TAILCALLREGHB64, PseudoReturn64)>;
 
 // MIPS64R6
 // ========
 
 def : InstRW<[GenericWriteJump], (instrs BEQC64, BEQZC64, BGEC64, BGEUC64,
                                   BGEZC64, BGTZC64, BLEZC64, BLTC64, BLTUC64,
                                   BLTZC64, BNEC64, BNEZC64, JIC64,
                                   PseudoIndirectBranch64,
                                   PseudoIndirectHazardBranch64)>;
 
 def : InstRW<[GenericWriteJumpAndLink], (instrs JIALC64)>;
 
 def : InstRW<[GenericWriteJump], (instrs JR_HB64_R6, TAILCALL64R6REG,
                                   TAILCALLHB64R6REG, PseudoIndirectBranch64R6,
                                   PseudoIndrectHazardBranch64R6)>;
 
 // COP0 Pipeline
 // =============
 
 def GenericCOP0 : ProcResource<1> { let BufferSize = 1; }
 
 def GenericIssueCOP0 : ProcResource<1> { let Super = GenericCOP0; }
 def GenericWriteCOP0TLB : SchedWriteRes<[GenericIssueCOP0]> { let Latency = 4; }
 def GenericWriteCOP0 : SchedWriteRes<[GenericIssueCOP0]> { let Latency = 3; }
 def GenericReadCOP0 : SchedWriteRes<[GenericIssueCOP0]> { let Latency = 2; }
 def GenericReadWritePGPR : SchedWriteRes<[GenericIssueCOP0]>;
 def GenericReadWriteCOP0Long : SchedWriteRes<[GenericIssueCOP0]> {
   let Latency = 5;
 }
 def GenericWriteCOP0Short : SchedWriteRes<[GenericIssueCOP0]>;
 
 def : InstRW<[GenericWriteCOP0TLB], (instrs TLBP, TLBR, TLBWI, TLBWR)>;
 def : InstRW<[GenericWriteCOP0TLB], (instrs TLBINV, TLBINVF)>;
 
 def : InstRW<[GenericReadCOP0], (instrs MFC0)>;
 def : InstRW<[GenericWriteCOP0], (instrs MTC0)>;
 
 def : InstRW<[GenericWriteCOP0], (instrs EVP, DVP)>;
 
 def : InstRW<[GenericWriteCOP0], (instrs DI, EI)>;
 
 def : InstRW<[GenericWriteCOP0], (instrs EHB, PAUSE, WAIT)>;
 
 // microMIPS
 // =========
 
 def : InstRW<[GenericWriteCOP0TLB], (instrs TLBP_MM, TLBR_MM, TLBWI_MM,
                                      TLBWR_MM)>;
 
 def : InstRW<[GenericWriteCOP0], (instrs DI_MM, EI_MM)>;
 
 def : InstRW<[GenericWriteCOP0], (instrs EHB_MM, PAUSE_MM, WAIT_MM)>;
 
 
 // microMIPS32R6
 // =============
 
 def : InstRW<[GenericWriteCOP0], (instrs RDPGPR_MMR6, WRPGPR_MMR6)>;
 
 def : InstRW<[GenericWriteCOP0TLB], (instrs TLBINV_MMR6, TLBINVF_MMR6)>;
 
 def : InstRW<[GenericReadCOP0], (instrs MFHC0_MMR6, MFC0_MMR6, MFHC2_MMR6,
                                  MFC2_MMR6)>;
 
 def : InstRW<[GenericWriteCOP0], (instrs MTHC0_MMR6, MTC0_MMR6, MTHC2_MMR6,
                                   MTC2_MMR6)>;
 
 def : InstRW<[GenericWriteCOP0], (instrs EVP_MMR6, DVP_MMR6)>;
 
 def : InstRW<[GenericWriteCOP0], (instrs DI_MMR6, EI_MMR6)>;
 
 def : InstRW<[GenericWriteCOP0], (instrs EHB_MMR6, PAUSE_MMR6, WAIT_MMR6)>;
 
 // MIPS64
 // ======
 
 def : InstRW<[GenericReadCOP0], (instrs DMFC0)>;
 
 def : InstRW<[GenericWriteCOP0], (instrs DMTC0)>;
 
 
 def GenericCOP2 : ProcResource<1> { let BufferSize = 1; }
 def GenericWriteCOPOther : SchedWriteRes<[GenericCOP2]>;
 
 def : InstRW<[GenericWriteCOPOther], (instrs MFC2, MTC2)>;
 
 def : InstRW<[GenericWriteCOPOther], (instrs DMFC2, DMTC2)>;
 
 // microMIPS32R6
 // =============
 
 // The latency and repeat rate of these instructions are implementation
 // dependant.
 def : InstRW<[GenericWriteMove], (instrs CFC2_MM, CTC2_MM)>;
 
 
 // MIPS MT ASE - hasMT
 // ====================
 
 def : InstRW<[GenericWriteMove], (instrs DMT, DVPE, EMT, EVPE, MFTR,
                                   MTTR)>;
 
 def : InstRW<[GenericReadWriteCOP0Long], (instrs YIELD)>;
 
 def : InstRW<[GenericWriteCOP0Short], (instrs FORK)>;
 
 // MIPS Virtualization ASE
 // =======================
 
 def : InstRW<[GenericWriteCOP0Short], (instrs HYPCALL, TLBGINV, TLBGINVF, TLBGP,
                                        TLBGR, TLBGWI, TLBGWR, MFGC0, MFHGC0,
                                        MTGC0, MTHGC0)>;
 
 // MIPS64 Virtualization ASE
 // =========================
 
 def : InstRW<[GenericWriteCOP0Short], (instrs DMFGC0, DMTGC0)>;
 
 // microMIPS virtualization ASE
 // ============================
 
 def : InstRW<[GenericWriteCOP0Short], (instrs HYPCALL_MM, TLBGINVF_MM,
                                        TLBGINV_MM, TLBGP_MM, TLBGR_MM,
                                        TLBGWI_MM, TLBGWR_MM, MFGC0_MM,
                                        MFHGC0_MM, MTGC0_MM, MTHGC0_MM)>;
 
 // LDST Pipeline
 // -------------
 
 def GenericWriteLoad : SchedWriteRes<[GenericIssueLDST]> {
   let Latency = 2;
 }
 
 def GenericWritePref : SchedWriteRes<[GenericIssueLDST]>;
 def GenericWriteSync : SchedWriteRes<[GenericIssueLDST]>;
 def GenericWriteCache : SchedWriteRes<[GenericIssueLDST]> { let Latency = 5; }
 
 def GenericWriteStore : SchedWriteRes<[GenericIssueLDST]>;
 def GenericWriteStoreSC : SchedWriteRes<[GenericIssueLDST]> { let Latency = 2; }
 
 def GenericWriteGPRFromBypass : SchedWriteRes<[GenericIssueLDST]> {
   let Latency = 2;
 }
 
 def GenericWriteStoreFromOtherUnits : SchedWriteRes<[GenericIssueLDST]>;
 def GenericWriteLoadToOtherUnits : SchedWriteRes<[GenericIssueLDST]> {
   let Latency = 0;
 }
 
 // l[bhw], l[bh]u, ll
 def : InstRW<[GenericWriteLoad], (instrs LB, LBu, LH, LHu, LW, LL,
                                   LWC2, LWC3, LDC2, LDC3)>;
 
 // lw[lr]
 def : InstRW<[GenericWriteLoad], (instrs LWL, LWR)>;
 
 // s[bhw], sc, s[dw]c[23]
 def : InstRW<[GenericWriteStore], (instrs SB, SH, SW, SWC2, SWC3,
                                    SDC2, SDC3)>;
 
 // PreMIPSR6 sw[lr]
 def : InstRW<[GenericWriteStore], (instrs SWL, SWR)>;
 
 def : InstRW<[GenericWriteStoreSC], (instrs SC, SC_MMR6)>;
 
 // pref
 def : InstRW<[GenericWritePref], (instrs PREF)>;
 // cache
 def : InstRW<[GenericWriteCache], (instrs CACHE)>;
 
 // sync
 def : InstRW<[GenericWriteSync], (instrs SYNC, SYNCI)>;
 
 // MIPSR6
 // ======
 
 def : InstRW<[GenericWriteLoad], (instrs LDC2_R6, LL_R6, LWC2_R6, LWPC)>;
 
 def : InstRW<[GenericWriteStore], (instrs SWC2_R6,  SDC2_R6)>;
 
 def : InstRW<[GenericWriteStoreSC], (instrs SC_R6)>;
 
 def : InstRW<[GenericWritePref], (instrs PREF_R6)>;
 
 def : InstRW<[GenericWriteCache], (instrs CACHE_R6)>;
 
 def : InstRW<[GenericWriteSync], (instrs GINVI, GINVT)>;
 
 // MIPS32 EVA
 // ==========
 
 def : InstRW<[GenericWriteLoad], (instrs LBE, LBuE, LHE, LHuE, LWE,
                                   LLE)>;
 
 def : InstRW<[GenericWriteStore], (instrs SBE, SHE, SWE, SCE)>;
 
 def : InstRW<[GenericWriteLoad], (instrs LWLE, LWRE)>;
 
 def : InstRW<[GenericWriteStore], (instrs SWLE, SWRE)>;
 
 def : InstRW<[GenericWritePref], (instrs PREFE)>;
 
 def : InstRW<[GenericWriteCache], (instrs CACHEE)>;
 
 // microMIPS EVA ASE - InMicroMipsMode, hasEVA
 // ===========================================
 
 def : InstRW<[GenericWriteLoad], (instrs LBE_MM, LBuE_MM, LHE_MM, LHuE_MM,
                                   LWE_MM, LWLE_MM, LWRE_MM, LLE_MM)>;
 
 def : InstRW<[GenericWriteStore], (instrs SBE_MM, SB_MM, SHE_MM, SWE_MM,
                                    SWLE_MM, SWRE_MM, SCE_MM)>;
 
 def : InstRW<[GenericWritePref], (instrs PREFE_MM)>;
 def : InstRW<[GenericWriteCache], (instrs CACHEE_MM)>;
 
 
 // MIPS16e
 // =======
 
 def : InstRW<[GenericWriteLoad], (instrs Restore16, RestoreX16,
                                   LbRxRyOffMemX16,
                                   LbuRxRyOffMemX16, LhRxRyOffMemX16,
                                   LhuRxRyOffMemX16, LwRxRyOffMemX16,
                                   LwRxSpImmX16, LwRxPcTcp16, LwRxPcTcpX16)>;
 
 def : InstRW<[GenericWriteStore], (instrs Save16, SaveX16, SbRxRyOffMemX16,
                                    ShRxRyOffMemX16, SwRxRyOffMemX16,
                                    SwRxSpImmX16)>;
 
 // microMIPS
 // =========
 
 def : InstRW<[GenericWriteLoad], (instrs LBU16_MM, LB_MM, LBu_MM, LHU16_MM,
                                   LH_MM, LHu_MM, LL_MM, LW16_MM, LWGP_MM,
                                   LWL_MM, LWM16_MM, LWM32_MM, LWP_MM, LWR_MM,
                                   LWSP_MM, LWU_MM, LWXS_MM, LW_MM)>;
 
 def : InstRW<[GenericWriteStore], (instrs SB16_MM, SC_MM, SH16_MM, SH_MM,
                                    SW16_MM, SWL_MM, SWM16_MM, SWM32_MM, SWM_MM,
                                    SWP_MM, SWR_MM, SWSP_MM, SW_MM)>;
 
 
 def : InstRW<[GenericWritePref], (instrs PREF_MM, PREFX_MM)>;
 
 def : InstRW<[GenericWriteCache], (instrs CACHE_MM)>;
 
 def : InstRW<[GenericWriteSync], (instrs SYNC_MM, SYNCI_MM)>;
 def : InstRW<[GenericWriteSync], (instrs GINVI_MMR6, GINVT_MMR6)>;
 
 // microMIPS32r6
 // =============
 
 def : InstRW<[GenericWriteLoad], (instrs LBU_MMR6, LB_MMR6, LDC2_MMR6, LL_MMR6,
                                   LWM16_MMR6, LWC2_MMR6, LWPC_MMR6, LW_MMR6)>;
 
 def : InstRW<[GenericWriteStore], (instrs SB16_MMR6, SB_MMR6, SDC2_MMR6,
                                    SH16_MMR6, SH_MMR6, SW16_MMR6, SWC2_MMR6,
                                    SWM16_MMR6, SWSP_MMR6, SW_MMR6)>;
 
 def : InstRW<[GenericWriteSync], (instrs SYNC_MMR6, SYNCI_MMR6)>;
 
 def : InstRW<[GenericWritePref], (instrs PREF_MMR6)>;
 
 def : InstRW<[GenericWriteCache], (instrs CACHE_MMR6)>;
 
 // MIPS64
 // ======
 
 def : InstRW<[GenericWriteLoad], (instrs LD, LL64, LLD, LWu, LB64, LBu64,
                                   LH64, LHu64, LW64)>;
 
 // l[dw][lr]
 def : InstRW<[GenericWriteLoad], (instrs LWL64, LWR64, LDL, LDR)>;
 
 def : InstRW<[GenericWriteStore], (instrs SD, SC64, SCD, SB64, SH64, SW64,
                                    SWL64, SWR64)>;
 
 def : InstRW<[GenericWriteStore], (instrs SDL, SDR)>;
 
 // MIPS64R6
 // ========
 
 def : InstRW<[GenericWriteLoad], (instrs LWUPC, LDPC)>;
 
 def : InstRW<[GenericWriteLoad], (instrs LLD_R6, LL64_R6)>;
 
 def : InstRW<[GenericWriteStoreSC], (instrs SC64_R6, SCD_R6)>;
 
 // MIPSR6 CRC ASE - hasCRC
 // =======================
 
 def : InstRW<[GenericWriteALU], (instrs CRC32B, CRC32H, CRC32W, CRC32CB,
                                  CRC32CH, CRC32CW)>;
 
 // MIPS64R6 CRC ASE - hasCRC
 // -------------------------
 
 def : InstRW<[GenericWriteALU], (instrs CRC32D, CRC32CD)>;
 
 
 // Cavium Networks MIPS (cnMIPS) - Octeon, HasCnMips
 // =================================================
 
 def : InstRW<[GenericWriteALU], (instrs BADDu, BBIT0, BBIT032, BBIT1, BBIT132,
                                  CINS, CINS32, CINS64_32, CINS_i32,
                                  DMFC2_OCTEON, DMTC2_OCTEON, DPOP, EXTS,
                                  EXTS32, MTM0, MTM1, MTM2, MTP0, MTP1, MTP2,
                                  POP, SEQ, SEQi, SNE, SNEi,
                                  V3MULU, VMM0, VMULU)>;
 
 def : InstRW<[GenericWriteMDUtoGPR], (instrs DMUL)>;
 
 // Cavium Networks MIPS (cnMIPSP) - Octeon+, HasCnMipsP
 // =================================================
 
 def : InstRW<[GenericWriteALU], (instrs SAA, SAAD)>;
 
 // FPU Pipelines
 // =============
 
 def GenericFPQ : ProcResource<1> { let BufferSize = 1; }
 def GenericIssueFPUS : ProcResource<1> { let Super = GenericFPQ; }
 def GenericIssueFPUL : ProcResource<1> { let Super = GenericFPQ; }
 def GenericIssueFPULoad : ProcResource<1> { let Super = GenericFPQ; }
 def GenericIssueFPUStore : ProcResource<1> { let Super = GenericFPQ; }
 def GenericIssueFPUMove : ProcResource<1> { let Super = GenericFPQ; }
 def GenericFPUDivSqrt : ProcResource<1> { let Super = GenericFPQ; }
 
 // The floating point compare of the 24k series including interAptiv has a
 // listed latency of 1-2. Using the higher latency here.
 
 def GenericWriteFPUCmp : SchedWriteRes<[GenericIssueFPUS]> { let Latency = 2; }
 def GenericWriteFPUS : SchedWriteRes<[GenericIssueFPUS]> { let Latency = 4; }
 def GenericWriteFPUL : SchedWriteRes<[GenericIssueFPUL]> { let Latency = 5; }
 def GenericWriteFPUStore : SchedWriteRes<[GenericIssueFPUStore]> { let
   Latency = 1;
 }
 def GenericWriteFPULoad : SchedWriteRes<[GenericIssueFPULoad]> {
   let Latency = 2;
 }
 def GenericWriteFPUMoveFP : SchedWriteRes<[GenericIssueFPUMove]> {
   let Latency = 4;
 }
 def GenericWriteFPUMoveGPRFPU : SchedWriteRes<[GenericIssueFPUMove]> {
   let Latency = 2;
 }
 def GenericWriteFPUDivS : SchedWriteRes<[GenericFPUDivSqrt]> {
   let Latency = 17;
   let ResourceCycles = [ 14 ];
 }
 def GenericWriteFPUDivD : SchedWriteRes<[GenericFPUDivSqrt]> {
   let Latency = 32;
   let ResourceCycles = [ 29 ];
 }
 def GenericWriteFPURcpS : SchedWriteRes<[GenericFPUDivSqrt]> {
   let Latency = 13;
   let ResourceCycles = [ 10 ];
 }
 def GenericWriteFPURcpD : SchedWriteRes<[GenericFPUDivSqrt]> {
   let Latency = 25;
   let ResourceCycles = [ 21 ];
 }
 def GenericWriteFPURsqrtS : SchedWriteRes<[GenericFPUDivSqrt]> {
   let Latency = 17;
   let ResourceCycles = [ 14 ];
 }
 def GenericWriteFPURsqrtD : SchedWriteRes<[GenericFPUDivSqrt]> {
   let Latency = 32;
   let ResourceCycles = [ 29 ];
 }
 def GenericWriteFPUSqrtS : SchedWriteRes<[GenericFPUDivSqrt]> {
   let Latency = 17;
   let ResourceCycles = [ 14 ];
 }
 def GenericWriteFPUSqrtD : SchedWriteRes<[GenericFPUDivSqrt]> {
   let Latency = 29;
   let ResourceCycles = [ 29 ];
 }
 
 // Floating point compare and branch
 // ---------------------------------
 //
 // c.<cc>.[ds], bc1[tf], bc1[tf]l
 def : InstRW<[GenericWriteFPUCmp], (instrs FCMP_D32, FCMP_D64, FCMP_S32, BC1F,
                                     BC1T, BC1FL, BC1TL)>;
 
 def : InstRW<[GenericWriteFPUCmp], (instregex "C_[A-Z]+_(S|D32|D64)$")>;
 
 // Short Pipe
 // ----------
 //
 // abs.[ds], abs.ps, add.[ds], neg.[ds], neg.ps, madd.s, msub.s, nmadd,s
 // nmsub.s, sub.[ds], mul.s
 
 def : InstRW<[GenericWriteFPUS], (instrs FABS_S, FABS_D32, FABS_D64, FADD_D32,
                                   FADD_D64, FADD_S, MADD_S, MSUB_S, FMUL_S,
                                   FNEG_S, FNEG_D32, FNEG_D64, NMADD_S, NMSUB_S,
                                   FSUB_S, FSUB_D32, FSUB_D64)>;
 
 // Long Pipe
 // ----------
 //
 // nmadd.d, nmsub.d, mul.[ds], mul.ps, ceil.[wl].[sd], cvt.d.[sw], cvt.s.[dw],
 // cvt.w.[sd], cvt.[sw].ps, trunc.w.[ds], trunc.w.ps, floor.[ds],
 // round.[lw].[ds], floor.[lw].ds
 
 // madd.d, msub.dm mul.d, mul.ps, nmadd.d, nmsub.d, ceil.[wl].[sd], cvt.d.[sw],
 // cvt.s.[dw], cvt.w.[sd], cvt.[sw].ps, round.[lw].[ds], floor.[lw].ds,
 // trunc.w.[ds], trunc.w.ps,
 def : InstRW<[GenericWriteFPUL], (instrs ADDR_PS64,
                                   CEIL_L_D64, CEIL_L_S, CEIL_W_D32,
                                   CEIL_W_D64, CEIL_W_S, CVT_D32_S, CVT_D32_W,
                                   CVT_D64_L, CVT_D64_S, CVT_D64_W, CVT_L_D64,
                                   CVT_L_S, CVT_S_D32, CVT_S_D64, CVT_S_L,
                                   CVT_S_W, CVT_W_D32, CVT_W_D64, CVT_W_S,
                                   CVT_PS_S64, CVT_S_PL64, CVT_S_PU64,
                                   CVT_PS_PW64, CVT_PW_PS64, FADD_PS64,
                                   FLOOR_L_D64, FLOOR_L_S, FLOOR_W_D32,
                                   FLOOR_W_D64, FLOOR_W_S, FMUL_D32, FMUL_D64,
                                   FMUL_PS64, FSUB_PS64, MADD_D32, MADD_D64,
                                   MSUB_D32, MSUB_D64, MULR_PS64,
                                   NMADD_D32, NMADD_D64, NMSUB_D32, NMSUB_D64,
                                   PLL_PS64, PLU_PS64, PUL_PS64, PUU_PS64,
                                   ROUND_L_D64, ROUND_L_S, ROUND_W_D32,
                                   ROUND_W_D64, ROUND_W_S, TRUNC_L_D64,
                                   TRUNC_L_S, TRUNC_W_D32, TRUNC_W_D64,
                                   TRUNC_W_S, PseudoTRUNC_W_D,
                                   PseudoTRUNC_W_D32, PseudoTRUNC_W_S)>;
 
 // Pseudo convert instruction
 def : InstRW<[GenericWriteFPUL], (instrs PseudoCVT_D32_W, PseudoCVT_D64_L,
                                   PseudoCVT_D64_W, PseudoCVT_S_L,
                                   PseudoCVT_S_W)>;
 
 // div.[ds], div.ps
 def : InstRW<[GenericWriteFPUDivS], (instrs FDIV_S)>;
 def : InstRW<[GenericWriteFPUDivD], (instrs FDIV_D32, FDIV_D64)>;
 
 // sqrt.[ds], sqrt.ps
 def : InstRW<[GenericWriteFPUSqrtS], (instrs FSQRT_S)>;
 def : InstRW<[GenericWriteFPUSqrtD], (instrs FSQRT_D32, FSQRT_D64)>;
 
 // rsqrt.[ds], recip.[ds]
 def : InstRW<[GenericWriteFPURcpS], (instrs RECIP_S, RSQRT_S)>;
 def : InstRW<[GenericWriteFPURcpD], (instrs RECIP_D32, RECIP_D64,
                                      RSQRT_D32, RSQRT_D64)>;
 
 
 // Load Pipe
 // ---------
 
 // ctc1, mtc1, mthc1, cfc1, mfc1, mfhc1
 def : InstRW<[GenericWriteFPUMoveGPRFPU], (instrs BuildPairF64,
                                            BuildPairF64_64, ExtractElementF64,
                                            ExtractElementF64_64, CFC1, CTC1,
                                            MFC1, MFC1_D64, MFHC1_D32,
                                            MFHC1_D64, MTC1, MTC1_D64,
                                            MTHC1_D32, MTHC1_D64)>;
 
 // swc1, swxc1
 def : InstRW<[GenericWriteFPUStore], (instrs SDC1, SDC164, SDXC1, SDXC164,
                                       SUXC1, SUXC164, SWC1, SWXC1)>;
 
 def : InstRW<[GenericWriteFPUMoveFP], (instrs FMOV_D32, FMOV_D64, FMOV_S)>;
 
 
 // movn.[ds], movz.[ds]
 def : InstRW<[GenericWriteFPUMoveFP], (instrs MOVF_I, MOVF_D32, MOVF_D64,
                                        MOVF_S, MOVT_I, MOVT_D32, MOVT_D64,
                                        MOVT_S, MOVN_I_D32, MOVN_I_D64,
                                        MOVN_I_S, MOVZ_I_D32, MOVZ_I_D64,
                                        MOVZ_I_S)>;
 
 def : InstRW<[GenericWriteFPUMoveFP], (instrs MOVT_I64, MOVF_I64, MOVZ_I64_S,
                                        MOVN_I64_D64, MOVN_I64_S,
                                        MOVZ_I64_D64)>;
 
 // l[dw]x?c1
 def : InstRW<[GenericWriteFPULoad], (instrs LDC1, LDC164, LDXC1, LDXC164,
                                      LUXC1, LUXC164, LWC1, LWXC1)>;
 
 // MIPSR6
 // ======
 
 // sel(eq|ne).[ds], max.[ds], maxa.[ds], min.[ds], mina.[ds], class.[ds]
 def : InstRW<[GenericWriteFPUS], (instrs SELEQZ_S, SELNEZ_S, SELEQZ_D, SELNEZ_D,
                                   MAX_S, MAX_D, MAXA_S, MAXA_D, MIN_S, MIN_D,
                                   MINA_S, MINA_D, CLASS_S, CLASS_D)>;
 
 def : InstRW<[GenericWriteFPUL], (instrs RINT_S, RINT_D)>;
 
 def : InstRW<[GenericWriteFPUCmp], (instrs BC1EQZ, BC1NEZ, SEL_D, SEL_S)>;
 
 def : InstRW<[GenericWriteFPUS], (instrs MADDF_S, MSUBF_S, MADDF_D, MSUBF_D)>;
 
 
 // microMIPS
 // =========
 
 def : InstRW<[GenericWriteFPUMoveFP], (instrs MOVF_D32_MM, MOVF_S_MM,
                                        MOVN_I_D32_MM, MOVN_I_S_MM,
                                        MOVT_D32_MM, MOVT_S_MM, MOVZ_I_D32_MM,
                                        MOVZ_I_S_MM)>;
 
 
 //  cvt.?.?, ceil.?, floor.?, round.?, trunc.? (n)madd.? (n)msub.?
 def : InstRW<[GenericWriteFPUL], (instrs CVT_D32_S_MM, CVT_D32_W_MM,
                                   CVT_D64_S_MM, CVT_D64_W_MM, CVT_L_D64_MM,
                                   CVT_L_S_MM, CVT_S_D32_MM, CVT_S_D64_MM,
                                   CVT_S_W_MM, CVT_W_D32_MM, CVT_W_D64_MM,
                                   CVT_W_S_MM, CEIL_W_MM, CEIL_W_S_MM,
                                   FLOOR_W_MM, FLOOR_W_S_MM, NMADD_S_MM,
                                   NMADD_D32_MM, NMSUB_S_MM, NMSUB_D32_MM,
                                   MADD_S_MM, MADD_D32_MM, ROUND_W_MM,
                                   ROUND_W_S_MM, TRUNC_W_MM, TRUNC_W_S_MM)>;
 
 def : InstRW<[GenericWriteFPUCmp], (instregex "^C_[A-Z]_(S|D32|D64)_MM$")>;
 def : InstRW<[GenericWriteFPUCmp], (instregex "^C_[A-Z][A-Z]_(S|D32|D64)_MM$")>;
 def : InstRW<[GenericWriteFPUCmp], (instregex "^C_[A-Z][A-Z][A-Z]_(S|D32|D64)_MM$")>;
 def : InstRW<[GenericWriteFPUCmp], (instregex "^C_NGLE_(S|D32|D64)_MM$")>;
 def : InstRW<[GenericWriteFPUCmp], (instrs FCMP_S32_MM, FCMP_D32_MM)>;
 
 def : InstRW<[GenericWriteFPUS], (instrs MFC1_MM, MFHC1_D32_MM, MFHC1_D64_MM,
                                   MTC1_MM, MTC1_D64_MM,
                                   MTHC1_D32_MM, MTHC1_D64_MM)>;
 
 def : InstRW<[GenericWriteFPUS], (instrs FABS_D32_MM, FABS_D64_MM, FABS_S_MM,
                                   FNEG_D32_MM, FNEG_D64_MM, FNEG_S_MM,
                                   FADD_D32_MM, FADD_D64_MM, FADD_S_MM,
                                   FMOV_D32_MM, FMOV_D64_MM, FMOV_S_MM,
                                   FMUL_D32_MM, FMUL_D64_MM, FMUL_S_MM,
                                   FSUB_D32_MM, FSUB_D64_MM, FSUB_S_MM,
                                   MSUB_S_MM, MSUB_D32_MM)>;
 
 def : InstRW<[GenericWriteFPUDivS], (instrs FDIV_S_MM)>;
 def : InstRW<[GenericWriteFPUDivD], (instrs FDIV_D32_MM, FDIV_D64_MM)>;
 
 def : InstRW<[GenericWriteFPUSqrtS], (instrs FSQRT_S_MM)>;
 def : InstRW<[GenericWriteFPUSqrtD], (instrs FSQRT_D32_MM, FSQRT_D64_MM)>;
 
 def : InstRW<[GenericWriteFPURcpS], (instrs RECIP_S_MM, RSQRT_S_MM)>;
 def : InstRW<[GenericWriteFPURcpD], (instrs RECIP_D32_MM, RECIP_D64_MM,
                                      RSQRT_D32_MM, RSQRT_D64_MM)>;
 
 def : InstRW<[GenericWriteFPUStore], (instrs SDC1_MM, SWC1_MM, SUXC1_MM,
                                       SWXC1_MM)>;
 
 def : InstRW<[GenericWriteFPUMoveGPRFPU], (instrs CFC1_MM, CTC1_MM)>;
 
 def : InstRW<[GenericWriteFPULoad], (instrs LDC1_MM, LUXC1_MM, LWC1_MM,
                                      LWXC1_MM)>;
 
 // microMIPS32r6
 // =============
 
 def : InstRW<[GenericWriteFPUS], (instrs FNEG_S_MMR6)>;
 
 def : InstRW<[GenericWriteFPUCmp], (instregex "CMP_[A-Z][A-Z]_(S|D)_MMR6")>;
 def : InstRW<[GenericWriteFPUCmp],
              (instregex "CMP_[A-Z][A-Z][A-Z]_(S|D)_MMR6")>;
 def : InstRW<[GenericWriteFPUCmp],
              (instregex "CMP_[A-Z][A-Z][A-Z][A-Z]_(S|D)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUL],
              (instregex "CVT_(L|D|S|W)_(L|D|S|L|W)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUL],
              (instregex "TRUNC_(L|W)_(D|S)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUL],
              (instregex "ROUND_(L|W)_(D|S)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUL],
              (instregex "FLOOR_(L|W)_(D|S)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUL],
              (instregex "CEIL_(L|W)_(S|D)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUS],
              (instrs MFC1_MMR6, MTC1_MMR6, CLASS_S_MMR6, CLASS_D_MMR6,
               FADD_S_MMR6)>;
 
 def : InstRW<[GenericWriteFPUS], (instregex "M(IN|AX)_(S|D)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUS], (instregex "M(IN|AX)A_(S|D)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUS], (instregex "SEL(EQ|NE)Z_(S|D)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUS], (instregex "SEL_(S|D)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUL], (instrs RINT_S_MMR6, RINT_D_MMR6)>;
 
 def : InstRW<[GenericWriteFPUS], (instregex "M(ADD|SUB)F_(S|D)_MMR6")>;
 
 def : InstRW<[GenericWriteFPUS], (instrs FMOV_S_MMR6, FMUL_S_MMR6,
                                   FSUB_S_MMR6, FMOV_D_MMR6)>;
 
 def : InstRW<[GenericWriteFPUL], (instrs FDIV_S_MMR6)>;
 
 def : InstRW<[GenericWriteFPUStore], (instrs SDC1_D64_MMR6)>;
 
 def : InstRW<[GenericWriteFPULoad], (instrs LDC1_D64_MMR6)>;
 
 // MIPS64
 // ======
 
 def : InstRW<[GenericWriteFPUMoveGPRFPU], (instrs DMFC1, DMTC1)>;
 
 // MIPS DSP ASE, HasDSP
 // ====================
 
 def : InstRW<[GenericWriteStore], (instrs SWDSP)>;
 
 def : InstRW<[GenericWriteLoad], (instrs LWDSP)>;
 
 def : InstRW<[GenericWriteMove], (instrs PseudoMTLOHI_DSP)>;
 
 def GenericDSP : ProcResource<1> { let BufferSize = 1; }
 def GenericDSPShort : SchedWriteRes<[GenericDSP]> { let Latency = 2; }
 def GenericDSPLong : SchedWriteRes<[GenericDSP]> { let Latency = 6; }
 def GenericDSPBypass : SchedWriteRes<[GenericDSP]> { let Latency = 1; }
 def GenericDSPMTHILO : SchedWriteRes<[GenericDSP]> { let Latency = 5; }
 def GenericDSPLoad : SchedWriteRes<[GenericDSP]> { let Latency = 4; }
 def GenericDSPMTHLIP : SchedWriteRes<[GenericDSP]> { let Latency = 5; }
 
 def : InstRW<[GenericDSPLong], (instregex "^EXTRV_RS_W$")>;
 def : InstRW<[GenericDSPLong], (instregex "^EXTRV_R_W$")>;
 def : InstRW<[GenericDSPLong], (instregex "^EXTRV_S_H$")>;
 def : InstRW<[GenericDSPLong], (instregex "^EXTRV_W$")>;
 def : InstRW<[GenericDSPLong], (instregex "^EXTR_RS_W$")>;
 def : InstRW<[GenericDSPLong], (instregex "^EXTR_R_W$")>;
 def : InstRW<[GenericDSPLong], (instregex "^EXTR_S_H$")>;
 def : InstRW<[GenericDSPLong], (instregex "^EXTR_W$")>;
 def : InstRW<[GenericDSPLong], (instregex "^INSV$")>;
 
 def : InstRW<[GenericDSPMTHLIP], (instregex "^MTHLIP$")>;
 def : InstRW<[GenericDSPMTHILO], (instregex "^MTHI_DSP$")>;
 def : InstRW<[GenericDSPMTHILO], (instregex "^MTLO_DSP$")>;
 
 def : InstRW<[GenericDSPShort], (instregex "^ABSQ_S_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ABSQ_S_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQ_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQ_S_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQ_S_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDSC$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDU_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDU_S_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDWC$")>;
 def : InstRW<[GenericDSPShort], (instregex "^BITREV$")>;
 def : InstRW<[GenericDSPShort], (instregex "^BPOSGE32$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGU_EQ_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGU_LE_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGU_LT_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPU_EQ_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPU_LE_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPU_LT_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMP_EQ_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMP_LE_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMP_LT_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAQ_SA_L_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAQ_S_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAU_H_QBL$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAU_H_QBR$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSQ_SA_L_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSQ_S_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSU_H_QBL$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSU_H_QBR$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTPDPV$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTPDP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTPV$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^LBUX$")>;
 def : InstRW<[GenericDSPShort], (instregex "^LHX$")>;
 def : InstRW<[GenericDSPShort], (instregex "^LWX$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MADDU_DSP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MADD_DSP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MAQ_SA_W_PHL$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MAQ_SA_W_PHR$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MAQ_S_W_PHL$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MAQ_S_W_PHR$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MFHI_DSP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MFLO_DSP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MODSUB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MSUBU_DSP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MSUB_DSP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULEQ_S_W_PHL$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULEQ_S_W_PHR$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULEU_S_PH_QBL$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULEU_S_PH_QBR$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULQ_RS_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULSAQ_S_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULTU_DSP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULT_DSP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PACKRL_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PICK_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PICK_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQU_PH_QBLA$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQU_PH_QBL$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQU_PH_QBRA$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQU_PH_QBR$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQ_W_PHL$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQ_W_PHR$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEU_PH_QBLA$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEU_PH_QBL$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEU_PH_QBRA$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEU_PH_QBR$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECRQU_S_QB_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECRQ_PH_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECRQ_QB_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECRQ_RS_PH_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^RADDU_W_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^RDDSP$")>;
 def : InstRW<[GenericDSPShort], (instregex "^REPLV_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^REPLV_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^REPL_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^REPL_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHILOV$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHILO$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLLV_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLLV_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLLV_S_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLLV_S_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLL_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLL_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLL_S_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLL_S_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRAV_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRAV_R_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRAV_R_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRA_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRA_R_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRA_R_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRLV_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRL_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQ_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQ_S_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQ_S_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBU_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBU_S_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^WRDSP$")>;
 
 def : InstRW<[GenericDSPShort],
              (instregex "^Pseudo(CMP|CMPU)_(EQ|LE|LT)_(PH|QB)$")>;
 def : InstRW<[GenericDSPShort],
 						 (instregex "^PseudoPICK_(PH|QB)$")>;
 
 // MIPS DSP R2 - hasDSP, HasDSPR2, InMicroMips
 // ===========================================
 
 def : InstRW<[GenericDSPShort], (instregex "^ABSQ_S_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQH_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQH_R_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQH_R_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQH_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDUH_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDUH_R_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDU_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDU_S_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^APPEND$")>;
 def : InstRW<[GenericDSPShort], (instregex "^BALIGN$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGDU_EQ_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGDU_LE_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGDU_LT_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPA_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAQX_SA_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAQX_S_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAX_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPS_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSQX_S_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSQX_SA_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSX_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MUL_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MUL_S_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULQ_RS_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULQ_S_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULQ_S_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULSA_W_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECR_QB_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECR_SRA_PH_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECR_SRA_R_PH_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PREPEND$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRA_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRA_R_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRAV_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRAV_R_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRL_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRLV_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQH_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQH_R_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQH_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQH_R_W$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBU_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBU_S_PH$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBUH_QB$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBUH_R_QB$")>;
 
 // microMIPS DSP R1 - HasDSP, InMicroMips
 // ======================================
 
 def : InstRW<[GenericWriteLoad], (instrs LWDSP_MM)>;
 
 def : InstRW<[GenericWriteStore], (instrs SWDSP_MM)>;
 
 def : InstRW<[GenericDSPShort], (instregex "^ABSQ_S_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ABSQ_S_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQ_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQ_S_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQ_S_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDSC_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDU_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDU_S_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDWC_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^BITREV_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^BPOSGE32_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGU_EQ_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGU_LE_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGU_LT_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPU_EQ_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPU_LE_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPU_LT_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMP_EQ_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMP_LE_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMP_LT_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAQ_SA_L_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAQ_S_W_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAU_H_QBL_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAU_H_QBR_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSQ_SA_L_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSQ_S_W_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSU_H_QBL_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSU_H_QBR_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTPDPV_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTPDP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTPV_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTRV_RS_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTRV_R_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTRV_S_H_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTRV_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTR_RS_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTR_R_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTR_S_H_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^EXTR_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^INSV_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^LBUX_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^LHX_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^LWX_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MADDU_DSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MADD_DSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MAQ_SA_W_PHL_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MAQ_SA_W_PHR_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MAQ_S_W_PHL_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MAQ_S_W_PHR_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MFHI_DSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MFLO_DSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MODSUB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MOVEP_MMR6$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MOVN_I_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MOVZ_I_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MSUBU_DSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MSUB_DSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MTHI_DSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MTHLIP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MTLO_DSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULEQ_S_W_PHL_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULEQ_S_W_PHR_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULEU_S_PH_QBL_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULEU_S_PH_QBR_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULQ_RS_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULSAQ_S_W_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULTU_DSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULT_DSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PACKRL_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PICK_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PICK_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQU_PH_QBLA_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQU_PH_QBL_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQU_PH_QBRA_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQU_PH_QBR_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQ_W_PHL_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEQ_W_PHR_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEU_PH_QBLA_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEU_PH_QBL_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEU_PH_QBRA_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECEU_PH_QBR_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECRQU_S_QB_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECRQ_PH_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECRQ_QB_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECRQ_RS_PH_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^RADDU_W_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^RDDSP_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^REPLV_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^REPLV_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^REPL_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^REPL_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHILOV_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHILO_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLLV_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLLV_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLLV_S_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLLV_S_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLL_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLL_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLL_S_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHLL_S_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRAV_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRAV_R_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRAV_R_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRA_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRA_R_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRA_R_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRLV_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRL_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQ_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQ_S_PH_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQ_S_W_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBU_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBU_S_QB_MM$")>;
 def : InstRW<[GenericDSPShort], (instregex "^WRDSP_MM$")>;
 
 
 // microMIPS DSP R2 - hasDSP, HasDSPR2, InMicroMips
 // ================================================
 
 def : InstRW<[GenericDSPShort], (instregex "^ABSQ_S_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQH_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQH_R_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQH_R_W_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDQH_W_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDUH_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDUH_R_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDU_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^ADDU_S_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^APPEND_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^BALIGN_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGDU_EQ_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGDU_LE_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^CMPGDU_LT_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPA_W_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAQX_SA_W_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAQX_S_W_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPAX_W_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPS_W_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSQX_S_W_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSQX_SA_W_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^DPSX_W_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MUL_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MUL_S_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULQ_RS_W_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULQ_S_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULQ_S_W_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^MULSA_W_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECR_QB_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECR_SRA_PH_W_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PRECR_SRA_R_PH_W_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^PREPEND_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRA_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRA_R_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRAV_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRAV_R_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRL_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SHRLV_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQH_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQH_R_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQH_W_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBQH_R_W_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBU_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBU_S_PH_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBUH_QB_MMR2$")>;
 def : InstRW<[GenericDSPShort], (instregex "^SUBUH_R_QB_MMR2$")>;
 
 // microMIPS DSP R3 - hasDSP, hasDSPR2, hasDSPR3, InMicroMips
 // ==========================================================
 
 def : InstRW<[GenericDSPShort], (instregex "^BPOSGE32C_MMR3$")>;
 
 // MIPS MSA ASE - hasMSA
 // =====================
 
 def GenericWriteMSAShortLogic : SchedWriteRes<[GenericIssueFPUS]>;
 def GenericWriteMSAShortInt : SchedWriteRes<[GenericIssueFPUS]> {
 let Latency = 2;
 }
 def GenericWriteMoveOtherUnitsToFPU : SchedWriteRes<[GenericIssueFPUS]>;
 def GenericWriteMSAOther3 : SchedWriteRes<[GenericIssueFPUS]> {
 let Latency = 3;
 }
 def GenericWriteMSALongInt : SchedWriteRes<[GenericIssueFPUS]> {
 let Latency = 5;
 }
 def GenericWriteFPUDivI : SchedWriteRes<[GenericFPQ]> {
   let Latency = 33;
   let ResourceCycles = [ 33 ];
 }
 
 // FPUS is also used in moves from floating point and MSA registers to general
 // purpose registers.
 def GenericWriteMoveFPUSToOtherUnits : SchedWriteRes<[GenericIssueFPUS]> {
   let Latency = 0;
 }
 
 // FPUL is also used in moves from floating point and MSA registers to general
 // purpose registers.
 def GenericWriteMoveFPULToOtherUnits : SchedWriteRes<[GenericIssueFPUL]>;
 
 
 // adds_a.[bhwd], adds_[asu].[bhwd], addvi?.[bhwd], asub_[us].[bhwd],
 // aver?_[us].[bhwd]
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^ADD_A_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^ADDS_[ASU]_[BHWD]$")>;
 
 // TODO: ADDVI_[BHW] might be 1 cycle latency rather than 2. Need to confirm it.
 // add.[bhwd], addvi.[bhwd], asub_[us].[bhwd], ave.[bhwd], aver.[bhwd]
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^ADDVI?_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^ASUB_[US].[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^AVER?_[US].[BHWD]$")>;
 
 // and.v, andi.b, move.v, ldi.[bhwd], xor.v, nor.v, xori.b, nori.b, lsa
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^MOVE_V$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^LDI_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instrs LSA)>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(AND|OR|[XN]OR)_V$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(AND|OR|[XN]OR)I_B$")>;
 def : InstRW<[GenericWriteMSAShortLogic],
              (instregex "^(AND|OR|[XN]OR)_V_[DHW]_PSEUDO$")>;
 
 // vshf.[bhwd], binsl.[bhwd], binsr.[bhwd], insert.[bhwd], sld?.[bhwd],
 // bset.[bhwd], bclr.[bhwd], bneg.[bhwd], bsel_v, bseli_b
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^VSHF_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^(BINSL|BINSLI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^(BINSR|BINSRI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^INSERT_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^(SLD|SLDI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^(BSET|BSETI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^(BCLR|BCLRI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^(BNEG|BNEGI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^(BSEL_V|BSELI_B)$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^BMN*Z.*$")>;
 def : InstRW<[GenericWriteMSAShortInt],
              (instregex "^BSEL_(H|W|D|FW|FD)_PSEUDO$")>;
 
 // pcnt.[bhwd], sat_s.[bhwd], sat_u.[bhwd]
 def : InstRW<[GenericWriteMSAOther3], (instregex "^PCNT_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAOther3], (instregex "^SAT_(S|U)_[BHWD]$")>;
 
 // bnz.[bhwdv], cfcmsa, ctcmsa
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(BNZ|BZ)_[BHWDV]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^C(F|T)CMSA$")>;
 
 // shf.[bhw], fill[bhwd], splat?.[bhwd]
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^SHF_[BHW]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^FILL_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^(SPLAT|SPLATI)_[BHWD]$")>;
 
 // fexp2_w, fexp2_d
 def : InstRW<[GenericWriteFPUS], (instregex "^FEXP2_(W|D)$")>;
 
 // compare, converts, round to int, floating point truncate.
 def : InstRW<[GenericWriteFPUS], (instregex "^(CLT|CLTI)_(S|U)_[BHWD]$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^(CLE|CLEI)_(S|U)_[BHWD]$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^(CEQ|CEQI)_[BHWD]$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_UN_(S|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_UEQ_(S|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_EQ_(S|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_LT_(S|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_ULT_(S|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_LE_(S|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_ULE_(S|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_F_(D|S)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_SAF_(D|S)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_SEQ_(D|S)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_SLE_(D|S)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_SLT_(D|S)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_SUEQ_(D|S)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_SULE_(D|S)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_SULT_(D|S)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^CMP_SUN_(D|S)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FS(AF|EQ|LT|LE|NE|OR)_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FSUEQ_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FSULE_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FSULT_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FSUNE_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FSUN_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCAF_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCEQ_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCLE_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCLT_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCNE_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCOR_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCUEQ_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCULE_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCULT_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCUNE_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FCUN_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FABS_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FFINT_(U|S)_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FFQL_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FFQR_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FTINT_(U|S)_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FRINT_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FTQ_(H|W)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FTRUNC_(U|S)_(W|D)$")>;
 
 // fexdo.[hw], fexupl.[wd], fexupr.[wd]
 def : InstRW<[GenericWriteFPUS], (instregex "^FEXDO_(H|W)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FEXUPL_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FEXUPR_(W|D)$")>;
 
 // fclass.[wd], fmax.[wd], fmax_a.[wd], fmin.[wd], fmin_a.[wd], flog2.[wd]
 def : InstRW<[GenericWriteFPUS], (instregex "^FCLASS_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FMAX_A_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FMAX_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FMIN_A_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FMIN_(W|D)$")>;
 def : InstRW<[GenericWriteFPUS], (instregex "^FLOG2_(W|D)$")>;
 
 // interleave right/left, interleave even/odd, insert
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(ILVR|ILVL)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(ILVEV|ILVOD)_[BHWD]$")>;
 
 // subs_?.[bhwd], subsus_?.[bhwd], subsuu_?.[bhwd], subvi.[bhwd], subv.[bhwd],
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^SUBS_(S|U)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^SUBSUS_(S|U)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^SUBSUU_(S|U)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^SUBVI_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortInt], (instregex "^SUBV_[BHWD]$")>;
 
 // mod_[su].[bhwd], div_[su].[bhwd]
 def : InstRW<[GenericWriteFPUDivI], (instregex "^MOD_(S|U)_[BHWD]$")>;
 def : InstRW<[GenericWriteFPUDivI], (instregex "^DIV_(S|U)_[BHWD]$")>;
 
 // hadd_[su].[bhwd], hsub_[su].[bhwd], max_[sua].[bhwd], min_[sua].[bhwd],
 // maxi_[su].[bhwd], mini_[su].[bhwd], sra?.[bhwd], srar?.[bhwd], srlr.[bhwd],
 // sll?.[bhwd], pckev.[bhwd], pckod.[bhwd], nloc.[bhwd], nlzc.[bhwd],
 // insve.[bhwd]
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^HADD_(S|U)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^HSUB_(S|U)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(MAX|MIN)_S_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(MAX|MIN)_U_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(MAX|MIN)_A_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic],
              (instregex "^(MAXI|MINI)_(S|U)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(SRA|SRAI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(SRL|SRLI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(SRAR|SRARI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(SRLR|SRLRI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(SLL|SLLI)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(PCKEV|PCKOD)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^(NLOC|NLZC)_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^INSVE_[BHWD]$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^INSERT_F(D|W)_PSEUDO$")>;
 def : InstRW<[GenericWriteMSAShortLogic], (instregex "^FILL_F(D|W)_PSEUDO$")>;
 
 // dpadd_?.[bhwd], dpsub_?.[bhwd], dotp_?.[bhwd], msubv.[bhwd], maddv.[bhwd]
 // mulv.[bhwd].
 def : InstRW<[GenericWriteMSALongInt], (instregex "^DPADD_(S|U)_[HWD]$")>;
 def : InstRW<[GenericWriteMSALongInt], (instregex "^DPSUB_(S|U)_[HWD]$")>;
 def : InstRW<[GenericWriteMSALongInt], (instregex "^DOTP_(S|U)_[HWD]$")>;
 def : InstRW<[GenericWriteMSALongInt], (instregex "^MSUBV_[BHWD]$")>;
 def : InstRW<[GenericWriteMSALongInt], (instregex "^MADDV_[BHWD]$")>;
 def : InstRW<[GenericWriteMSALongInt], (instregex "^MULV_[BHWD]$")>;
 
 // madd?.q.[hw], msub?.q.[hw], mul?.q.[hw]
 def : InstRW<[GenericWriteMSALongInt], (instregex "^MADDR_Q_[HW]$")>;
 def : InstRW<[GenericWriteMSALongInt], (instregex "^MADD_Q_[HW]$")>;
 def : InstRW<[GenericWriteMSALongInt], (instregex "^MSUBR_Q_[HW]$")>;
 def : InstRW<[GenericWriteMSALongInt], (instregex "^MSUB_Q_[HW]$")>;
 def : InstRW<[GenericWriteMSALongInt], (instregex "^MULR_Q_[HW]$")>;
 def : InstRW<[GenericWriteMSALongInt], (instregex "^MUL_Q_[HW]$")>;
 
 // fadd.[dw], fmadd.[dw], fmul.[dw], frcp.[dw], frsqrt.[dw], fsqrt.[dw]
 // fsub.[dw], fdiv.[dw]
 def : InstRW<[GenericWriteFPUL], (instregex "^FADD_[DW]$")>;
 def : InstRW<[GenericWriteFPUL], (instregex "^FMADD_[DW]$")>;
 def : InstRW<[GenericWriteFPUL], (instregex "^FMSUB_[DW]$")>;
 def : InstRW<[GenericWriteFPUL], (instregex "^FMUL_[DW]$")>;
 def : InstRW<[GenericWriteFPUL], (instregex "^FRCP_[DW]$")>;
 def : InstRW<[GenericWriteFPUL], (instregex "^FRSQRT_[DW]$")>;
 def : InstRW<[GenericWriteFPUL], (instregex "^FSQRT_[DW]$")>;
 def : InstRW<[GenericWriteFPUL], (instregex "^FSUB_[DW]$")>;
 def : InstRW<[GenericWriteFPUL], (instregex "^FDIV_[DW]$")>;
 
 // copy.[su]_[bhwd]
 def : InstRW<[GenericWriteFPUMoveGPRFPU], (instregex "^COPY_U_[BHW]$")>;
 def : InstRW<[GenericWriteFPUMoveGPRFPU], (instregex "^COPY_S_[BHWD]$")>;
 
 def : InstRW<[GenericWriteFPUStore], (instregex "^ST_[BHWD]$")>;
 def : InstRW<[GenericWriteFPUStore], (instrs ST_F16)>;
 def : InstRW<[GenericWriteFPULoad], (instregex "^LD_[BHWD]$")>;
 def : InstRW<[GenericWriteFPULoad], (instrs LD_F16)>;
 
 // Atomic instructions
 
 // FIXME: Define `WriteAtomic` in the MipsSchedule.td and
 // attach it to the Atomic2OpsPostRA, AtomicCmpSwapPostRA, ...
 // classes. Then just define resources for the `WriteAtomic` in each
 // machine models.
 def GenericAtomic : ProcResource<1> { let BufferSize = 1; }
 def GenericWriteAtomic : SchedWriteRes<[GenericAtomic]> { let Latency = 2; }
 
 def : InstRW<[GenericWriteAtomic],
     (instregex "^ATOMIC_SWAP_I(8|16|32|64)_POSTRA$")>;
 def : InstRW<[GenericWriteAtomic],
     (instregex "^ATOMIC_CMP_SWAP_I(8|16|32|64)_POSTRA$")>;
 def : InstRW<[GenericWriteAtomic],
     (instregex "^ATOMIC_LOAD_(ADD|SUB|AND|OR|XOR|NAND|MIN|MAX|UMIN|UMAX)"
                "_I(8|16|32|64)_POSTRA$")>;
 }
diff --git a/llvm/test/CodeGen/Mips/mips16-mult.ll b/llvm/test/CodeGen/Mips/mips16-mult.ll
new file mode 100644
index 000000000000..54227077ec23
--- /dev/null
+++ b/llvm/test/CodeGen/Mips/mips16-mult.ll
@@ -0,0 +1,19 @@
+; RUN: llc -march=mipsel -mattr=mips16 < %s | FileCheck %s
+
+; Function Attrs: noinline nounwind optnone
+define dso_local i32 @do_mult(i32 signext %n, i32 signext %m) #0 {
+entry:
+  %n.addr = alloca i32, align 4
+  %m.addr = alloca i32, align 4
+  store i32 %n, i32* %n.addr, align 4
+  store i32 %m, i32* %m.addr, align 4
+  %0 = load i32, i32* %n.addr, align 4
+  %1 = load i32, i32* %m.addr, align 4
+  %mul = mul i32 %0, %1
+;CHECK: mult ${{[0-9]+}}, ${{[0-9]+}}
+;CHECK: mflo ${{[0-9]+}}
+  ret i32 %mul
+}
+
+attributes #0 = { noinline nounwind optnone "frame-pointer"="all" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="mips32r2" "target-features"="+mips16,+mips32r2,-noabicalls" }
+
